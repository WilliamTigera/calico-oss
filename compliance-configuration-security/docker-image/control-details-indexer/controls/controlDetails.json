[{"controlDetailsID":"C-0001","name":"Forbidden Container Registries","description":"In cases where the Kubernetes cluster is provided by a CSP (e.g., AKS in Azure, GKE in GCP, or EKS in AWS), compromised cloud credential can lead to the cluster takeover. Attackers may abuse cloud account credentials or IAM mechanism to the cluster’s management layer.","category":"Workload","subCategory":"","remediation":"Limit the registries from which you pull container images from","framework":[],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"Checking image from pod spec, if the registry of the image is from the list of blocked registries we raise an alert.","manualCheck":"","configuration":[{"path":"settings.postureControlInputs.publicRegistries","name":"Public registries","description":"Kubescape checks none of these public container registries are in use."},{"path":"settings.postureControlInputs.untrustedRegistries","name":"Registries block list","description":"Kubescape checks none of these user-provided container registries are in use."}]},{"controlDetailsID":"C-0002","name":"Prevent containers from allowing command execution","description":"Attackers with relevant permissions can run malicious commands in the context of legitimate containers in the cluster using “kubectl exec” command. This control determines which subjects have permissions to use this command.","category":"Access control","subCategory":"","remediation":"It is recommended to prohibit “kubectl exec” command in production environments. It is also recommended not to use subjects with this permission for daily cluster operations.","framework":["AllControls","ArmoBest","ClusterScan","MITRE","NSA"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Prevent containers from allowing command execution","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"Prevent containers from allowing command execution","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ClusterScan","name":"Prevent containers from allowing command execution","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"MITRE","name":"Prevent containers from allowing command execution","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"NSA","name":"Prevent containers from allowing command execution","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["RoleBinding","ClusterRoleBinding","Role","ClusterRole"],"testCriteria":"Check which subjects have RBAC permissions to exec into pods– if they have the “pods/exec” verb.","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0004","name":"Resources memory limit and request","description":"This control identifies all Pods for which the memory limit is not set.","category":"Workload","subCategory":"Resource management","remediation":"Set the memory limit or use exception mechanism to avoid unnecessary notifications.","framework":[],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"","manualCheck":"","configuration":[{"path":"settings.postureControlInputs.memory_request_max","name":"memory_request_max","description":"Ensure a memory resource request is set and is under this defined maximum value."},{"path":"settings.postureControlInputs.memory_request_min","name":"memory_request_min","description":"Ensure a memory resource request is set and is above this defined minimum value."},{"path":"settings.postureControlInputs.memory_limit_max","name":"memory_limit_max","description":"Ensure a memory resource limit is set and is under this defined maximum value."},{"path":"settings.postureControlInputs.memory_limit_min","name":"memory_limit_min","description":"Ensure a memory resource limit is set and is under this defined maximum value."}]},{"controlDetailsID":"C-0005","name":"API server insecure port is enabled","description":"Kubernetes control plane API is running with non-secure port enabled which allows attackers to gain unprotected access to the cluster.","category":"Control plane","subCategory":"","remediation":"Set the insecure-port flag of the API server to zero.","framework":["AllControls","ArmoBest","ClusterScan","NSA","security"],"prerequisites":{"cloudProviders":null},"severity":9,"frameworkOverrides":[{"frameworkName":"AllControls","name":"API server insecure port is enabled","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"API server insecure port is enabled","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ClusterScan","name":"API server insecure port is enabled","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"NSA","name":"API server insecure port is enabled","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"security","name":"API server insecure port is enabled","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"Check if the insecure-port flag is set (in case of cloud vendor hosted Kubernetes service this verification will not be effective).","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0007","name":"Roles with delete capabilities","description":"Attackers may attempt to destroy data and resources in the cluster. This includes deleting deployments, configurations, storage, and compute resources. This control identifies all subjects that can delete resources.","category":"Access control","subCategory":"","remediation":"You should follow the least privilege principle and minimize the number of subjects that can delete resources.","framework":["AllControls","ClusterScan","MITRE"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Roles with delete capabilities","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ClusterScan","name":"Roles with delete capabilities","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"MITRE","name":"Roles with delete capabilities","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Role","ClusterRole","ClusterRoleBinding","RoleBinding"],"testCriteria":"Check which subjects have delete/deletecollection RBAC permissions on workloads.","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0009","name":"Resource limits","description":"CPU and memory resources should have a limit set for every container or a namespace to prevent resource exhaustion. This control identifies all the pods without resource limit definitions by checking their yaml definition file as well as their namespace LimitRange objects. It is also recommended to use ResourceQuota object to restrict overall namespace resources, but this is not verified by this control.","category":"Workload","subCategory":"","remediation":"Define LimitRange and Resource Limits in the namespace or in the deployment/pod manifests.","framework":[],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":" Check for each container if there is a ‘limits’ field defined for both cpu and memory","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0012","name":"Applications credentials in configuration files","description":"Attackers who have access to configuration files can steal the stored secrets and use them. This control checks if ConfigMaps or pod specifications have sensitive information in their configuration.","category":"Secrets","subCategory":"","remediation":"Use Kubernetes secrets or Key Management Systems to store credentials.","framework":["AllControls","ArmoBest","ClusterScan","MITRE","NSA","security","SOC2","WorkloadScan"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Applications credentials in configuration files","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"Applications credentials in configuration files","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ClusterScan","name":"Applications credentials in configuration files","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"MITRE","name":"Applications credentials in configuration files","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"NSA","name":"Applications credentials in configuration files","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"security","name":"Applications credentials in configuration files","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"SOC2","name":"Cryptographic key management - misplaced secrets (CC6.1,CC6.6,CC6.7)","description":"Encryption keys used to protect data at rest and in transit are stored and managed in accordance with the organization's cryptography policy. Access to encryption keys are restricted to authorized personnel.","long_description":"Encryption keys used to protect data at rest and in transit are stored and managed in accordance with the organization's cryptography policy. Access to encryption keys are restricted to authorized personnel.","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"WorkloadScan","name":"Applications credentials in configuration files","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["ConfigMap"],"testCriteria":"Check if the pod has sensitive information in environment variables, by using list of known sensitive key names. Check if there are configmaps with sensitive information.","manualCheck":"","configuration":[{"path":"settings.postureControlInputs.sensitiveValues","name":"Sensitive Values","description":"Strings that identify a value that Kubescape believes should be stored in a Secret, and not in a ConfigMap or an environment variable."},{"path":"settings.postureControlInputs.sensitiveValuesAllowed","name":"Allowed Values","description":"Reduce false positives with known values."},{"path":"settings.postureControlInputs.sensitiveKeyNames","name":"Sensitive Keys","description":"Key names that identify a potential value that should be stored in a Secret, and not in a ConfigMap or an environment variable."},{"path":"settings.postureControlInputs.sensitiveKeyNamesAllowed","name":"Allowed Keys","description":"Reduce false positives with known key names."}]},{"controlDetailsID":"C-0013","name":"Non-root containers","description":"Potential attackers may gain access to a container and leverage its existing privileges to conduct an attack. Therefore, it is not recommended to deploy containers with root privileges unless it is absolutely necessary. This control identifies all the pods running as root or can escalate to root.","category":"Workload","subCategory":"Node escape","remediation":"If your application does not need root privileges, make sure to define runAsNonRoot as true or explicitly set the runAsUser using ID 1000 or higher under the PodSecurityContext or container securityContext. In addition, set an explicit value for runAsGroup using ID 1000 or higher.","framework":["AllControls","ArmoBest","ClusterScan","NSA","security","WorkloadScan"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Non-root containers","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"Non-root containers","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ClusterScan","name":"Non-root containers","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"NSA","name":"Non-root containers","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"security","name":"Non-root containers","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"WorkloadScan","name":"Non-root containers","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"Verify that runAsUser is set to a user id greater than 0 or that runAsNonRoot is set to true, and that runAsGroup is set to an id greater than 0. Check all the combinations with PodSecurityContext and SecurityContext (for containers).","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0014","name":"Access Kubernetes dashboard","description":"Attackers who gain access to the dashboard service account or have its RBAC permissions can use its network access to retrieve information about resources in the cluster or change them. This control checks if a subject that is not dashboard service account is bound to dashboard role/clusterrole, or - if anyone that is not the dashboard pod is associated with dashboard service account.","category":"Access control","subCategory":"","remediation":"Make sure that the “Kubernetes Dashboard” service account is only bound to the Kubernetes dashboard following the least privilege principle.","framework":["AllControls","MITRE"],"prerequisites":{"cloudProviders":null},"severity":2,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Access Kubernetes dashboard","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"MITRE","name":"Access Kubernetes dashboard","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"Check who is associated with the dashboard service account or bound to dashboard role/clusterrole.","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0015","name":"List Kubernetes secrets","description":"Attackers who have permissions to access secrets can access sensitive information that might include credentials to various services. This control determines which user, group or service account can list/get secrets.","category":"Access control","subCategory":"","remediation":"Monitor and approve list of users, groups and service accounts that can access secrets. Use exception mechanism to prevent repetitive the notifications.","framework":["AllControls","ClusterScan","MITRE"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"AllControls","name":"List Kubernetes secrets","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ClusterScan","name":"List Kubernetes secrets","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"MITRE","name":"List Kubernetes secrets","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Role","ClusterRole","ClusterRoleBinding","RoleBinding"],"testCriteria":"Alerting on users  which have get/list/watch RBAC permissions on secrets. ","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0016","name":"Allow privilege escalation","description":"Attackers may gain access to a container and uplift its privilege to enable excessive capabilities.","category":"Workload","subCategory":"Node escape","remediation":"If your application does not need it, make sure the allowPrivilegeEscalation field of the securityContext is set to false.","framework":["AllControls","ArmoBest","NSA","security","WorkloadScan"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Allow privilege escalation","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"Allow privilege escalation","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"NSA","name":"Allow privilege escalation","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"security","name":"Allow privilege escalation","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"WorkloadScan","name":"Allow privilege escalation","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob","PodSecurityPolicy"],"testCriteria":" Check that the allowPrivilegeEscalation field in securityContext of container is set to false.   ","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0017","name":"Immutable container filesystem","description":"Mutable container filesystem can be abused to inject malicious code or data into containers. Use immutable (read-only) filesystem to limit potential attacks.","category":"Workload","subCategory":"Node escape","remediation":"Set the filesystem of the container to read-only when possible (pod securityContext, readOnlyRootFilesystem: true). If containers application needs to write into the filesystem, it is recommended to mount secondary filesystems for specific directories where application require write access.","framework":["AllControls","ArmoBest","NSA","security","WorkloadScan"],"prerequisites":{"cloudProviders":null},"severity":3,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Immutable container filesystem","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"Immutable container filesystem","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"NSA","name":"Immutable container filesystem","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"security","name":"Immutable container filesystem","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"WorkloadScan","name":"Immutable container filesystem","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"Check whether the readOnlyRootFilesystem field in the SecurityContext is set to true. ","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0018","name":"Configured readiness probe","description":"Readiness probe is intended to ensure that workload is ready to process network traffic. It is highly recommended to define readiness probe for every worker container. This control finds all the pods where the readiness probe is not configured.","category":"Workload","subCategory":"","remediation":"Ensure Readiness probes are configured wherever possible.","framework":["AllControls","DevOpsBest"],"prerequisites":{"cloudProviders":null},"severity":3,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Configured readiness probe","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"DevOpsBest","name":"Configured readiness probe","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0020","name":"Mount service principal","description":"When a cluster is deployed in the cloud, in some cases attackers can leverage their access to a container in the cluster to gain cloud credentials. This control determines if any workload contains a volume with potential access to cloud credential.","category":"Workload","subCategory":"","remediation":"Refrain from using path mount to known cloud credentials folders or files .","framework":["AllControls","MITRE"],"prerequisites":{"cloudProviders":["EKS","GKE","AKS"]},"severity":4,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Mount service principal","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"MITRE","name":"Mount service principal","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"Check which workloads have volumes with potential access to known cloud credentials folders or files in node, like “/etc/kubernetes/azure.json” for Azure.","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0021","name":"Exposed sensitive interfaces","description":"Exposing a sensitive interface to the internet poses a security risk. It might enable attackers to run malicious code or deploy containers in the cluster. This control checks if known components (e.g. Kubeflow, Argo Workflows, etc.) are deployed and exposed services externally.","category":"Workload","subCategory":"","remediation":"Consider blocking external interfaces or protect them with appropriate security tools.","framework":["AllControls","MITRE"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Exposed sensitive interfaces","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"MITRE","name":"Exposed sensitive interfaces","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Service","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"Checking if a service of type nodeport/loadbalancer to one of the known exploited interfaces (Apache NiFi, Kubeflow, Argo Workflows, Weave Scope Kubernetes dashboard) exists. Needs to add user config","manualCheck":"","configuration":[{"path":"settings.postureControlInputs.sensitiveInterfaces","name":"Sensitive interfaces","description":"List of known software interfaces that should not generally be exposed to the Internet."}]},{"controlDetailsID":"C-0026","name":"Kubernetes CronJob","description":"Attackers may use Kubernetes CronJob for scheduling execution of malicious code that would run as a pod in the cluster. This control lists all the CronJobs that exist in the cluster for the user to approve.","category":"Workload","subCategory":"","remediation":"Watch Kubernetes CronJobs and make sure they are legitimate.","framework":["AllControls","MITRE"],"prerequisites":{"cloudProviders":null},"severity":1,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Kubernetes CronJob","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"MITRE","name":"Kubernetes CronJob","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["CronJob"],"testCriteria":"We list all CronJobs that exist in cluster for the user to approve.","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0030","name":"Ingress and Egress blocked","description":"Disable Ingress and Egress traffic on all pods wherever possible. It is recommended to define restrictive network policy on all new pods, and then enable sources/destinations that this pod must communicate with.","category":"Network","subCategory":"","remediation":"Define a network policy that restricts ingress and egress connections.","framework":["AllControls","ArmoBest","NSA"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Ingress and Egress blocked","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"Ingress and Egress blocked","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"NSA","name":"Ingress and Egress blocked","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob","NetworkPolicy"],"testCriteria":"Check for each Pod whether there is an ingress and egress policy defined (whether using Pod or Namespace). ","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0031","name":"Delete Kubernetes events","description":"Attackers may delete Kubernetes events to avoid detection of their activity in the cluster. This control identifies all the subjects that can delete Kubernetes events.","category":"Access control","subCategory":"","remediation":"You should follow the least privilege principle. Minimize the number of subjects who can delete Kubernetes events. Avoid using these subjects in the daily operations.","framework":["AllControls","MITRE"],"prerequisites":{"cloudProviders":null},"severity":4,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Delete Kubernetes events","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"MITRE","name":"Delete Kubernetes events","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Role","ClusterRole","ClusterRoleBinding","RoleBinding"],"testCriteria":"List who has delete/deletecollection RBAC permissions on events.","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0034","name":"Automatic mapping of service account","description":"Potential attacker may gain access to a pod and steal its service account token. Therefore, it is recommended to disable automatic mapping of the service account tokens in service account configuration and enable it only for pods that need to use them.","category":"Secrets","subCategory":"","remediation":"Disable automatic mounting of service account tokens to pods either at the service account level or at the individual pod level, by specifying the automountServiceAccountToken: false. Note that pod level takes precedence.","framework":["AllControls","ArmoBest","NSA","security","WorkloadScan"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Automatic mapping of service account","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"Automatic mapping of service account","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"NSA","name":"Automatic mapping of service account","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"security","name":"Automatic mapping of service account","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"WorkloadScan","name":"Automatic mapping of service account","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","ServiceAccount","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"Check all service accounts on which automount is not disabled.  Check all workloads on which they and their service account don't disable automount ","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0035","name":"Administrative Roles","description":"Attackers who have cluster admin permissions (can perform any action on any resource), can take advantage of their privileges for malicious activities. This control determines which subjects have cluster admin permissions.","category":"Access control","subCategory":"","remediation":"You should apply least privilege principle. Make sure cluster admin permissions are granted only when it is absolutely necessary. Don't use subjects with such high permissions for daily operations.","framework":["AllControls","ArmoBest","ClusterScan","MITRE","NSA","security","SOC2"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Administrative Roles","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"Administrative Roles","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ClusterScan","name":"Administrative Roles","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"MITRE","name":"Administrative Roles","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"NSA","name":"Administrative Roles","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"security","name":"Administrative Roles","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"SOC2","name":"Access restriction to infrastructure - admin access (CC6.1 ,CC6.2, CC6.7, CC6.8)","description":"Administrative access on the in-scope production infrastructure (cloud platform, servers, database) are restricted to authorized users based on job responsibilities.","long_description":"Administrative access on the in-scope production infrastructure (cloud platform, servers, database) are restricted to authorized users based on job responsibilities.","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Role","ClusterRole","ClusterRoleBinding","RoleBinding"],"testCriteria":"Check which subjects have cluster-admin RBAC permissions – either by being bound to the cluster-admin clusterrole, or by having equivalent high privileges.  ","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0036","name":"Validate admission controller (validating)","description":"Attackers can use validating webhooks to intercept and discover all the resources in the cluster. This control lists all the validating webhook configurations that must be verified.","category":"Access control","subCategory":"","remediation":"Ensure all the webhooks are necessary. Use exception mechanism to prevent repititive notifications.","framework":["AllControls","ClusterScan","MITRE"],"prerequisites":{"cloudProviders":null},"severity":3,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Validate admission controller (validating)","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ClusterScan","name":"Validate admission controller (validating)","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"MITRE","name":"Validate admission controller (validating)","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["ValidatingWebhookConfiguration"],"testCriteria":"","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0037","name":"CoreDNS poisoning","description":"If attackers have permissions to modify the coredns ConfigMap they can change the behavior of the cluster’s DNS, poison it, and override the network identity of other services. This control identifies all subjects allowed to update the 'coredns' configmap.","category":"Access control","subCategory":"","remediation":"You should follow the least privilege principle. Monitor and approve all the subjects allowed to modify the 'coredns' configmap. It is also recommended to remove this permission from the users/service accounts used in the daily operations.","framework":["MITRE"],"prerequisites":{"cloudProviders":null},"severity":4,"frameworkOverrides":[{"frameworkName":"MITRE","name":"CoreDNS poisoning","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Role","ClusterRole","ClusterRoleBinding","RoleBinding"],"testCriteria":"Check who has update/patch RBAC permissions on ‘coredns’ configmaps, or to all configmaps.","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0038","name":"Host PID/IPC privileges","description":"Containers should be isolated from the host machine as much as possible. The hostPID and hostIPC fields in deployment yaml may allow cross-container influence and may expose the host itself to potentially malicious or destructive actions. This control identifies all pods using hostPID or hostIPC privileges.","category":"Workload","subCategory":"Node escape","remediation":"Remove hostPID and hostIPC from the yaml file(s) privileges unless they are absolutely necessary.","framework":["AllControls","ArmoBest","ClusterScan","NSA","security","WorkloadScan"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Host PID/IPC privileges","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"Host PID/IPC privileges","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ClusterScan","name":"Host PID/IPC privileges","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"NSA","name":"Host PID/IPC privileges","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"security","name":"Host PID/IPC privileges","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"WorkloadScan","name":"Host PID/IPC privileges","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0039","name":"Validate admission controller (mutating)","description":"Attackers may use mutating webhooks to intercept and modify all the resources in the cluster. This control lists all mutating webhook configurations that must be verified.","category":"Access control","subCategory":"","remediation":"Ensure all the webhooks are necessary. Use exception mechanism to prevent repititive notifications.","framework":["AllControls","ClusterScan","MITRE"],"prerequisites":{"cloudProviders":null},"severity":4,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Validate admission controller (mutating)","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ClusterScan","name":"Validate admission controller (mutating)","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"MITRE","name":"Validate admission controller (mutating)","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["MutatingWebhookConfiguration"],"testCriteria":"","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0041","name":"HostNetwork access","description":"Potential attackers may gain access to a pod and inherit access to the entire host network. For example, in AWS case, they will have access to the entire VPC. This control identifies all the pods with host network access enabled.","category":"Workload","subCategory":"Network","remediation":"Only connect pods to host network when it is necessary. If not, set the hostNetwork field of the pod spec to false, or completely remove it (false is the default). Whitelist only those pods that must have access to host network by design.","framework":["AllControls","ArmoBest","ClusterScan","NSA","security","WorkloadScan"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"AllControls","name":"HostNetwork access","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"HostNetwork access","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ClusterScan","name":"HostNetwork access","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"NSA","name":"HostNetwork access","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"security","name":"HostNetwork access","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"WorkloadScan","name":"HostNetwork access","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0042","name":"SSH server running inside container","description":"An SSH server that is running inside a container may be used by attackers to get remote access to the container. This control checks if pods have an open SSH port (22/2222).","category":"Workload","subCategory":"","remediation":"Remove SSH from the container image or limit the access to the SSH server using network policies.","framework":["AllControls","MITRE"],"prerequisites":{"cloudProviders":null},"severity":3,"frameworkOverrides":[{"frameworkName":"AllControls","name":"SSH server running inside container","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"MITRE","name":"SSH server running inside container","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Service","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"Check if service connected to some workload has an SSH port (22/2222). If so we raise an alert. ","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0044","name":"Container hostPort","description":"Configuring hostPort requires a particular port number. If two objects specify the same HostPort, they could not be deployed to the same node. It may prevent the second object from starting, even if Kubernetes will try reschedule it on another node, provided there are available nodes with sufficient amount of resources. Also, if the number of replicas of such workload is higher than the number of nodes, the deployment will consistently fail.","category":"Network","subCategory":"","remediation":"Avoid usage of hostPort unless it is absolutely necessary, in which case define appropriate exception. Use NodePort / ClusterIP instead.","framework":["AllControls","ArmoBest","DevOpsBest","NSA","security","WorkloadScan"],"prerequisites":{"cloudProviders":null},"severity":4,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Container hostPort","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"Container hostPort","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"DevOpsBest","name":"Container hostPort","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"NSA","name":"Container hostPort","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"security","name":"Container hostPort","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"WorkloadScan","name":"Container hostPort","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"Check for each workload (with container) if it exists inside the container hostPort.  ","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0045","name":"Writable hostPath mount","description":"Mounting host directory to the container can be used by attackers to get access to the underlying host and gain persistence.","category":"Workload","subCategory":"Storage","remediation":"Refrain from using the hostPath mount or use the exception mechanism to remove unnecessary notifications.","framework":["AllControls","MITRE","security","WorkloadScan"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Writable hostPath mount","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"MITRE","name":"Writable hostPath mount","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"security","name":"Writable hostPath mount","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"WorkloadScan","name":"Writable hostPath mount","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"Checking in Pod spec if there is a hostPath volume, if it has the section mount.readOnly == false (or doesn’t exist) we raise an alert.","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0046","name":"Insecure capabilities","description":"Giving insecure or excessive capabilities to a container can increase the impact of the container compromise. This control identifies all the pods with dangerous capabilities (see documentation pages for details).","category":"Workload","subCategory":"Node escape","remediation":"Remove all insecure capabilities which are not necessary for the container.","framework":["AllControls","ArmoBest","NSA","security","WorkloadScan"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Insecure capabilities","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"Insecure capabilities","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"NSA","name":"Insecure capabilities","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"security","name":"Insecure capabilities","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"WorkloadScan","name":"Insecure capabilities","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"Check capabilities given against a configurable blacklist of insecure capabilities (https://man7.org/linux/man-pages/man7/capabilities.7.html). ","manualCheck":"","configuration":[{"path":"settings.postureControlInputs.insecureCapabilities","name":"Insecure capabilities","description":"Kubescape looks for these capabilities in containers, which might lead to attackers getting elevated privileges in your cluster. You can see the full list of possible capabilities at https://man7.org/linux/man-pages/man7/capabilities.7.html."}]},{"controlDetailsID":"C-0048","name":"HostPath mount","description":"Mounting host directory to the container can be used by attackers to get access to the underlying host. This control identifies all the pods using hostPath mount.","category":"Workload","subCategory":"Storage","remediation":"Remove hostPath mounts unless they are absolutely necessary and use exception mechanism to remove notifications.","framework":["AllControls","ClusterScan","MITRE","security","WorkloadScan"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"AllControls","name":"HostPath mount","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ClusterScan","name":"HostPath mount","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"MITRE","name":"HostPath mount","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"security","name":"HostPath mount","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"WorkloadScan","name":"HostPath mount","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0049","name":"Network mapping","description":"If no network policy is defined, attackers who gain access to a single container may use it to probe the network. This control lists all namespaces in which no network policies are defined.","category":"Network","subCategory":"","remediation":"Define network policies or use similar network protection mechanisms.","framework":["AllControls","ArmoBest"],"prerequisites":{"cloudProviders":null},"severity":3,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Network mapping","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"Network mapping","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Namespace","NetworkPolicy"],"testCriteria":"Check for each namespace if there is a network policy defined.","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0050","name":"Resources CPU limit and request","description":"This control identifies all Pods for which the CPU limit is not set.","category":"Workload","subCategory":"Resource management","remediation":"Set the CPU limit or use exception mechanism to avoid unnecessary notifications.","framework":[],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"","manualCheck":"","configuration":[{"path":"settings.postureControlInputs.cpu_request_max","name":"cpu_request_max","description":"Ensure a CPU resource request is set and is under this defined maximum value."},{"path":"settings.postureControlInputs.cpu_request_min","name":"cpu_request_min","description":"Ensure a CPU resource request is set and is above this defined minimum value."},{"path":"settings.postureControlInputs.cpu_limit_max","name":"cpu_limit_max","description":"Ensure a CPU resource limit is set and is under this defined maximum value."},{"path":"settings.postureControlInputs.cpu_limit_min","name":"cpu_limit_min","description":"Ensure a CPU resource limit is set and is above this defined minimum value."}]},{"controlDetailsID":"C-0052","name":"Instance Metadata API","description":"Attackers who gain access to a container, may query the metadata API service for getting information about the underlying node. This control checks if there is access from the nodes to cloud providers instance metadata services.","category":"Control plane","subCategory":"","remediation":"Disable metadata services for pods in cloud provider settings.","framework":["AllControls","MITRE"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Instance Metadata API","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"MITRE","name":"Instance Metadata API","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"Check which nodes have access to instance metadata services. The check is for AWS, GCP and Azure.","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0053","name":"Access container service account","description":"Attackers who obtain access to a pod can use its SA token to communicate with KubeAPI server. All pods with SA token mounted (if such token has a Role or a ClusterRole binding) are considerred potentially dangerous.","category":"Access control","subCategory":"","remediation":"Verify that RBAC is enabled. Follow the least privilege principle and ensure that only necessary pods have SA token mounted into them.","framework":["AllControls","MITRE"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Access container service account","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"MITRE","name":"Access container service account","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["RoleBinding","ClusterRoleBinding","Role","ClusterRole"],"testCriteria":"Control checks if RBAC is enabled. If it's not, the SA has unlimited permissions. If RBAC is enabled, it lists  all permissions for each SA.","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0054","name":"Cluster internal networking","description":"If no network policy is defined, attackers who gain access to a container may use it to move laterally in the cluster. This control lists namespaces in which no network policy is defined.","category":"Network","subCategory":"","remediation":"Define Kubernetes network policies or use alternative products to protect cluster network.","framework":["AllControls","ArmoBest","MITRE","NSA"],"prerequisites":{"cloudProviders":null},"severity":4,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Cluster internal networking","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"Cluster internal networking","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"MITRE","name":"Cluster internal networking","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"NSA","name":"Cluster internal networking","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Namespace","NetworkPolicy"],"testCriteria":"Check for each namespace if there is a network policy defined.","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0055","name":"Linux hardening","description":"Containers may be given more privileges than they actually need. This can increase the potential impact of a container compromise.","category":"Workload","subCategory":"Node escape","remediation":"You can use AppArmor, Seccomp, SELinux and Linux Capabilities mechanisms to restrict containers abilities to utilize unwanted privileges.","framework":["AllControls","ArmoBest","NSA","WorkloadScan"],"prerequisites":{"cloudProviders":null},"severity":4,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Linux hardening","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"Linux hardening","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"NSA","name":"Linux hardening","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"WorkloadScan","name":"Linux hardening","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"Check if there is AppArmor or Seccomp or SELinux or Capabilities are defined in the securityContext of container and pod. If none of these fields are defined for both the container and pod, alert.","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0056","name":"Configured liveness probe","description":"Liveness probe is intended to ensure that workload remains healthy during its entire execution lifecycle, or otherwise restrat the container. It is highly recommended to define liveness probe for every worker container. This control finds all the pods where the Liveness probe is not configured.","category":"Workload","subCategory":"","remediation":"Ensure Liveness probes are configured wherever possible.","framework":["AllControls","DevOpsBest"],"prerequisites":{"cloudProviders":null},"severity":4,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Configured liveness probe","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"DevOpsBest","name":"Configured liveness probe","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0057","name":"Privileged container","description":"Potential attackers may gain access to privileged containers and inherit access to the host resources. Therefore, it is not recommended to deploy privileged containers unless it is absolutely necessary. This control identifies all the privileged Pods.","category":"Workload","subCategory":"Node escape","remediation":"Remove privileged capabilities by setting the securityContext.privileged to false. If you must deploy a Pod as privileged, add other restriction to it, such as network policy, Seccomp etc and still remove all unnecessary capabilities. Use the exception mechanism to remove unnecessary notifications.","framework":["AllControls","ArmoBest","ClusterScan","MITRE","NSA","security","WorkloadScan"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Privileged container","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"Privileged container","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ClusterScan","name":"Privileged container","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"MITRE","name":"Privileged container","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"NSA","name":"Privileged container","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"security","name":"Privileged container","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"WorkloadScan","name":"Privileged container","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"Check in Pod spec if securityContext.privileged == true, if so raise an alert.","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0058","name":"CVE-2021-25741 - Using symlink for arbitrary host file system access.","description":"A user may be able to create a container with subPath or subPathExpr volume mounts to access files \u0026 directories anywhere on the host filesystem. Following Kubernetes versions are affected: v1.22.0 - v1.22.1, v1.21.0 - v1.21.4, v1.20.0 - v1.20.10, version v1.19.14 and lower. This control checks the vulnerable versions and the actual usage of the subPath feature in all Pods in the cluster. If you want to learn more about the CVE, please refer to the CVE link: https://nvd.nist.gov/vuln/detail/CVE-2021-25741","category":"Workload","subCategory":"","remediation":"To mitigate this vulnerability without upgrading kubelet, you can disable the VolumeSubpath feature gate on kubelet and kube-apiserver, or remove any existing Pods using subPath or subPathExpr feature.","framework":["AllControls","ArmoBest","MITRE","NSA"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"AllControls","name":"CVE-2021-25741 - Using symlink for arbitrary host file system access.","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"CVE-2021-25741 - Using symlink for arbitrary host file system access.","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"MITRE","name":"CVE-2021-25741 - Using symlink for arbitrary host file system access.","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"NSA","name":"CVE-2021-25741 - Using symlink for arbitrary host file system access.","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Node","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0059","name":"CVE-2021-25742-nginx-ingress-snippet-annotation-vulnerability","description":"Security issue in ingress-nginx where a user that can create or update ingress objects can use the custom snippets feature to obtain all secrets in the cluster (see more at https://github.com/kubernetes/ingress-nginx/issues/7837)","category":"Workload","subCategory":"","remediation":"To mitigate this vulnerability: 1. Upgrade to a version that allows mitigation (\u003e= v0.49.1 or \u003e= v1.0.1), 2. Set allow-snippet-annotations to false in your ingress-nginx ConfigMap based on how you deploy ingress-nginx","framework":["AllControls","ArmoBest","MITRE","NSA"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"AllControls","name":"CVE-2021-25742-nginx-ingress-snippet-annotation-vulnerability","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"CVE-2021-25742-nginx-ingress-snippet-annotation-vulnerability","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"MITRE","name":"CVE-2021-25742-nginx-ingress-snippet-annotation-vulnerability","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"NSA","name":"CVE-2021-25742-nginx-ingress-snippet-annotation-vulnerability","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Deployment","ConfigMap"],"testCriteria":"The control checks if the nginx-ingress-controller contains the ability to disable allowSnippetAnnotations and that indeed this feature is turned off","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0061","name":"Pods in default namespace","description":"It is recommended to avoid running pods in cluster without explicit namespace assignment. This control identifies all the pods running in the default namespace.","category":"Workload","subCategory":"","remediation":"Create necessary namespaces and move all the pods from default namespace there.","framework":["AllControls","ArmoBest","DevOpsBest"],"prerequisites":{"cloudProviders":null},"severity":3,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Pods in default namespace","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"Pods in default namespace","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"DevOpsBest","name":"Pods in default namespace","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"Check that there are no pods in the 'default' namespace","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0062","name":"Sudo in container entrypoint","description":"Adding sudo to a container entry point command may escalate process privileges and allow access to forbidden resources. This control checks all the entry point commands in all containers in the pod to find those that have sudo command.","category":"Workload","subCategory":"","remediation":"Remove sudo from the command line and use Kubernetes native root and capabilities controls to provide necessary privileges where they are required.","framework":["AllControls","ArmoBest"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Sudo in container entrypoint","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"Sudo in container entrypoint","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"Check that there is no 'sudo' in the container entrypoint","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0063","name":"Portforwarding privileges","description":"Attackers with relevant RBAC permission can use “kubectl portforward” command to establish direct communication with pods from within the cluster or even remotely. Such communication will most likely bypass existing security measures in the cluster. This control determines which subjects have permissions to use this command.","category":"Access control","subCategory":"","remediation":"It is recommended to prohibit “kubectl portforward” command in production environments. It is also recommended not to use subjects with this permission for daily cluster operations.","framework":["AllControls","ArmoBest","ClusterScan"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Portforwarding privileges","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"Portforwarding privileges","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ClusterScan","name":"Portforwarding privileges","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Role","ClusterRole","ClusterRoleBinding","RoleBinding"],"testCriteria":"Check which subjects have RBAC permissions to portforward into pods– if they have the “pods/portforward” resource.","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0065","name":"No impersonation","description":"Impersonation is an explicit RBAC permission to use other roles rather than the one assigned to a user, group or service account. This is sometimes needed for testing purposes. However, it is highly recommended not to use this capability in the production environments for daily operations. This control identifies all subjects whose roles include impersonate verb.","category":"Access control","subCategory":"","remediation":"Either remove the impersonate verb from the role where it was found or make sure that this role is not bound to users, groups or service accounts used for ongoing cluster operations. If necessary, bind this role to a subject only for specific needs for limited time period.","framework":["AllControls","ArmoBest"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"AllControls","name":"No impersonation","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"No impersonation","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Role","ClusterRole","ClusterRoleBinding","RoleBinding"],"testCriteria":"Check for RBACs giving 'impersonate' verb to users/groups/uids/serviceaccounts","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0066","name":"Secret/etcd encryption enabled","description":"All Kubernetes Secrets are stored primarily in etcd therefore it is important to encrypt it.","category":"Control plane","subCategory":"","remediation":"Turn on the etcd encryption in your cluster, for more see the vendor documentation.","framework":["AllControls","ArmoBest","cis-eks-t1.2.0","ClusterScan","MITRE","NSA","security"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Secret/etcd encryption enabled","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"Secret/etcd encryption enabled","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-5.3.1 Ensure Kubernetes Secrets are encrypted using Customer Master Keys (CMKs) managed in AWS KMS","description":"Encrypt Kubernetes secrets, stored in etcd, using secrets encryption feature during Amazon EKS cluster creation.","long_description":"Kubernetes can store secrets that pods can access via a mounted volume. Today, Kubernetes secrets are stored with Base64 encoding, but encrypting is the recommended approach. Amazon EKS clusters version 1.13 and higher support the capability of encrypting your Kubernetes secrets using AWS Key Management Service (KMS) Customer Managed Keys (CMK). The only requirement is to enable the encryption provider support during EKS cluster creation.\n\n Use AWS Key Management Service (KMS) keys to provide envelope encryption of Kubernetes secrets stored in Amazon EKS. Implementing envelope encryption is considered a security best practice for applications that store sensitive data and is part of a defense in depth security strategy.\n\n Application-layer Secrets Encryption provides an additional layer of security for sensitive data, such as user defined Secrets and Secrets required for the operation of the cluster, such as service account keys, which are all stored in etcd.\n\n Using this functionality, you can use a key, that you manage in AWS KMS, to encrypt data at the application layer. This protects against attackers in the event that they manage to gain access to etcd.","remediation":"This process can only be performed during Cluster Creation.\n\n Enable 'Secrets Encryption' during Amazon EKS cluster creation as described in the links within the 'References' section.","references":["https://aws.amazon.com/about-aws/whats-new/2020/03/amazon-eks-adds-envelope-encryption-for-secrets-with-aws-kms/"],"manualCheck":"Using the etcdctl commandline, read that secret out of etcd:\n\n \n```\netcdCTL_API=3 etcdctl get /registry/secrets/default/secret1 [...] | hexdump -C\n\n```\n where [...] must be the additional arguments for connecting to the etcd server.\n\n Verify the stored secret is prefixed with k8s:enc:aescbc:v1: which indicates the aescbc provider has encrypted the resulting data.","impactStatement":"","defaultValue":"By default secrets created using the Kubernetes API are stored in *tmpfs* and are encrypted at rest."},{"frameworkName":"ClusterScan","name":"Secret/etcd encryption enabled","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"MITRE","name":"Secret/etcd encryption enabled","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"NSA","name":"Secret/etcd encryption enabled","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"security","name":"Secret/etcd encryption enabled","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"Reading the cluster description from the managed cloud API (EKS, GKE), or the API server pod configuration for native K8s and checking if etcd encryption is enabled","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0067","name":"Audit logs enabled","description":"Audit logging is an important security feature in Kubernetes, it enables the operator to track requests to the cluster. It is important to use it so the operator has a record of events happened in Kubernetes","category":"Control plane","subCategory":"","remediation":"Turn on audit logging for your cluster. Look at the vendor guidelines for more details","framework":["AllControls","ArmoBest","cis-eks-t1.2.0","ClusterScan","MITRE","NSA","SOC2"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Audit logs enabled","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"Audit logs enabled","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-2.1.1 Enable audit Logs","description":"Control plane logs provide visibility into operation of the EKS Control plane component systems. The API server audit logs record all accepted and rejected requests in the cluster. When enabled via EKS configuration the control plane logs for a cluster are exported to a CloudWatch Log Group for persistence.","long_description":"Audit logs enable visibility into all API server requests from authentic and anonymous sources. Stored log data can be analyzed manually or with tools to identify and understand anomalous or negative activity and lead to intelligent remediations.","remediation":"**From Console:**\n\n 1. For each EKS Cluster in each region;\n2. Go to 'Amazon EKS' \u003e 'Clusters' \u003e '' \u003e 'Configuration' \u003e 'Logging'.\n3. Click 'Manage logging'.\n4. Ensure that all options are toggled to 'Enabled'.\n\n \n```\nAPI server: Enabled\nAudit: Enabled\t\nAuthenticator: Enabled\nController manager: Enabled\nScheduler: Enabled\n\n```\n 5. Click 'Save Changes'.\n\n **From CLI:**\n\n \n```\n# For each EKS Cluster in each region;\naws eks update-cluster-config \\\n    --region '${REGION_CODE}' \\\n    --name '${CLUSTER_NAME}' \\\n    --logging '{\"clusterLogging\":[{\"types\":[\"api\",\"audit\",\"authenticator\",\"controllerManager\",\"scheduler\"],\"enabled\":true}]}'\n\n```","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ClusterScan","name":"Audit logs enabled","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"MITRE","name":"Audit logs enabled","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"NSA","name":"Audit logs enabled","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"SOC2","name":"Event logging (CC6.8,CC7.1,CC7.2)","description":"Logging is enabled to monitor the following events at the application and/or infrastructure layers.","long_description":"Logging is enabled to monitor the following events at the application and/or infrastructure layers: - Logon attempts - Data deletions - Application and system errors - Changes to software and configuration settings - Changes to system files, configuration files or content files  The logs are monitored by IT Operations staff and significant issues are investigated and resolved within a timely manner.","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"Reading the cluster description from the managed cloud API (EKS, GKE), or the API server pod configuration for native K8s and checking if audit logging is enabled","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0068","name":"PSP enabled","description":"PSP enable fine-grained authorization of pod creation and it is important to enable it","category":"Control plane","subCategory":"","remediation":"Turn Pod Security Policies on in your cluster, if you use other admission controllers to control the behavior that PSP controls, exclude this control from your scans","framework":["AllControls","ArmoBest","MITRE","NSA"],"prerequisites":{"cloudProviders":null},"severity":1,"frameworkOverrides":[{"frameworkName":"AllControls","name":"PSP enabled","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"PSP enabled","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"MITRE","name":"PSP enabled","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"NSA","name":"PSP enabled","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"Reading the cluster description from the managed cloud API (EKS, GKE), or the API server pod configuration for native K8s and checking if PSP is enabled","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0069","name":"Disable anonymous access to Kubelet service","description":"By default, requests to the kubelet's HTTPS endpoint that are not rejected by other configured authentication methods are treated as anonymous requests, and given a username of system:anonymous and a group of system:unauthenticated.","category":"Control plane","subCategory":"","remediation":"Start the kubelet with the --anonymous-auth=false flag.","framework":["AllControls","ArmoBest","MITRE","NSA","security"],"prerequisites":{"cloudProviders":null},"severity":10,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Disable anonymous access to Kubelet service","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"Disable anonymous access to Kubelet service","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"MITRE","name":"Disable anonymous access to Kubelet service","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"NSA","name":"Disable anonymous access to Kubelet service","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"security","name":"Disable anonymous access to Kubelet service","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"Reading the kubelet command lines and configuration file looking for anonymous-auth configuration. If this configuration is set on both, the command line values take precedence over it.","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0070","name":"Enforce Kubelet client TLS authentication","description":"Kubelets are the node level orchestrator in Kubernetes control plane. They are publishing service port 10250 where they accept commands from API server. Operator must make sure that only API server is allowed to submit commands to Kubelet. This is done through client certificate verification, must configure Kubelet with client CA file to use for this purpose.","category":"Control plane","subCategory":"","remediation":"Start the kubelet with the --client-ca-file flag, providing a CA bundle to verify client certificates with.","framework":["AllControls","ArmoBest","MITRE","NSA","security"],"prerequisites":{"cloudProviders":null},"severity":9,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Enforce Kubelet client TLS authentication","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"Enforce Kubelet client TLS authentication","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"MITRE","name":"Enforce Kubelet client TLS authentication","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"NSA","name":"Enforce Kubelet client TLS authentication","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"security","name":"Enforce Kubelet client TLS authentication","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"Reading the kubelet command lines and configuration file looking for client TLS configuration.","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0073","name":"Naked pods","description":"It is not recommended to create pods without parental Deployment, ReplicaSet, StatefulSet etc.Manual creation if pods may lead to a configuration drifts and other untracked changes in the system. Such pods won't be automatically rescheduled by Kubernetes in case of a crash or infrastructure failure. This control identifies every pod that does not have corresponding parental object.","category":"Workload","subCategory":"","remediation":"Create necessary Deployment object for every pod making any pod a first class citizen in your IaC architecture.","framework":["AllControls","DevOpsBest"],"prerequisites":{"cloudProviders":null},"severity":3,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Naked pods","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"DevOpsBest","name":"Naked pods","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"Test if pods are not associated with Deployment, ReplicaSet etc. If not, fail.","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0074","name":"Container runtime socket mounted","description":"Mounting Container runtime socket (Unix socket) enables container to access Container runtime, retrieve sensitive information and execute commands, if Container runtime is available. This control identifies pods that attempt to mount Container runtime socket for accessing Container runtime.","category":"","subCategory":"","remediation":"Remove container runtime socket mount request or define an exception.","framework":["AllControls","DevOpsBest","security"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Container runtime socket mounted","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"DevOpsBest","name":"Container runtime socket mounted","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"security","name":"Container runtime socket mounted","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"Check hostpath. If the path is set to one of the container runtime socket, the container has access to container runtime - fail.","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0075","name":"Image pull policy on latest tag","description":"While usage of the latest tag is not generally recommended, in some cases this is necessary. If it is, the ImagePullPolicy must be set to Always, otherwise Kubernetes may run an older image with the same name that happens to be present in the node cache. Note that using Always will not cause additional image downloads because Kubernetes will check the image hash of the local local against the registry and only pull the image if this hash has changed, which is exactly what users want when use the latest tag. This control will identify all pods with latest tag that have ImagePullSecret not set to Always.","category":"Workload","subCategory":"","remediation":"Set ImagePullPolicy to Always in all pods found by this control.","framework":["AllControls","DevOpsBest"],"prerequisites":{"cloudProviders":null},"severity":2,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Image pull policy on latest tag","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"DevOpsBest","name":"Image pull policy on latest tag","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"If  imagePullPolicy = always pass, else fail.","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0076","name":"Label usage for resources","description":"It is recommended to set labels that identify semantic attributes of your application or deployment. For example, { app: myapp, tier: frontend, phase: test, deployment: v3 }. These labels can used to assign policies to logical groups of the deployments as well as for presentation and tracking purposes. This control helps you find deployments without any of the expected labels.","category":"Workload","subCategory":"","remediation":"Define labels that are most suitable to your needs of use the exceptions to prevent further notifications.","framework":["AllControls","DevOpsBest"],"prerequisites":{"cloudProviders":null},"severity":2,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Label usage for resources","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"DevOpsBest","name":"Label usage for resources","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"Test will check if a certain set of labels is defined, this is a configurable control. Initial list: app, tier, phase, version, owner, env.","manualCheck":"","configuration":[{"path":"settings.postureControlInputs.recommendedLabels","name":"Recommended Labels","description":"Kubescape checks that workloads have at least one label that identifies semantic attributes."}]},{"controlDetailsID":"C-0077","name":"K8s common labels usage","description":"Kubernetes common labels help manage and monitor Kubernetes cluster using different tools such as kubectl, dashboard and others in an interoperable way. Refer to https://kubernetes.io/docs/concepts/overview/working-with-objects/common-labels/ for more information. This control helps you find objects that don't have any of these labels defined.","category":"Workload","subCategory":"","remediation":"Define applicable labels or use the exception mechanism to prevent further notifications.","framework":["AllControls","DevOpsBest"],"prerequisites":{"cloudProviders":null},"severity":2,"frameworkOverrides":[{"frameworkName":"AllControls","name":"K8s common labels usage","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"DevOpsBest","name":"K8s common labels usage","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"Test will check if the list of label that start with app.kubernetes.io/ are defined.","manualCheck":"","configuration":[{"path":"settings.postureControlInputs.k8sRecommendedLabels","name":"Kubernetes Recommended Labels","description":"Kubescape checks that workloads have at least one of this list of configurable labels, as recommended in the Kubernetes documentation."}]},{"controlDetailsID":"C-0078","name":"Images from allowed registry","description":"This control is intended to ensure that all the used container images are taken from the authorized repositories. It allows user to list all the approved repositories and will fail all the images taken from any repository outside of this list.","category":"Workload","subCategory":"Supply chain","remediation":"You should enable all trusted repositories in the parameters of this control.","framework":["AllControls","ArmoBest","cis-aks-t1.2.0","cis-eks-t1.2.0","WorkloadScan"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Images from allowed registry","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"Images from allowed registry","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"cis-aks-t1.2.0","name":"CIS-5.1.4 Minimize Container Registries to only those approved","description":"Use approved container registries.","long_description":"Allowing unrestricted access to external container registries provides the opportunity for malicious or unapproved containers to be deployed into the cluster. Allowlisting only approved container registries reduces this risk.","remediation":"If you are using Azure Container Registry you have this option:\u003chttps://docs.microsoft.com/en-us/azure/container-registry/container-registry-firewall-access-rules\u003e\n\n For other non-AKS repos using admission controllers or Azure Policy will also work.\n\n Limiting or locking down egress traffic is also recommended:\n\u003chttps://docs.microsoft.com/en-us/azure/aks/limit-egress-traffic\u003e","references":["\u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-asset-management#am-6-use-only-approved-applications-in-compute-resources\u003e\n\n  \u003chttps://docs.microsoft.com/en-us/azure/aks/limit-egress-traffic\u003e\n\n  \u003chttps://docs.microsoft.com/en-us/azure/container-registry/container-registry-firewall-access-rules\u003e"],"manualCheck":"","impactStatement":"All container images to be deployed to the cluster must be hosted within an approved container image registry.","defaultValue":""},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-5.1.4 Minimize Container Registries to only those approved","description":"Use approved container registries.","long_description":"Allowing unrestricted access to external container registries provides the opportunity for malicious or unapproved containers to be deployed into the cluster. Allowlisting only approved container registries reduces this risk.","remediation":"","references":["https://aws.amazon.com/blogs/opensource/using-open-policy-agent-on-amazon-eks/"],"manualCheck":"","impactStatement":"All container images to be deployed to the cluster must be hosted within an approved container image registry.","defaultValue":""},{"frameworkName":"WorkloadScan","name":"Images from allowed registry","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"Checks if image is from allowed listed registry.","manualCheck":"","configuration":[{"path":"settings.postureControlInputs.imageRepositoryAllowList","name":"Allowed image repositories","description":"Kubescape checks that all container images are from repositories explicitly allowed in this list."}]},{"controlDetailsID":"C-0079","name":"CVE-2022-0185-linux-kernel-container-escape","description":"CVE-2022-0185 is a kernel vulnerability enabling privilege escalation and it can lead attackers to escape containers and take control over nodes. This control alerts on vulnerable kernel versions of Kubernetes nodes","category":"Workload","subCategory":"","remediation":"Patch Linux kernel version to 5.16.2 or above","framework":["AllControls","ArmoBest"],"prerequisites":{"cloudProviders":null},"severity":4,"frameworkOverrides":[{"frameworkName":"AllControls","name":"CVE-2022-0185-linux-kernel-container-escape","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"CVE-2022-0185-linux-kernel-container-escape","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Node"],"testCriteria":"Checking Linux kernel version of the Node objects, if it is above 5.1 or below 5.16.2 it fires an alert","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0081","name":"CVE-2022-24348-argocddirtraversal","description":"CVE-2022-24348 is a major software supply chain 0-day vulnerability in the popular open source CD platform Argo CD which can lead to privilege escalation and information disclosure.","category":"Workload","subCategory":"","remediation":"Update your ArgoCD deployment to fixed versions (v2.1.9,v2.2.4 or v2.3.0)","framework":["AllControls","ArmoBest"],"prerequisites":{"cloudProviders":null},"severity":4,"frameworkOverrides":[{"frameworkName":"AllControls","name":"CVE-2022-24348-argocddirtraversal","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"CVE-2022-24348-argocddirtraversal","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Deployment"],"testCriteria":"Checking Linux kernel version of the Node objects, if it is above 5.1 or below 5.16.2 it fires an alert","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0083","name":"Workloads with Critical vulnerabilities exposed to external traffic","description":"Container images with known critical vulnerabilities pose elevated risk if they are exposed to the external traffic. This control lists all images with such vulnerabilities if either LoadBalancer or NodePort service is assigned to them.","category":"Workload","subCategory":"","remediation":"Either update the container image to fix the vulnerabilities (if such fix is available) or reassess if this workload must be exposed to the outseide traffic. If no fix is available, consider periodic restart of the pod to minimize the risk of persistant intrusion. Use exception mechanism if you don't want to see this report again.","framework":[],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[],"relatedResources":["Service","Pod"],"testCriteria":"This control enumerates external facing workloads, that have LoadBalancer or NodePort services and checks image vulnerability information to see if the image has critical vulnerabilities.","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0084","name":"Workloads with RCE vulnerabilities exposed to external traffic","description":"Container images with known Remote Code Execution (RCE) vulnerabilities pose significantly higher risk if they are exposed to the external traffic. This control lists all images with such vulnerabilities if their pod has either LoadBalancer or NodePort service.","category":"Workload","subCategory":"","remediation":"Either update the container image to fix the vulnerabilities (if such fix is available) or reassess if this workload must be exposed to the outseide traffic. If no fix is available, consider periodic restart of the pod to minimize the risk of persistant intrusion. Use exception mechanism if you don't want to see this report again.","framework":[],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[],"relatedResources":["Service","Pod"],"testCriteria":"This control enumerates external facing workloads, that have LoadBalancer or NodePort service and checks the image vulnerability information for the RCE vulnerability.","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0085","name":"Workloads with excessive amount of vulnerabilities","description":"Container images with multiple Critical and High sevirity vulnerabilities increase the risk of potential exploit. This control lists all such images according to the threashold provided by the customer.","category":"Workload","subCategory":"","remediation":"Update your workload images as soon as possible when fixes become available.","framework":[],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[],"relatedResources":["Pod"],"testCriteria":"This control enumerates workloads and checks if they have excessive amount of vulnerabilities in their container images. The threshold of “excessive number” is configurable.","manualCheck":"","configuration":[{"path":"settings.postureControlInputs.max_critical_vulnerabilities","name":"Max Critical vulnerabilities","description":"The maximum number of Critical severity vulnerabilities permitted."},{"path":"settings.postureControlInputs.max_high_vulnerabilities","name":"Max High vulnerabilities","description":"The maximum number of High severity vulnerabilities permitted."}]},{"controlDetailsID":"C-0087","name":"CVE-2022-23648-containerd-fs-escape","description":"CVE-2022-23648 is a vulnerability of containerd enabling attacker to gain access to read-only copies of arbitrary files from the host using specially-crafted manifests","category":"Workload","subCategory":"","remediation":"Patch containerd to 1.6.1, 1.5.10, 1.4.12  or above","framework":["AllControls","ArmoBest"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"AllControls","name":"CVE-2022-23648-containerd-fs-escape","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"CVE-2022-23648-containerd-fs-escape","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Node"],"testCriteria":"Checking containerd version to see if it is a vulnerable version (where the container runtime is containerd)","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0088","name":"RBAC enabled","description":"RBAC is the most advanced and well accepted mode of authorizing users of the Kubernetes API","category":"Control plane","subCategory":"","remediation":"Enable RBAC either in the API server configuration or with the Kubernetes provider API","framework":["AllControls","cis-aks-t1.2.0","ClusterScan"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"AllControls","name":"RBAC enabled","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"cis-aks-t1.2.0","name":"CIS-5.5.1 Manage Kubernetes RBAC users with Azure AD","description":"Azure Kubernetes Service (AKS) can be configured to use Azure Active Directory (AD) for user authentication. In this configuration, you sign in to an AKS cluster using an Azure AD authentication token. You can also configure Kubernetes role-based access control (Kubernetes RBAC) to limit access to cluster resources based a user's identity or group membership.","long_description":"Kubernetes RBAC and AKS help you secure your cluster access and provide only the minimum required permissions to developers and operators.","remediation":"","references":["\u003chttps://docs.microsoft.com/en-us/azure/aks/azure-ad-rbac\u003e\n\n  \u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-privileged-access#pa-7-follow-just-enough-administration-least-privilege-principle\u003e"],"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ClusterScan","name":"RBAC enabled","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"Testing API server or managed Kubernetes vendor API to determine if RBAC is enabled","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0089","name":"CVE-2022-3172-aggregated-API-server-redirect","description":"The API server allows an aggregated API to redirect client traffic to any URL. This could lead to the client performing unexpected actions as well as forwarding the client's API server credentials to third parties","category":"Workload","subCategory":"","remediation":"Upgrade the Kubernetes version to one of the following versions (or higher patchs): `v1.25.1`, `v1.24.5`, `v1.23.11`, `v1.22.14`","framework":["ArmoBest"],"prerequisites":{"cloudProviders":null},"severity":3,"frameworkOverrides":[{"frameworkName":"ArmoBest","name":"CVE-2022-3172-aggregated-API-server-redirect","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["APIService","Service"],"testCriteria":"List the aggregated-API-server services that could potentially be used to redirect client traffic to any URL, if the API server version is vulnerable to CVE-2022-3172","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0090","name":"CVE-2022-39328-grafana-auth-bypass","description":"CVE-2022-39328 is a critical vulnerability in Grafana, it might enable attacker to access unauthorized endpoints under heavy load.","category":"Workload","subCategory":"","remediation":"Update your Grafana to 9.2.4 or above","framework":["AllControls"],"prerequisites":{"cloudProviders":null},"severity":9,"frameworkOverrides":[{"frameworkName":"AllControls","name":"CVE-2022-39328-grafana-auth-bypass","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Deployment"],"testCriteria":"This control test for vulnerable versions of Grafana (between 9.2 and 9.2.3)","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0091","name":"CVE-2022-47633-kyverno-signature-bypass","description":"CVE-2022-47633 is a high severity vulnerability in Kyverno, it enables attackers to bypass the image signature validation of policies using a malicious image repository or MITM proxy","category":"Workload","subCategory":"","remediation":"Update your Grafana to 9.2.4 or above","framework":["AllControls","ArmoBest"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"AllControls","name":"CVE-2022-47633-kyverno-signature-bypass","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"CVE-2022-47633-kyverno-signature-bypass","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Deployment"],"testCriteria":"This control test for vulnerable versions of Grafana (between 1.8.3 and 1.8.4)","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0092","name":"Ensure that the API server pod specification file permissions are set to 600 or more restrictive","description":"Ensure that the API server pod specification file has permissions of `600` or more restrictive.","category":"Control plane","subCategory":"","remediation":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nchmod 600 /etc/kubernetes/manifests/kube-apiserver.yaml\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.1.1 Ensure that the API server pod specification file permissions are set to 600 or more restrictive","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126653/recommendations/1838561"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nstat -c %a /etc/kubernetes/manifests/kube-apiserver.yaml\n\n```\n Verify that the permissions are `600` or more restrictive.","configuration":[]},{"controlDetailsID":"C-0093","name":"Ensure that the API server pod specification file ownership is set to root:root","description":"Ensure that the API server pod specification file ownership is set to `root:root`.","category":"Control plane","subCategory":"","remediation":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nchown root:root /etc/kubernetes/manifests/kube-apiserver.yaml\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.1.2 Ensure that the API server pod specification file ownership is set to root:root","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126653/recommendations/1838563"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nstat -c %U:%G /etc/kubernetes/manifests/kube-apiserver.yaml\n\n```\n Verify that the ownership is set to `root:root`.","configuration":[]},{"controlDetailsID":"C-0094","name":"Ensure that the controller manager pod specification file permissions are set to 600 or more restrictive","description":"Ensure that the controller manager pod specification file has permissions of `600` or more restrictive.","category":"Control plane","subCategory":"","remediation":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nchmod 600 /etc/kubernetes/manifests/kube-controller-manager.yaml\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.1.3 Ensure that the controller manager pod specification file permissions are set to 600 or more restrictive","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126653/recommendations/1838564"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nstat -c %a /etc/kubernetes/manifests/kube-controller-manager.yaml\n\n```\n Verify that the permissions are `600` or more restrictive.","configuration":[]},{"controlDetailsID":"C-0095","name":"Ensure that the controller manager pod specification file ownership is set to root:root","description":"Ensure that the controller manager pod specification file ownership is set to `root:root`.","category":"Control plane","subCategory":"","remediation":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nchown root:root /etc/kubernetes/manifests/kube-controller-manager.yaml\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.1.4 Ensure that the controller manager pod specification file ownership is set to root:root","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126653/recommendations/1838566"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nstat -c %U:%G /etc/kubernetes/manifests/kube-controller-manager.yaml\n\n```\n Verify that the ownership is set to `root:root`.","configuration":[]},{"controlDetailsID":"C-0096","name":"Ensure that the scheduler pod specification file permissions are set to 600 or more restrictive","description":"Ensure that the scheduler pod specification file has permissions of `600` or more restrictive.","category":"Control plane","subCategory":"","remediation":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nchmod 600 /etc/kubernetes/manifests/kube-scheduler.yaml\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.1.5 Ensure that the scheduler pod specification file permissions are set to 600 or more restrictive","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126653/recommendations/1838568"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nstat -c %a /etc/kubernetes/manifests/kube-scheduler.yaml\n\n```\n Verify that the permissions are `600` or more restrictive.","configuration":[]},{"controlDetailsID":"C-0097","name":"Ensure that the scheduler pod specification file ownership is set to root:root","description":"Ensure that the scheduler pod specification file ownership is set to `root:root`.","category":"Control plane","subCategory":"","remediation":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nchown root:root /etc/kubernetes/manifests/kube-scheduler.yaml\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.1.6 Ensure that the scheduler pod specification file ownership is set to root:root","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126653/recommendations/1838570"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nstat -c %U:%G /etc/kubernetes/manifests/kube-scheduler.yaml\n\n```\n Verify that the ownership is set to `root:root`.","configuration":[]},{"controlDetailsID":"C-0098","name":"Ensure that the etcd pod specification file permissions are set to 600 or more restrictive","description":"Ensure that the `/etc/kubernetes/manifests/etcd.yaml` file has permissions of `600` or more restrictive.","category":"Control plane","subCategory":"","remediation":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nchmod 600 /etc/kubernetes/manifests/etcd.yaml\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.1.7 Ensure that the etcd pod specification file permissions are set to 600 or more restrictive","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126653/recommendations/1838571"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nstat -c %a /etc/kubernetes/manifests/etcd.yaml\n\n```\n Verify that the permissions are `600` or more restrictive.","configuration":[]},{"controlDetailsID":"C-0099","name":"Ensure that the etcd pod specification file ownership is set to root:root","description":"Ensure that the `/etc/kubernetes/manifests/etcd.yaml` file ownership is set to `root:root`.","category":"Control plane","subCategory":"","remediation":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nchown root:root /etc/kubernetes/manifests/etcd.yaml\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.1.8 Ensure that the etcd pod specification file ownership is set to root:root","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126653/recommendations/1838573"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nstat -c %U:%G /etc/kubernetes/manifests/etcd.yaml\n\n```\n Verify that the ownership is set to `root:root`.","configuration":[]},{"controlDetailsID":"C-0100","name":"Ensure that the Container Network Interface file permissions are set to 600 or more restrictive","description":"Ensure that the Container Network Interface files have permissions of `600` or more restrictive.","category":"Network","subCategory":"","remediation":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nchmod 600 \u003cpath/to/cni/files\u003e\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.1.9 Ensure that the Container Network Interface file permissions are set to 600 or more restrictive","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126653/recommendations/1838574"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nstat -c %a \u003cpath/to/cni/files\u003e\n\n```\n Verify that the permissions are `600` or more restrictive.","configuration":[]},{"controlDetailsID":"C-0101","name":"Ensure that the Container Network Interface file ownership is set to root:root","description":"Ensure that the Container Network Interface files have ownership set to `root:root`.","category":"Network","subCategory":"","remediation":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nchown root:root \u003cpath/to/cni/files\u003e\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.1.10 Ensure that the Container Network Interface file ownership is set to root:root","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126653/recommendations/1838576"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nstat -c %U:%G \u003cpath/to/cni/files\u003e\n\n```\n Verify that the ownership is set to `root:root`.","configuration":[]},{"controlDetailsID":"C-0102","name":"Ensure that the etcd data directory permissions are set to 700 or more restrictive","description":"Ensure that the etcd data directory has permissions of `700` or more restrictive.","category":"Control plane","subCategory":"","remediation":"On the etcd server node, get the etcd data directory, passed as an argument `--data-dir`, from the below command:\n\n \n```\nps -ef | grep etcd\n\n```\n Run the below command (based on the etcd data directory found above). For example,\n\n \n```\nchmod 700 /var/lib/etcd\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.1.11 Ensure that the etcd data directory permissions are set to 700 or more restrictive","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126653/recommendations/1838577"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"On the etcd server node, get the etcd data directory, passed as an argument `--data-dir`, from the below command:\n\n \n```\nps -ef | grep etcd\n\n```\n Run the below command (based on the etcd data directory found above). For example,\n\n \n```\nstat -c %a /var/lib/etcd\n\n```\n Verify that the permissions are `700` or more restrictive.","configuration":[]},{"controlDetailsID":"C-0103","name":"Ensure that the etcd data directory ownership is set to etcd:etcd","description":"Ensure that the etcd data directory ownership is set to `etcd:etcd`.","category":"Control plane","subCategory":"","remediation":"On the etcd server node, get the etcd data directory, passed as an argument `--data-dir`, from the below command:\n\n \n```\nps -ef | grep etcd\n\n```\n Run the below command (based on the etcd data directory found above). For example,\n\n \n```\nchown etcd:etcd /var/lib/etcd\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.1.12 Ensure that the etcd data directory ownership is set to etcd:etcd","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126653/recommendations/1838579"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"On the etcd server node, get the etcd data directory, passed as an argument `--data-dir`, from the below command:\n\n \n```\nps -ef | grep etcd\n\n```\n Run the below command (based on the etcd data directory found above). For example,\n\n \n```\nstat -c %U:%G /var/lib/etcd\n\n```\n Verify that the ownership is set to `etcd:etcd`.","configuration":[]},{"controlDetailsID":"C-0104","name":"Ensure that the admin.conf file permissions are set to 600","description":"Ensure that the `admin.conf` file has permissions of `600`.","category":"Control plane","subCategory":"","remediation":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nchmod 600 /etc/kubernetes/admin.conf\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.1.13 Ensure that the admin.conf file permissions are set to 600","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126653/recommendations/1838580"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the following command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nstat -c %a /etc/kubernetes/admin.conf\n\n```\n Verify that the permissions are `600` or more restrictive.","configuration":[]},{"controlDetailsID":"C-0105","name":"Ensure that the admin.conf file ownership is set to root:root","description":"Ensure that the `admin.conf` file ownership is set to `root:root`.","category":"Control plane","subCategory":"","remediation":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nchown root:root /etc/kubernetes/admin.conf\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.1.14 Ensure that the admin.conf file ownership is set to root:root","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126653/recommendations/1838584"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nstat -c %U:%G /etc/kubernetes/admin.conf\n\n```\n Verify that the ownership is set to `root:root`.","configuration":[]},{"controlDetailsID":"C-0106","name":"Ensure that the scheduler.conf file permissions are set to 600 or more restrictive","description":"Ensure that the `scheduler.conf` file has permissions of `600` or more restrictive.","category":"Control plane","subCategory":"","remediation":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nchmod 600 /etc/kubernetes/scheduler.conf\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.1.15 Ensure that the scheduler.conf file permissions are set to 600 or more restrictive","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126653/recommendations/1838586"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the following command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nstat -c %a /etc/kubernetes/scheduler.conf\n\n```\n Verify that the permissions are `600` or more restrictive.","configuration":[]},{"controlDetailsID":"C-0107","name":"Ensure that the scheduler.conf file ownership is set to root:root","description":"Ensure that the `scheduler.conf` file ownership is set to `root:root`.","category":"Control plane","subCategory":"","remediation":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nchown root:root /etc/kubernetes/scheduler.conf\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.1.16 Ensure that the scheduler.conf file ownership is set to root:root","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126653/recommendations/1838587"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nstat -c %U:%G /etc/kubernetes/scheduler.conf\n\n```\n Verify that the ownership is set to `root:root`.","configuration":[]},{"controlDetailsID":"C-0108","name":"Ensure that the controller-manager.conf file permissions are set to 600 or more restrictive","description":"Ensure that the `controller-manager.conf` file has permissions of 600 or more restrictive.","category":"Control plane","subCategory":"","remediation":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nchmod 600 /etc/kubernetes/controller-manager.conf\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.1.17 Ensure that the controller-manager.conf file permissions are set to 600 or more restrictive","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126653/recommendations/1838593"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the following command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nstat -c %a /etc/kubernetes/controller-manager.conf\n\n```\n Verify that the permissions are `600` or more restrictive.","configuration":[]},{"controlDetailsID":"C-0109","name":"Ensure that the controller-manager.conf file ownership is set to root:root","description":"Ensure that the `controller-manager.conf` file ownership is set to `root:root`.","category":"Control plane","subCategory":"","remediation":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nchown root:root /etc/kubernetes/controller-manager.conf\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.1.18 Ensure that the controller-manager.conf file ownership is set to root:root","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126653/recommendations/1838599"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nstat -c %U:%G /etc/kubernetes/controller-manager.conf\n\n```\n Verify that the ownership is set to `root:root`.","configuration":[]},{"controlDetailsID":"C-0110","name":"Ensure that the Kubernetes PKI directory and file ownership is set to root:root","description":"Ensure that the Kubernetes PKI directory and file ownership is set to `root:root`.","category":"Control plane","subCategory":"","remediation":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nchown -R root:root /etc/kubernetes/pki/\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.1.19 Ensure that the Kubernetes PKI directory and file ownership is set to root:root","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126653/recommendations/1838604"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nls -laR /etc/kubernetes/pki/\n\n```\n Verify that the ownership of all files and directories in this hierarchy is set to `root:root`.","configuration":[]},{"controlDetailsID":"C-0111","name":"Ensure that the Kubernetes PKI certificate file permissions are set to 600 or more restrictive","description":"Ensure that Kubernetes PKI certificate files have permissions of `600` or more restrictive.","category":"Control plane","subCategory":"","remediation":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nchmod -R 600 /etc/kubernetes/pki/*.crt\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.1.20 Ensure that the Kubernetes PKI certificate file permissions are set to 600 or more restrictive","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126653/recommendations/1838606"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nls -laR /etc/kubernetes/pki/*.crt\n\n```\n Verify that the permissions are `600` or more restrictive.","configuration":[]},{"controlDetailsID":"C-0112","name":"Ensure that the Kubernetes PKI key file permissions are set to 600","description":"Ensure that Kubernetes PKI key files have permissions of `600`.","category":"Control plane","subCategory":"","remediation":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nchmod -R 600 /etc/kubernetes/pki/*.key\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.1.21 Ensure that the Kubernetes PKI key file permissions are set to 600","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126653/recommendations/1838608"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the below command (based on the file location on your system) on the Control Plane node. For example,\n\n \n```\nls -laR /etc/kubernetes/pki/*.key\n\n```\n Verify that the permissions are `600`.","configuration":[]},{"controlDetailsID":"C-0113","name":"Ensure that the API Server --anonymous-auth argument is set to false","description":"Disable anonymous requests to the API server.","category":"Control plane","subCategory":"","remediation":"Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and set the below parameter.\n\n \n```\n--anonymous-auth=false\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.2.1 Ensure that the API Server --anonymous-auth argument is set to false","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126663/recommendations/1838609"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-apiserver\n\n```\n Verify that the `--anonymous-auth` argument is set to `false`.","configuration":[]},{"controlDetailsID":"C-0114","name":"Ensure that the API Server --token-auth-file parameter is not set","description":"Do not use token based authentication.","category":"Control plane","subCategory":"","remediation":"Follow the documentation and configure alternate mechanisms for authentication. Then, edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the master node and remove the `--token-auth-file=\u003cfilename\u003e` parameter.","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.2.2 Ensure that the API Server --token-auth-file parameter is not set","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126663/recommendations/1838611"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-apiserver\n\n```\n Verify that the `--token-auth-file` argument does not exist.","configuration":[]},{"controlDetailsID":"C-0115","name":"Ensure that the API Server --DenyServiceExternalIPs is not set","description":"This admission controller rejects all net-new usage of the Service field externalIPs.","category":"Control plane","subCategory":"","remediation":"Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the master node and remove the `--DenyServiceExternalIPs'parameter\n\n or\n\n The Kubernetes API server flag disable-admission-plugins takes a comma-delimited list of admission control plugins to be disabled, even if they are in the list of plugins enabled by default.\n\n `kube-apiserver --disable-admission-plugins=DenyServiceExternalIPs,AlwaysDeny ...`","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":4,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.2.3 Ensure that the API Server --DenyServiceExternalIPs is not set","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126663/recommendations/1838614"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-apiserver\n\n```\n Verify that the `--DenyServiceExternalIPs argument does not exist.","configuration":[]},{"controlDetailsID":"C-0116","name":"Ensure that the API Server --kubelet-client-certificate and --kubelet-client-key arguments are set as appropriate","description":"Enable certificate based kubelet authentication.","category":"Control plane","subCategory":"","remediation":"Follow the Kubernetes documentation and set up the TLS connection between the apiserver and kubelets. Then, edit API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and set the kubelet client certificate and key parameters as below.\n\n \n```\n--kubelet-client-certificate=\u003cpath/to/client-certificate-file\u003e\n--kubelet-client-key=\u003cpath/to/client-key-file\u003e\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.2.4 Ensure that the API Server --kubelet-client-certificate and --kubelet-client-key arguments are set as appropriate","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126663/recommendations/1838624"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-apiserver\n\n```\n Verify that the `--kubelet-client-certificate` and `--kubelet-client-key` arguments exist and they are set as appropriate.","configuration":[]},{"controlDetailsID":"C-0117","name":"Ensure that the API Server --kubelet-certificate-authority argument is set as appropriate","description":"Verify kubelet's certificate before establishing connection.","category":"Control plane","subCategory":"","remediation":"Follow the Kubernetes documentation and setup the TLS connection between the apiserver and kubelets. Then, edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and set the `--kubelet-certificate-authority` parameter to the path to the cert file for the certificate authority.\n\n \n```\n--kubelet-certificate-authority=\u003cca-string\u003e\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.2.5 Ensure that the API Server --kubelet-certificate-authority argument is set as appropriate","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126663/recommendations/1838634"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-apiserver\n\n```\n Verify that the `--kubelet-certificate-authority` argument exists and is set as appropriate.","configuration":[]},{"controlDetailsID":"C-0118","name":"Ensure that the API Server --authorization-mode argument is not set to AlwaysAllow","description":"Do not always authorize all requests.","category":"Control plane","subCategory":"","remediation":"Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and set the `--authorization-mode` parameter to values other than `AlwaysAllow`. One such example could be as below.\n\n \n```\n--authorization-mode=RBAC\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.2.6 Ensure that the API Server --authorization-mode argument is not set to AlwaysAllow","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126663/recommendations/1838639"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-apiserver\n\n```\n Verify that the `--authorization-mode` argument exists and is not set to `AlwaysAllow`.","configuration":[]},{"controlDetailsID":"C-0119","name":"Ensure that the API Server --authorization-mode argument includes Node","description":"Restrict kubelet nodes to reading only objects associated with them.","category":"Control plane","subCategory":"","remediation":"Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and set the `--authorization-mode` parameter to a value that includes `Node`.\n\n \n```\n--authorization-mode=Node,RBAC\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.2.7 Ensure that the API Server --authorization-mode argument includes Node","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126663/recommendations/1838641"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-apiserver\n\n```\n Verify that the `--authorization-mode` argument exists and is set to a value to include `Node`.","configuration":[]},{"controlDetailsID":"C-0120","name":"Ensure that the API Server --authorization-mode argument includes RBAC","description":"Turn on Role Based Access Control.","category":"Control plane","subCategory":"","remediation":"Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and set the `--authorization-mode` parameter to a value that includes `RBAC`, for example:\n\n \n```\n--authorization-mode=Node,RBAC\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.2.8 Ensure that the API Server --authorization-mode argument includes RBAC","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126663/recommendations/1838642"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-apiserver\n\n```\n Verify that the `--authorization-mode` argument exists and is set to a value to include `RBAC`.","configuration":[]},{"controlDetailsID":"C-0121","name":"Ensure that the admission control plugin EventRateLimit is set","description":"Limit the rate at which the API server accepts requests.","category":"Control plane","subCategory":"","remediation":"Follow the Kubernetes documentation and set the desired limits in a configuration file.\n\n Then, edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` and set the below parameters.\n\n \n```\n--enable-admission-plugins=...,EventRateLimit,...\n--admission-control-config-file=\u003cpath/to/configuration/file\u003e\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":4,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.2.9 Ensure that the admission control plugin EventRateLimit is set","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126663/recommendations/1838644"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-apiserver\n\n```\n Verify that the `--enable-admission-plugins` argument is set to a value that includes `EventRateLimit`.","configuration":[]},{"controlDetailsID":"C-0122","name":"Ensure that the admission control plugin AlwaysAdmit is not set","description":"Do not allow all requests.","category":"Control plane","subCategory":"","remediation":"Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and either remove the `--enable-admission-plugins` parameter, or set it to a value that does not include `AlwaysAdmit`.","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.2.10 Ensure that the admission control plugin AlwaysAdmit is not set","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126663/recommendations/1838647"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-apiserver\n\n```\n Verify that if the `--enable-admission-plugins` argument is set, its value does not include `AlwaysAdmit`.","configuration":[]},{"controlDetailsID":"C-0123","name":"Ensure that the admission control plugin AlwaysPullImages is set","description":"Always pull images.","category":"Control plane","subCategory":"","remediation":"Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and set the `--enable-admission-plugins` parameter to include `AlwaysPullImages`.\n\n \n```\n--enable-admission-plugins=...,AlwaysPullImages,...\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":4,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.2.11 Ensure that the admission control plugin AlwaysPullImages is set","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126663/recommendations/1838649"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-apiserver\n\n```\n Verify that the `--enable-admission-plugins` argument is set to a value that includes `AlwaysPullImages`.","configuration":[]},{"controlDetailsID":"C-0124","name":"Ensure that the admission control plugin SecurityContextDeny is set if PodSecurityPolicy is not used","description":"The SecurityContextDeny admission controller can be used to deny pods which make use of some SecurityContext fields which could allow for privilege escalation in the cluster. This should be used where PodSecurityPolicy is not in place within the cluster.","category":"Control plane","subCategory":"","remediation":"Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and set the `--enable-admission-plugins` parameter to include `SecurityContextDeny`, unless `PodSecurityPolicy` is already in place.\n\n \n```\n--enable-admission-plugins=...,SecurityContextDeny,...\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":4,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.2.12 Ensure that the admission control plugin SecurityContextDeny is set if PodSecurityPolicy is not used","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126663/recommendations/1838650"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-apiserver\n\n```\n Verify that the `--enable-admission-plugins` argument is set to a value that includes `SecurityContextDeny`, if `PodSecurityPolicy` is not included.","configuration":[]},{"controlDetailsID":"C-0125","name":"Ensure that the admission control plugin ServiceAccount is set","description":"Automate service accounts management.","category":"Control plane","subCategory":"","remediation":"Follow the documentation and create `ServiceAccount` objects as per your environment. Then, edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the master node and ensure that the `--disable-admission-plugins` parameter is set to a value that does not include `ServiceAccount`.","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":3,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.2.13 Ensure that the admission control plugin ServiceAccount is set","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126663/recommendations/1838652"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-apiserver\n\n```\n Verify that the `--disable-admission-plugins` argument is set to a value that does not includes `ServiceAccount`.","configuration":[]},{"controlDetailsID":"C-0126","name":"Ensure that the admission control plugin NamespaceLifecycle is set","description":"Reject creating objects in a namespace that is undergoing termination.","category":"Control plane","subCategory":"","remediation":"Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and set the `--disable-admission-plugins` parameter to ensure it does not include `NamespaceLifecycle`.","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":3,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.2.14 Ensure that the admission control plugin NamespaceLifecycle is set","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126663/recommendations/1838653"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-apiserver\n\n```\n Verify that the `--disable-admission-plugins` argument is set to a value that does not include `NamespaceLifecycle`.","configuration":[]},{"controlDetailsID":"C-0127","name":"Ensure that the admission control plugin NodeRestriction is set","description":"Limit the `Node` and `Pod` objects that a kubelet could modify.","category":"Control plane","subCategory":"","remediation":"Follow the Kubernetes documentation and configure `NodeRestriction` plug-in on kubelets. Then, edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the master node and set the `--enable-admission-plugins` parameter to a value that includes `NodeRestriction`.\n\n \n```\n--enable-admission-plugins=...,NodeRestriction,...\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":4,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.2.15 Ensure that the admission control plugin NodeRestriction is set","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126663/recommendations/1838655"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-apiserver\n\n```\n Verify that the `--enable-admission-plugins` argument is set to a value that includes `NodeRestriction`.","configuration":[]},{"controlDetailsID":"C-0128","name":"Ensure that the API Server --secure-port argument is not set to 0","description":"Do not disable the secure port.","category":"Control plane","subCategory":"","remediation":"Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and either remove the `--secure-port` parameter or set it to a different (non-zero) desired port.","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.2.16 Ensure that the API Server --secure-port argument is not set to 0","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126663/recommendations/1838659"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-apiserver\n\n```\n Verify that the `--secure-port` argument is either not set or is set to an integer value between 1 and 65535.","configuration":[]},{"controlDetailsID":"C-0129","name":"Ensure that the API Server --profiling argument is set to false","description":"Disable profiling, if not needed.","category":"Control plane","subCategory":"","remediation":"Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and set the below parameter.\n\n \n```\n--profiling=false\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":3,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.2.17 Ensure that the API Server --profiling argument is set to false","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126663/recommendations/1838660"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-apiserver\n\n```\n Verify that the `--profiling` argument is set to `false`.","configuration":[]},{"controlDetailsID":"C-0130","name":"Ensure that the API Server --audit-log-path argument is set","description":"Enable auditing on the Kubernetes API Server and set the desired audit log path.","category":"Control plane","subCategory":"","remediation":"Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and set the `--audit-log-path` parameter to a suitable path and file where you would like audit logs to be written, for example:\n\n \n```\n--audit-log-path=/var/log/apiserver/audit.log\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.2.18 Ensure that the API Server --audit-log-path argument is set","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126663/recommendations/1838662"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-apiserver\n\n```\n Verify that the `--audit-log-path` argument is set as appropriate.","configuration":[]},{"controlDetailsID":"C-0131","name":"Ensure that the API Server --audit-log-maxage argument is set to 30 or as appropriate","description":"Retain the logs for at least 30 days or as appropriate.","category":"Control plane","subCategory":"","remediation":"Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and set the `--audit-log-maxage` parameter to 30 or as an appropriate number of days:\n\n \n```\n--audit-log-maxage=30\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":4,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.2.19 Ensure that the API Server --audit-log-maxage argument is set to 30 or as appropriate","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126663/recommendations/1838664"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-apiserver\n\n```\n Verify that the `--audit-log-maxage` argument is set to `30` or as appropriate.","configuration":[]},{"controlDetailsID":"C-0132","name":"Ensure that the API Server --audit-log-maxbackup argument is set to 10 or as appropriate","description":"Retain 10 or an appropriate number of old log files.","category":"Control plane","subCategory":"","remediation":"Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and set the `--audit-log-maxbackup` parameter to 10 or to an appropriate value.\n\n \n```\n--audit-log-maxbackup=10\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":4,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.2.20 Ensure that the API Server --audit-log-maxbackup argument is set to 10 or as appropriate","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126663/recommendations/1838665"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-apiserver\n\n```\n Verify that the `--audit-log-maxbackup` argument is set to `10` or as appropriate.","configuration":[]},{"controlDetailsID":"C-0133","name":"Ensure that the API Server --audit-log-maxsize argument is set to 100 or as appropriate","description":"Rotate log files on reaching 100 MB or as appropriate.","category":"Control plane","subCategory":"","remediation":"Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and set the `--audit-log-maxsize` parameter to an appropriate size in MB. For example, to set it as 100 MB:\n\n \n```\n--audit-log-maxsize=100\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":4,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.2.21 Ensure that the API Server --audit-log-maxsize argument is set to 100 or as appropriate","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126663/recommendations/1838666"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-apiserver\n\n```\n Verify that the `--audit-log-maxsize` argument is set to `100` or as appropriate.","configuration":[]},{"controlDetailsID":"C-0134","name":"Ensure that the API Server --request-timeout argument is set as appropriate","description":"Set global request timeout for API server requests as appropriate.","category":"Control plane","subCategory":"","remediation":"Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` and set the below parameter as appropriate and if needed. For example,\n\n \n```\n--request-timeout=300s\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":4,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.2.22 Ensure that the API Server --request-timeout argument is set as appropriate","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126663/recommendations/1838667"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-apiserver\n\n```\n Verify that the `--request-timeout` argument is either not set or set to an appropriate value.","configuration":[]},{"controlDetailsID":"C-0135","name":"Ensure that the API Server --service-account-lookup argument is set to true","description":"Validate service account before validating token.","category":"Control plane","subCategory":"","remediation":"Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and set the below parameter.\n\n \n```\n--service-account-lookup=true\n\n```\n Alternatively, you can delete the `--service-account-lookup` parameter from this file so that the default takes effect.","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.2.23 Ensure that the API Server --service-account-lookup argument is set to true","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126663/recommendations/1838668"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-apiserver\n\n```\n Verify that if the `--service-account-lookup` argument exists it is set to `true`.","configuration":[]},{"controlDetailsID":"C-0136","name":"Ensure that the API Server --service-account-key-file argument is set as appropriate","description":"Explicitly set a service account public key file for service accounts on the apiserver.","category":"Control plane","subCategory":"","remediation":"Edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the Control Plane node and set the `--service-account-key-file` parameter to the public key file for service accounts:\n\n \n```\n--service-account-key-file=\u003cfilename\u003e\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.2.24 Ensure that the API Server --service-account-key-file argument is set as appropriate","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126663/recommendations/1838669"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-apiserver\n\n```\n Verify that the `--service-account-key-file` argument exists and is set as appropriate.","configuration":[]},{"controlDetailsID":"C-0137","name":"Ensure that the API Server --etcd-certfile and --etcd-keyfile arguments are set as appropriate","description":"etcd should be configured to make use of TLS encryption for client connections.","category":"Control plane","subCategory":"","remediation":"Follow the Kubernetes documentation and set up the TLS connection between the apiserver and etcd. Then, edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the master node and set the etcd certificate and key file parameters.\n\n \n```\n--etcd-certfile=\u003cpath/to/client-certificate-file\u003e \n--etcd-keyfile=\u003cpath/to/client-key-file\u003e\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.2.25 Ensure that the API Server --etcd-certfile and --etcd-keyfile arguments are set as appropriate","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126663/recommendations/1838670"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-apiserver\n\n```\n Verify that the `--etcd-certfile` and `--etcd-keyfile` arguments exist and they are set as appropriate.","configuration":[]},{"controlDetailsID":"C-0138","name":"Ensure that the API Server --tls-cert-file and --tls-private-key-file arguments are set as appropriate","description":"Setup TLS connection on the API server.","category":"Control plane","subCategory":"","remediation":"Follow the Kubernetes documentation and set up the TLS connection on the apiserver. Then, edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the master node and set the TLS certificate and private key file parameters.\n\n \n```\n--tls-cert-file=\u003cpath/to/tls-certificate-file\u003e \n--tls-private-key-file=\u003cpath/to/tls-key-file\u003e\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.2.26 Ensure that the API Server --tls-cert-file and --tls-private-key-file arguments are set as appropriate","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126663/recommendations/1838671"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-apiserver\n\n```\n Verify that the `--tls-cert-file` and `--tls-private-key-file` arguments exist and they are set as appropriate.","configuration":[]},{"controlDetailsID":"C-0139","name":"Ensure that the API Server --client-ca-file argument is set as appropriate","description":"Setup TLS connection on the API server.","category":"Control plane","subCategory":"","remediation":"Follow the Kubernetes documentation and set up the TLS connection on the apiserver. Then, edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the master node and set the client certificate authority file.\n\n \n```\n--client-ca-file=\u003cpath/to/client-ca-file\u003e\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.2.27 Ensure that the API Server --client-ca-file argument is set as appropriate","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126663/recommendations/1838672"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-apiserver\n\n```\n Verify that the `--client-ca-file` argument exists and it is set as appropriate.","configuration":[]},{"controlDetailsID":"C-0140","name":"Ensure that the API Server --etcd-cafile argument is set as appropriate","description":"etcd should be configured to make use of TLS encryption for client connections.","category":"Control plane","subCategory":"","remediation":"Follow the Kubernetes documentation and set up the TLS connection between the apiserver and etcd. Then, edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the master node and set the etcd certificate authority file parameter.\n\n \n```\n--etcd-cafile=\u003cpath/to/ca-file\u003e\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.2.28 Ensure that the API Server --etcd-cafile argument is set as appropriate","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126663/recommendations/1838673"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-apiserver\n\n```\n Verify that the `--etcd-cafile` argument exists and it is set as appropriate.","configuration":[]},{"controlDetailsID":"C-0141","name":"Ensure that the API Server --encryption-provider-config argument is set as appropriate","description":"Encrypt etcd key-value store.","category":"Control plane","subCategory":"","remediation":"Follow the Kubernetes documentation and configure a `EncryptionConfig` file. Then, edit the API server pod specification file `/etc/kubernetes/manifests/kube-apiserver.yaml` on the master node and set the `--encryption-provider-config` parameter to the path of that file:\n\n \n```\n--encryption-provider-config=\u003c/path/to/EncryptionConfig/File\u003e\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.2.29 Ensure that the API Server --encryption-provider-config argument is set as appropriate","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126663/recommendations/1838674"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-apiserver\n\n```\n Verify that the `--encryption-provider-config` argument is set to a `EncryptionConfig` file. Additionally, ensure that the `EncryptionConfig` file has all the desired `resources` covered especially any secrets.","configuration":[]},{"controlDetailsID":"C-0142","name":"Ensure that encryption providers are appropriately configured","description":"Where `etcd` encryption is used, appropriate providers should be configured.","category":"Control plane","subCategory":"","remediation":"Follow the Kubernetes documentation and configure a `EncryptionConfig` file. In this file, choose `aescbc`, `kms` or `secretbox` as the encryption provider.","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.2.30 Ensure that encryption providers are appropriately configured","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126663/recommendations/1838675"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-apiserver\n\n```\n Get the `EncryptionConfig` file set for `--encryption-provider-config` argument. Verify that `aescbc`, `kms` or `secretbox` is set as the encryption provider for all the desired `resources`.","configuration":[]},{"controlDetailsID":"C-0143","name":"Ensure that the API Server only makes use of Strong Cryptographic Ciphers","description":"Ensure that the API server is configured to only use strong cryptographic ciphers.","category":"Control plane","subCategory":"","remediation":"Edit the API server pod specification file /etc/kubernetes/manifests/kube-apiserver.yaml on the Control Plane node and set the below parameter.\n\n \n```\n--tls-cipher-suites=TLS_AES_128_GCM_SHA256, TLS_AES_256_GCM_SHA384, TLS_CHACHA20_POLY1305_SHA256, TLS_ECDHE_ECDSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_ECDSA_WITH_AES_256_CBC_SHA, TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305, TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305_SHA256, TLS_ECDHE_RSA_WITH_3DES_EDE_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_128_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256, TLS_ECDHE_RSA_WITH_AES_256_CBC_SHA, TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384, TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305, TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305_SHA256, TLS_RSA_WITH_3DES_EDE_CBC_SHA, TLS_RSA_WITH_AES_128_CBC_SHA, TLS_RSA_WITH_AES_128_GCM_SHA256, TLS_RSA_WITH_AES_256_CBC_SHA, TLS_RSA_WITH_AES_256_GCM_SHA384.\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.2.31 Ensure that the API Server only makes use of Strong Cryptographic Ciphers","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126663/recommendations/1838676"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-apiserver\n\n```\n Verify that the `--tls-cipher-suites` argument is set as outlined in the remediation procedure below.","configuration":[]},{"controlDetailsID":"C-0144","name":"Ensure that the Controller Manager --terminated-pod-gc-threshold argument is set as appropriate","description":"Activate garbage collector on pod termination, as appropriate.","category":"Control plane","subCategory":"","remediation":"Edit the Controller Manager pod specification file `/etc/kubernetes/manifests/kube-controller-manager.yaml` on the Control Plane node and set the `--terminated-pod-gc-threshold` to an appropriate threshold, for example:\n\n \n```\n--terminated-pod-gc-threshold=10\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":4,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.3.1 Ensure that the Controller Manager --terminated-pod-gc-threshold argument is set as appropriate","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126669/recommendations/1838677"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-controller-manager\n\n```\n Verify that the `--terminated-pod-gc-threshold` argument is set as appropriate.","configuration":[]},{"controlDetailsID":"C-0145","name":"Ensure that the Controller Manager --profiling argument is set to false","description":"Disable profiling, if not needed.","category":"Control plane","subCategory":"","remediation":"Edit the Controller Manager pod specification file `/etc/kubernetes/manifests/kube-controller-manager.yaml` on the Control Plane node and set the below parameter.\n\n \n```\n--profiling=false\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":3,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.3.2 Ensure that the Controller Manager --profiling argument is set to false","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126669/recommendations/1838678"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-controller-manager\n\n```\n Verify that the `--profiling` argument is set to `false`.","configuration":[]},{"controlDetailsID":"C-0146","name":"Ensure that the Controller Manager --use-service-account-credentials argument is set to true","description":"Use individual service account credentials for each controller.","category":"Control plane","subCategory":"","remediation":"Edit the Controller Manager pod specification file `/etc/kubernetes/manifests/kube-controller-manager.yaml` on the Control Plane node to set the below parameter.\n\n \n```\n--use-service-account-credentials=true\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":4,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.3.3 Ensure that the Controller Manager --use-service-account-credentials argument is set to true","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126669/recommendations/1838679"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-controller-manager\n\n```\n Verify that the `--use-service-account-credentials` argument is set to `true`.","configuration":[]},{"controlDetailsID":"C-0147","name":"Ensure that the Controller Manager --service-account-private-key-file argument is set as appropriate","description":"Explicitly set a service account private key file for service accounts on the controller manager.","category":"Control plane","subCategory":"","remediation":"Edit the Controller Manager pod specification file `/etc/kubernetes/manifests/kube-controller-manager.yaml` on the Control Plane node and set the `--service-account-private-key-file` parameter to the private key file for service accounts.\n\n \n```\n--service-account-private-key-file=\u003cfilename\u003e\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.3.4 Ensure that the Controller Manager --service-account-private-key-file argument is set as appropriate","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126669/recommendations/1838680"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-controller-manager\n\n```\n Verify that the `--service-account-private-key-file` argument is set as appropriate.","configuration":[]},{"controlDetailsID":"C-0148","name":"Ensure that the Controller Manager --root-ca-file argument is set as appropriate","description":"Allow pods to verify the API server's serving certificate before establishing connections.","category":"Control plane","subCategory":"","remediation":"Edit the Controller Manager pod specification file `/etc/kubernetes/manifests/kube-controller-manager.yaml` on the Control Plane node and set the `--root-ca-file` parameter to the certificate bundle file`.\n\n \n```\n--root-ca-file=\u003cpath/to/file\u003e\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.3.5 Ensure that the Controller Manager --root-ca-file argument is set as appropriate","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126669/recommendations/1838681"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-controller-manager\n\n```\n Verify that the `--root-ca-file` argument exists and is set to a certificate bundle file containing the root certificate for the API server's serving certificate.","configuration":[]},{"controlDetailsID":"C-0149","name":"Ensure that the Controller Manager RotateKubeletServerCertificate argument is set to true","description":"Enable kubelet server certificate rotation on controller-manager.","category":"Control plane","subCategory":"","remediation":"Edit the Controller Manager pod specification file `/etc/kubernetes/manifests/kube-controller-manager.yaml` on the Control Plane node and set the `--feature-gates` parameter to include `RotateKubeletServerCertificate=true`.\n\n \n```\n--feature-gates=RotateKubeletServerCertificate=true\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.3.6 Ensure that the Controller Manager RotateKubeletServerCertificate argument is set to true","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126669/recommendations/1838682"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-controller-manager\n\n```\n Verify that `RotateKubeletServerCertificate` argument exists and is set to `true`.","configuration":[]},{"controlDetailsID":"C-0150","name":"Ensure that the Controller Manager --bind-address argument is set to 127.0.0.1","description":"Do not bind the Controller Manager service to non-loopback insecure addresses.","category":"Control plane","subCategory":"","remediation":"Edit the Controller Manager pod specification file `/etc/kubernetes/manifests/kube-controller-manager.yaml` on the Control Plane node and ensure the correct value for the `--bind-address` parameter","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.3.7 Ensure that the Controller Manager --bind-address argument is set to 127.0.0.1","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126669/recommendations/1838683"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-controller-manager\n\n```\n Verify that the `--bind-address` argument is set to 127.0.0.1","configuration":[]},{"controlDetailsID":"C-0151","name":"Ensure that the Scheduler --profiling argument is set to false","description":"Disable profiling, if not needed.","category":"Control plane","subCategory":"","remediation":"Edit the Scheduler pod specification file `/etc/kubernetes/manifests/kube-scheduler.yaml` file on the Control Plane node and set the below parameter.\n\n \n```\n--profiling=false\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":3,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.4.1 Ensure that the Scheduler --profiling argument is set to false","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126670/recommendations/1838684"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-scheduler\n\n```\n Verify that the `--profiling` argument is set to `false`.","configuration":[]},{"controlDetailsID":"C-0152","name":"Ensure that the Scheduler --bind-address argument is set to 127.0.0.1","description":"Do not bind the scheduler service to non-loopback insecure addresses.","category":"Control plane","subCategory":"","remediation":"Edit the Scheduler pod specification file `/etc/kubernetes/manifests/kube-scheduler.yaml` on the Control Plane node and ensure the correct value for the `--bind-address` parameter","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-1.4.2 Ensure that the Scheduler --bind-address argument is set to 127.0.0.1","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126670/recommendations/1838685"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the Control Plane node:\n\n \n```\nps -ef | grep kube-scheduler\n\n```\n Verify that the `--bind-address` argument is set to 127.0.0.1","configuration":[]},{"controlDetailsID":"C-0153","name":"Ensure that the --cert-file and --key-file arguments are set as appropriate","description":"Configure TLS encryption for the etcd service.","category":"Control plane","subCategory":"","remediation":"Follow the etcd service documentation and configure TLS encryption.\n\n Then, edit the etcd pod specification file `/etc/kubernetes/manifests/etcd.yaml` on the master node and set the below parameters.\n\n \n```\n--cert-file=\u003c/path/to/ca-file\u003e\n--key-file=\u003c/path/to/key-file\u003e\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-2.1 Ensure that the --cert-file and --key-file arguments are set as appropriate","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126654/recommendations/1838562"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the etcd server node\n\n \n```\nps -ef | grep etcd\n\n```\n Verify that the `--cert-file` and the `--key-file` arguments are set as appropriate.","configuration":[]},{"controlDetailsID":"C-0154","name":"Ensure that the --client-cert-auth argument is set to true","description":"Enable client authentication on etcd service.","category":"Control plane","subCategory":"","remediation":"Edit the etcd pod specification file `/etc/kubernetes/manifests/etcd.yaml` on the master node and set the below parameter.\n\n \n```\n--client-cert-auth=\"true\"\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-2.2 Ensure that the --client-cert-auth argument is set to true","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126654/recommendations/1838565"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the etcd server node:\n\n \n```\nps -ef | grep etcd\n\n```\n Verify that the `--client-cert-auth` argument is set to `true`.","configuration":[]},{"controlDetailsID":"C-0155","name":"Ensure that the --auto-tls argument is not set to true","description":"Do not use self-signed certificates for TLS.","category":"Control plane","subCategory":"","remediation":"Edit the etcd pod specification file `/etc/kubernetes/manifests/etcd.yaml` on the master node and either remove the `--auto-tls` parameter or set it to `false`.\n\n \n```\n--auto-tls=false\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-2.3 Ensure that the --auto-tls argument is not set to true","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126654/recommendations/1838567"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the etcd server node:\n\n \n```\nps -ef | grep etcd\n\n```\n Verify that if the `--auto-tls` argument exists, it is not set to `true`.","configuration":[]},{"controlDetailsID":"C-0156","name":"Ensure that the --peer-cert-file and --peer-key-file arguments are set as appropriate","description":"etcd should be configured to make use of TLS encryption for peer connections.","category":"Control plane","subCategory":"","remediation":"Follow the etcd service documentation and configure peer TLS encryption as appropriate for your etcd cluster.\n\n Then, edit the etcd pod specification file `/etc/kubernetes/manifests/etcd.yaml` on the master node and set the below parameters.\n\n \n```\n--peer-client-file=\u003c/path/to/peer-cert-file\u003e\n--peer-key-file=\u003c/path/to/peer-key-file\u003e\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-2.4 Ensure that the --peer-cert-file and --peer-key-file arguments are set as appropriate","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126654/recommendations/1838569"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the etcd server node:\n\n \n```\nps -ef | grep etcd\n\n```\n Verify that the `--peer-cert-file` and `--peer-key-file` arguments are set as appropriate.\n\n **Note:** This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable.","configuration":[]},{"controlDetailsID":"C-0157","name":"Ensure that the --peer-client-cert-auth argument is set to true","description":"etcd should be configured for peer authentication.","category":"Control plane","subCategory":"","remediation":"Edit the etcd pod specification file `/etc/kubernetes/manifests/etcd.yaml` on the master node and set the below parameter. ```--peer-client-cert-auth=true```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-2.5 Ensure that the --peer-client-cert-auth argument is set to true","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126654/recommendations/1838572"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the etcd server node: ```ps -ef | grep etcd``` Verify that the `--peer-client-cert-auth` argument is set to `true`. **Note:** This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable.","configuration":[]},{"controlDetailsID":"C-0158","name":"Ensure that the --peer-auto-tls argument is not set to true","description":"Do not use automatically generated self-signed certificates for TLS connections between peers.","category":"Control plane","subCategory":"","remediation":"Edit the etcd pod specification file `/etc/kubernetes/manifests/etcd.yaml` on the master node and either remove the `--peer-auto-tls` parameter or set it to `false`.\n\n \n```\n--peer-auto-tls=false\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-2.6 Ensure that the --peer-auto-tls argument is not set to true","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126654/recommendations/1838575"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on the etcd server node:\n\n \n```\nps -ef | grep etcd\n\n```\n Verify that if the `--peer-auto-tls` argument exists, it is not set to `true`.\n**Note:** This recommendation is applicable only for etcd clusters. If you are using only one etcd server in your environment then this recommendation is not applicable.","configuration":[]},{"controlDetailsID":"C-0159","name":"Ensure that a unique Certificate Authority is used for etcd","description":"Use a different certificate authority for etcd from the one used for Kubernetes.","category":"Control plane","subCategory":"","remediation":"Follow the etcd documentation and create a dedicated certificate authority setup for the etcd service.\n\n Then, edit the etcd pod specification file `/etc/kubernetes/manifests/etcd.yaml` on the master node and set the below parameter.\n\n \n```\n--trusted-ca-file=\u003c/path/to/ca-file\u003e\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-2.7 Ensure that a unique Certificate Authority is used for etcd","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126654/recommendations/1838578"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Review the CA used by the etcd environment and ensure that it does not match the CA certificate file used for the management of the overall Kubernetes cluster.\n\n Run the following command on the master node:\n\n \n```\nps -ef | grep etcd\n\n```\n Note the file referenced by the `--trusted-ca-file` argument.\n\n Run the following command on the master node:\n\n \n```\nps -ef | grep apiserver\n\n```\n Verify that the file referenced by the `--client-ca-file` for apiserver is different from the `--trusted-ca-file` used by etcd.","configuration":[]},{"controlDetailsID":"C-0160","name":"Ensure that a minimal audit policy is created","description":"Kubernetes can audit the details of requests made to the API server. The `--audit-policy-file` flag must be set for this logging to be enabled.","category":"Control plane","subCategory":"","remediation":"Create an audit policy file for your cluster.","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-3.2.1 Ensure that a minimal audit policy is created","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126657/recommendations/1838582"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod"],"testCriteria":"","manualCheck":"Run the following command on one of the cluster master nodes:\n\n \n```\nps -ef | grep kube-apiserver\n\n```\n Verify that the `--audit-policy-file` is set. Review the contents of the file specified and ensure that it contains a valid audit policy.","configuration":[]},{"controlDetailsID":"C-0161","name":"Ensure that the audit policy covers key security concerns","description":"Ensure that the audit policy created for the cluster covers key security concerns.","category":"Control plane","subCategory":"","remediation":"Consider modification of the audit policy in use on the cluster to include these items, at a minimum.","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-3.2.2 Ensure that the audit policy covers key security concerns","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126657/recommendations/1838583"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["APIServerInfo"],"testCriteria":"","manualCheck":"Review the audit policy provided for the cluster and ensure that it covers at least the following areas :-\n\n * Access to Secrets managed by the cluster. Care should be taken to only log Metadata for requests to Secrets, ConfigMaps, and TokenReviews, in order to avoid the risk of logging sensitive data.\n* Modification of `pod` and `deployment` objects.\n* Use of `pods/exec`, `pods/portforward`, `pods/proxy` and `services/proxy`.\n\n For most requests, minimally logging at the Metadata level is recommended (the most basic level of logging).","configuration":[]},{"controlDetailsID":"C-0162","name":"Ensure that the kubelet service file permissions are set to 600 or more restrictive","description":"Ensure that the `kubelet` service file has permissions of `600` or more restrictive.","category":"Control plane","subCategory":"","remediation":"Run the below command (based on the file location on your system) on the each worker node. For example,\n\n \n```\nchmod 600 /etc/systemd/system/kubelet.service.d/kubeadm.conf\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-4.1.1 Ensure that the kubelet service file permissions are set to 600 or more restrictive","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126659/recommendations/1838585"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the below command (based on the file location on your system) on the each worker node. For example,\n\n \n```\nstat -c %a /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\n\n```\n Verify that the permissions are `600` or more restrictive.","configuration":[]},{"controlDetailsID":"C-0163","name":"Ensure that the kubelet service file ownership is set to root:root","description":"Ensure that the `kubelet` service file ownership is set to `root:root`.","category":"Control plane","subCategory":"","remediation":"Run the below command (based on the file location on your system) on the each worker node. For example,\n\n \n```\nchown root:root /etc/systemd/system/kubelet.service.d/kubeadm.conf\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-4.1.2 Ensure that the kubelet service file ownership is set to root:root","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126659/recommendations/1838589"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the below command (based on the file location on your system) on the each worker node. For example,\n\n \n```\nstat -c %a /etc/systemd/system/kubelet.service.d/10-kubeadm.conf\n\n```\n Verify that the ownership is set to `root:root`.","configuration":[]},{"controlDetailsID":"C-0164","name":"If proxy kubeconfig file exists ensure permissions are set to 600 or more restrictive","description":"If `kube-proxy` is running, and if it is using a file-based kubeconfig file, ensure that the proxy kubeconfig file has permissions of `600` or more restrictive.","category":"Control plane","subCategory":"","remediation":"Run the below command (based on the file location on your system) on the each worker node. For example,\n\n \n```\nchmod 600 \u003cproxy kubeconfig file\u003e\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-4.1.3 If proxy kubeconfig file exists ensure permissions are set to 600 or more restrictive","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126659/recommendations/1838598"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Find the kubeconfig file being used by `kube-proxy` by running the following command:\n\n \n```\nps -ef | grep kube-proxy\n\n```\n If `kube-proxy` is running, get the kubeconfig file location from the `--kubeconfig` parameter.\n\n To perform the audit:\n\n Run the below command (based on the file location on your system) on the each worker node. For example,\n\n \n```\nstat -c %a \u003cpath\u003e\u003cfilename\u003e\n\n```\n Verify that a file is specified and it exists with permissions are `600` or more restrictive.","configuration":[]},{"controlDetailsID":"C-0165","name":"If proxy kubeconfig file exists ensure ownership is set to root:root","description":"If `kube-proxy` is running, ensure that the file ownership of its kubeconfig file is set to `root:root`.","category":"Control plane","subCategory":"","remediation":"Run the below command (based on the file location on your system) on the each worker node. For example,\n\n \n```\nchown root:root \u003cproxy kubeconfig file\u003e\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-4.1.4 If proxy kubeconfig file exists ensure ownership is set to root:root","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126659/recommendations/1838603"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Find the kubeconfig file being used by `kube-proxy` by running the following command:\n\n \n```\nps -ef | grep kube-proxy\n\n```\n If `kube-proxy` is running, get the kubeconfig file location from the `--kubeconfig` parameter.\n\n To perform the audit:\n\n Run the below command (based on the file location on your system) on the each worker node. For example,\n\n \n```\nstat -c %U:%G \u003cpath\u003e\u003cfilename\u003e\n\n```\n Verify that the ownership is set to `root:root`.","configuration":[]},{"controlDetailsID":"C-0166","name":"Ensure that the --kubeconfig kubelet.conf file permissions are set to 600 or more restrictive","description":"Ensure that the `kubelet.conf` file has permissions of `600` or more restrictive.","category":"Control plane","subCategory":"","remediation":"Run the below command (based on the file location on your system) on the each worker node. For example,\n\n \n```\nchmod 600 /etc/kubernetes/kubelet.conf\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-4.1.5 Ensure that the --kubeconfig kubelet.conf file permissions are set to 600 or more restrictive","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126659/recommendations/1838607"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the below command (based on the file location on your system) on the each worker node. For example,\n\n \n```\nstat -c %a /etc/kubernetes/kubelet.conf\n\n```\n Verify that the ownership is set to `root:root`.Verify that the permissions are `600` or more restrictive.","configuration":[]},{"controlDetailsID":"C-0167","name":"Ensure that the --kubeconfig kubelet.conf file ownership is set to root:root","description":"Ensure that the `kubelet.conf` file ownership is set to `root:root`.","category":"Control plane","subCategory":"","remediation":"Run the below command (based on the file location on your system) on the each worker node. For example,\n\n \n```\nchown root:root /etc/kubernetes/kubelet.conf\n\n```","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0","cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-3.1.2 Ensure that the kubelet kubeconfig file ownership is set to root:root","description":"If `kubelet` is running, ensure that the file ownership of its kubeconfig file is set to `root:root`.","long_description":"The kubeconfig file for `kubelet` controls various parameters for the `kubelet` service in the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by `root:root`.","remediation":"Run the below command (based on the file location on your system) on each worker node. For example,\n\n \n```\nchown root:root \u003cproxy kubeconfig file\u003e\n\n```","references":["\u003chttps://kubernetes.io/docs/admin/kube-proxy/\u003e\n\n  \u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-posture-vulnerability-management#pv-3-establish-secure-configurations-for-compute-resources\u003e"],"manualCheck":"SSH to the worker nodes\n\n To check to see if the Kubelet Service is running:\n\n \n```\nsudo systemctl status kubelet\n\n```\n The output should return `Active: active (running) since..`\n\n Run the following command on each node to find the appropriate kubeconfig file:\n\n \n```\nps -ef | grep kubelet\n\n```\n The output of the above command should return something similar to `--kubeconfig /var/lib/kubelet/kubeconfig` which is the location of the kubeconfig file.\n\n Run this command to obtain the kubeconfig file ownership:\n\n \n```\nstat -c %U:%G /var/lib/kubelet/kubeconfig\n\n```\n The output of the above command gives you the kubeconfig file's ownership. Verify that the ownership is set to `root:root`.","impactStatement":"","defaultValue":"See the Azure AKS documentation for the default value."},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-3.1.2 Ensure that the kubelet kubeconfig file ownership is set to root:root","description":"If `kubelet` is running, ensure that the file ownership of its kubeconfig file is set to `root:root`.","long_description":"The kubeconfig file for `kubelet` controls various parameters for the `kubelet` service in the worker node. You should set its file ownership to maintain the integrity of the file. The file should be owned by `root:root`.","remediation":"Run the below command (based on the file location on your system) on each worker node.\n\n For example,\n\n \n```\nchown root:root \u003cproxy kubeconfig file\u003e\n\n```","references":["https://kubernetes.io/docs/admin/kube-proxy/"],"manualCheck":"SSH to the worker nodes\n\n To check to see if the Kubelet Service is running:\n\n \n```\nsudo systemctl status kubelet\n\n```\n The output should return `Active: active (running) since..`\n\n Run the following command on each node to find the appropriate kubeconfig file:\n\n \n```\nps -ef | grep kubelet\n\n```\n The output of the above command should return something similar to `--kubeconfig /var/lib/kubelet/kubeconfig` which is the location of the kubeconfig file.\n\n Run this command to obtain the kubeconfig file ownership:\n\n \n```\nstat -c %U:%G /var/lib/kubelet/kubeconfig\n\n```\n The output of the above command gives you the kubeconfig file's ownership. Verify that the ownership is set to `root:root`.","impactStatement":"","defaultValue":"See the AWS EKS documentation for the default value."},{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-4.1.6 Ensure that the --kubeconfig kubelet.conf file ownership is set to root:root","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126659/recommendations/1838613"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the below command (based on the file location on your system) on the each worker node. For example,\n\n \n```\nstat -c %U %G /etc/kubernetes/kubelet.conf\n\n```\n Verify that the ownership is set to `root:root`.","configuration":[]},{"controlDetailsID":"C-0168","name":"Ensure that the certificate authorities file permissions are set to 600 or more restrictive","description":"Ensure that the certificate authorities file has permissions of `600` or more restrictive.","category":"Control plane","subCategory":"","remediation":"Run the following command to modify the file permissions of the `--client-ca-file`\n\n \n```\nchmod 600 \u003cfilename\u003e\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-4.1.7 Ensure that the certificate authorities file permissions are set to 600 or more restrictive","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126659/recommendations/1838618"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the following command:\n\n \n```\nps -ef | grep kubelet\n\n```\n Find the file specified by the `--client-ca-file` argument.\n\n Run the following command:\n\n \n```\nstat -c %a \u003cfilename\u003e\n\n```\n Verify that the permissions are `644` or more restrictive.","configuration":[]},{"controlDetailsID":"C-0169","name":"Ensure that the client certificate authorities file ownership is set to root:root","description":"Ensure that the certificate authorities file ownership is set to `root:root`.","category":"Control plane","subCategory":"","remediation":"Run the following command to modify the ownership of the `--client-ca-file`.\n\n \n```\nchown root:root \u003cfilename\u003e\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-4.1.8 Ensure that the client certificate authorities file ownership is set to root:root","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126659/recommendations/1838619"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the following command:\n\n \n```\nps -ef | grep kubelet\n\n```\n Find the file specified by the `--client-ca-file` argument.\n\n Run the following command:\n\n \n```\nstat -c %U:%G \u003cfilename\u003e\n\n```\n Verify that the ownership is set to `root:root`.","configuration":[]},{"controlDetailsID":"C-0170","name":"If the kubelet config.yaml configuration file is being used validate permissions set to 600 or more restrictive","description":"Ensure that if the kubelet refers to a configuration file with the `--config` argument, that file has permissions of 600 or more restrictive.","category":"Control plane","subCategory":"","remediation":"Run the following command (using the config file location identied in the Audit step)\n\n \n```\nchmod 600 /var/lib/kubelet/config.yaml\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-4.1.9 If the kubelet config.yaml configuration file is being used validate permissions set to 600 or more restrictive","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126659/recommendations/1838620"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the below command (based on the file location on your system) on the each worker node. For example,\n\n \n```\nstat -c %a /var/lib/kubelet/config.yaml\n\n```\n Verify that the permissions are `600` or more restrictive.","configuration":[]},{"controlDetailsID":"C-0171","name":"If the kubelet config.yaml configuration file is being used validate file ownership is set to root:root","description":"Ensure that if the kubelet refers to a configuration file with the `--config` argument, that file is owned by root:root.","category":"Control plane","subCategory":"","remediation":"Run the following command (using the config file location identied in the Audit step)\n\n \n```\nchown root:root /etc/kubernetes/kubelet.conf\n\n```","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0","cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-3.1.4 Ensure that the kubelet configuration file ownership is set to root:root","description":"","long_description":"The kubelet reads various parameters, including security settings, from a config file specified by the `--config` argument. If this file is specified you should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system.","remediation":"Run the following command (using the config file location identified in the Audit step)\n\n \n```\nchown root:root /etc/kubernetes/kubelet/kubelet-config.json\n\n```","references":["\u003chttps://kubernetes.io/docs/admin/kube-proxy/\u003e\n\n  \u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-posture-vulnerability-management#pv-3-establish-secure-configurations-for-compute-resources\u003e"],"manualCheck":"First, SSH to the relevant worker node:\n\n To check to see if the Kubelet Service is running:\n\n \n```\nsudo systemctl status kubelet\n\n```\n The output should return `Active: active (running) since..`\n\n Run the following command on each node to find the appropriate Kubelet config file:\n\n \n```\nps -ef | grep kubelet\n\n```\n The output of the above command should return something similar to `--config /etc/kubernetes/kubelet/kubelet-config.json` which is the location of the Kubelet config file.\n\n Run the following command:\n\n \n```\nstat -c %U:%G /etc/kubernetes/kubelet/kubelet-config.json\n\n```\n The output of the above command is the Kubelet config file's ownership. Verify that the ownership is set to `root:root`","impactStatement":"","defaultValue":"See the Azure AKS documentation for the default value."},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-3.1.4 Ensure that the kubelet configuration file ownership is set to root:root","description":"","long_description":"The kubelet reads various parameters, including security settings, from a config file specified by the `--config` argument. If this file is specified you should restrict its file permissions to maintain the integrity of the file. The file should be writable by only the administrators on the system.","remediation":"Run the following command (using the config file location identified in the Audit step)\n\n \n```\nchown root:root /etc/kubernetes/kubelet/kubelet-config.json\n\n```","references":["https://kubernetes.io/docs/admin/kube-proxy/"],"manualCheck":"First, SSH to the relevant worker node:\n\n To check to see if the Kubelet Service is running:\n\n \n```\nsudo systemctl status kubelet\n\n```\n The output should return `Active: active (running) since..`\n\n Run the following command on each node to find the appropriate Kubelet config file:\n\n \n```\nps -ef | grep kubelet\n\n```\n The output of the above command should return something similar to `--config /etc/kubernetes/kubelet/kubelet-config.json` which is the location of the Kubelet config file.\n\n Run the following command:\n\n \n```\nstat -c %U:%G /etc/kubernetes/kubelet/kubelet-config.json\n\n```\n The output of the above command is the Kubelet config file's ownership. Verify that the ownership is set to `root:root`","impactStatement":"","defaultValue":"See the AWS EKS documentation for the default value."},{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-4.1.10 If the kubelet config.yaml configuration file is being used validate file ownership is set to root:root","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126659/recommendations/1838629"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the below command (based on the file location on your system) on the each worker node. For example,\n\n \n```\nstat -c %a /var/lib/kubelet/config.yaml\n```Verify that the ownership is set to `root:root`.\n\n```","configuration":[]},{"controlDetailsID":"C-0172","name":"Ensure that the --anonymous-auth argument is set to false","description":"Disable anonymous requests to the Kubelet server.","category":"Control plane","subCategory":"","remediation":"If using a Kubelet config file, edit the file to set `authentication: anonymous: enabled` to `false`.\n\n If using executable arguments, edit the kubelet service file `/etc/kubernetes/kubelet.conf` on each worker node and set the below parameter in `KUBELET_SYSTEM_PODS_ARGS` variable.\n\n \n```\n--anonymous-auth=false\n\n```\n Based on your system, restart the `kubelet` service. For example:\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n\n```","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0","cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-3.2.1 Ensure that the --anonymous-auth argument is set to false","description":"","long_description":"","remediation":"**Remediation Method 1:**\n\n If modifying the Kubelet config file, edit the kubelet-config.json file `/etc/kubernetes/kubelet/kubelet-config.json` and set the below parameter to false\n\n \n```\n\"anonymous\": \"enabled\": false\n\n```\n **Remediation Method 2:**\n\n If using executable arguments, edit the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` on each worker node and add the below parameter at the end of the `KUBELET_ARGS` variable string.\n\n \n```\n--anonymous-auth=false\n\n```\n **Remediation Method 3:**\n\n If using the api configz endpoint consider searching for the status of `\"authentication.*anonymous\":{\"enabled\":false}\"` by extracting the live configuration from the nodes running kubelet.\\*\\*See detailed step-by-step configmap procedures in[Reconfigure a Node's Kubelet in a Live Cluster](https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/), and then rerun the curl statement from audit process to check for kubelet configuration changes\n\n \n```\nkubectl proxy --port=8001 \u0026\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"\n\n```\n **For all three remediations:**\nBased on your system, restart the `kubelet` service and check status\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\nsystemctl status kubelet -l\n\n```","references":["\u003chttps://kubernetes.io/docs/admin/kubelet/\u003e\n\n  \u003chttps://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet-authentication\u003e\n\n  \u003chttps://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/\u003e\n\n  \u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-governance-strategy#gs-6-define-identity-and-privileged-access-strategy\u003e"],"manualCheck":"**Audit Method 1:**\n\n If using a Kubelet configuration file, check that there is an entry for `authentication: anonymous: enabled` set to `false`.\n\n First, SSH to the relevant node:\n\n Run the following command on each node to find the appropriate Kubelet config file:\n\n \n```\nps -ef | grep kubelet\n\n```\n The output of the above command should return something similar to `--config /etc/kubernetes/kubelet/kubelet-config.json` which is the location of the Kubelet config file.\n\n Open the Kubelet config file:\n\n \n```\nsudo more /etc/kubernetes/kubelet/kubelet-config.json\n\n```\n Verify that the `\"authentication\": { \"anonymous\": { \"enabled\": false }` argument is set to `false`.\n\n **Audit Method 2:**\n\n If using the api configz endpoint consider searching for the status of `authentication... \"anonymous\":{\"enabled\":false}` by extracting the live configuration from the nodes running kubelet.\n\n Set the local proxy port and the following variables and provide proxy port number and node name;\n`HOSTNAME_PORT=\"localhost-and-port-number\"` `NODE_NAME=\"The-Name-Of-Node-To-Extract-Configuration\" from the output of \"kubectl get nodes\"`\n\n \n```\nkubectl proxy --port=8001 \u0026\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"\n\n```","impactStatement":"","defaultValue":"See the Azure AKS documentation for the default value."},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-3.2.1 Ensure that the Anonymous Auth is Not Enabled","description":"","long_description":"","remediation":"**Remediation Method 1:**\n\n If configuring via the Kubelet config file, you first need to locate the file.\n\n To do this, SSH to each node and execute the following command to find the kubelet process:\n\n \n```\nps -ef | grep kubelet\n\n```\n The output of the above command provides details of the active kubelet process, from which we can see the location of the configuration file provided to the kubelet service with the `--config` argument. The file can be viewed with a command such as `more` or `less`, like so:\n\n \n```\nsudo less /path/to/kubelet-config.json\n\n```\n Disable Anonymous Authentication by setting the following parameter:\n\n \n```\n\"authentication\": { \"anonymous\": { \"enabled\": false } }\n\n```\n **Remediation Method 2:**\n\n If using executable arguments, edit the kubelet service file on each worker node and ensure the below parameters are part of the `KUBELET_ARGS` variable string.\n\n For systems using `systemd`, such as the Amazon EKS Optimised Amazon Linux or Bottlerocket AMIs, then this file can be found at `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf`. Otherwise, you may need to look up documentation for your chosen operating system to determine which service manager is configured:\n\n \n```\n--anonymous-auth=false\n\n```\n **For Both Remediation Steps:**\n\n Based on your system, restart the `kubelet` service and check the service status.\n\n The following example is for operating systems using `systemd`, such as the Amazon EKS Optimised Amazon Linux or Bottlerocket AMIs, and invokes the `systemctl` command. If `systemctl` is not available then you will need to look up documentation for your chosen operating system to determine which service manager is configured:\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\nsystemctl status kubelet -l\n\n```","references":["https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/","https://kubernetes.io/docs/reference/access-authn-authz/kubelet-authn-authz/#kubelet-authentication","https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/"],"manualCheck":"**Audit Method 1:**Kubelets can accept configuration via a configuration file and in some cases via command line arguments. It is important to note that parameters provided as command line arguments will override their counterpart parameters in the configuration file (see `--config` details in the[Kubelet CLI Reference](https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/) for more info, where you can also find out which configuration parameters can be supplied as a command line argument).\n\n With this in mind, it is important to check for the existence of command line arguments as well as configuration file entries when auditing Kubelet configuration.\n\n Firstly, SSH to each node and execute the following command to find the Kubelet process:\n\n \n```\nps -ef | grep kubelet\n\n```\n The output of the above command provides details of the active Kubelet process, from which we can see the command line arguments provided to the process. Also note the location of the configuration file, provided with the `--config` argument, as this will be needed to verify configuration. The file can be viewed with a command such as `more` or `less`, like so:\n\n \n```\nsudo less /path/to/kubelet-config.json\n\n```\n Verify that Anonymous Authentication is not enabled. This may be configured as a command line argument to the kubelet service with `--anonymous-auth=false` or in the kubelet configuration file via `\"authentication\": { \"anonymous\": { \"enabled\": false }`.\n\n **Audit Method 2:**\n\n It is also possible to review the running configuration of a Kubelet via the /configz endpoint of the Kubernetes API. This can be achieved using `kubectl` to proxy your requests to the API.\n\n Discover all nodes in your cluster by running the following command:\n\n \n```\nkubectl get nodes\n\n```\n Next, initiate a proxy with `kubectl` on a local port of your choice. In this example we will use 8080:\n\n \n```\nkubectl proxy --port=8080\n\n```\n With this running, in a separate terminal run the following command for each node:\n\n \n```\nexport NODE_NAME=my-node-name\ncurl http://localhost:8080/api/v1/nodes/${NODE_NAME}/proxy/configz    \n\n```\n The curl command will return the API response which will be a JSON formatted string representing the Kubelet configuration.\n\n Verify that Anonymous Authentication is not enabled checking that `\"authentication\": { \"anonymous\": { \"enabled\": false }` is in the API response.","impactStatement":"","defaultValue":"See the EKS documentation for the default value."},{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-4.2.1 Ensure that the --anonymous-auth argument is set to false","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126668/recommendations/1838638"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"If using a Kubelet configuration file, check that there is an entry for `authentication: anonymous: enabled` set to `false`.\n\n Run the following command on each node:\n\n \n```\nps -ef | grep kubelet\n\n```\n Verify that the `--anonymous-auth` argument is set to `false`.\n\n This executable argument may be omitted, provided there is a corresponding entry set to `false` in the Kubelet config file.","configuration":[]},{"controlDetailsID":"C-0173","name":"Ensure that the --authorization-mode argument is not set to AlwaysAllow","description":"Do not allow all requests. Enable explicit authorization.","category":"Control plane","subCategory":"","remediation":"If using a Kubelet config file, edit the file to set `authorization: mode` to `Webhook`.\n\n If using executable arguments, edit the kubelet service file `/etc/kubernetes/kubelet.conf` on each worker node and set the below parameter in `KUBELET_AUTHZ_ARGS` variable.\n\n \n```\n--authorization-mode=Webhook\n\n```\n Based on your system, restart the `kubelet` service. For example:\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n\n```","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0","cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-3.2.2 Ensure that the --authorization-mode argument is not set to AlwaysAllow","description":"","long_description":"","remediation":"**Remediation Method 1:**\n\n If modifying the Kubelet config file, edit the kubelet-config.json file `/etc/kubernetes/kubelet/kubelet-config.json` and set the below parameter to false\n\n \n```\n\"authentication\"... \"webhook\":{\"enabled\":true\n\n```\n **Remediation Method 2:**\n\n If using executable arguments, edit the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` on each worker node and add the below parameter at the end of the `KUBELET_ARGS` variable string.\n\n \n```\n--authorization-mode=Webhook\n\n```\n **Remediation Method 3:**\n\n If using the api configz endpoint consider searching for the status of `\"authentication.*webhook\":{\"enabled\":true\"` by extracting the live configuration from the nodes running kubelet.\n\n \\*\\*See detailed step-by-step configmap procedures in [Reconfigure a Node's Kubelet in a Live Cluster](https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/), and then rerun the curl statement from audit process to check for kubelet configuration changes\n\n \n```\nkubectl proxy --port=8001 \u0026\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"\n\n```\n **For all three remediations:**\nBased on your system, restart the `kubelet` service and check status\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\nsystemctl status kubelet -l\n\n```","references":["\u003chttps://kubernetes.io/docs/admin/kubelet/\u003e\n\n  \u003chttps://kubernetes.io/docs/admin/kubelet-authentication-authorization/#kubelet-authentication\u003e\n\n  \u003chttps://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/\u003e\n\n  \u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-governance-strategy#gs-6-define-identity-and-privileged-access-strategy\u003e"],"manualCheck":"**Audit Method 1:**\n\n If using a Kubelet configuration file, check that there is an entry for `\"authentication\": \"webhook\": \"enabled\"` set to `true`.\n\n First, SSH to the relevant node:\n\n Run the following command on each node to find the appropriate Kubelet config file:\n\n \n```\nps -ef | grep kubelet\n\n```\n The output of the above command should return something similar to `--config /etc/kubernetes/kubelet/kubelet-config.json` which is the location of the Kubelet config file.\n\n Open the Kubelet config file:\n\n \n```\nsudo more /etc/kubernetes/kubelet/kubelet-config.json\n\n```\n Verify that the `\"authentication\": {\"webhook\": { \"enabled\": is set to true`.\n\n If the `\"authentication\": {\"mode\": {` argument is present check that it is not set to `AlwaysAllow`. If it is not present check that there is a Kubelet config file specified by `--config`, and that file sets `\"authentication\": {\"mode\": {` to something other than `AlwaysAllow`.\n\n **Audit Method 2:**\n\n If using the api configz endpoint consider searching for the status of `authentication... \"webhook\":{\"enabled\":true}` by extracting the live configuration from the nodes running kubelet.\n\n Set the local proxy port and the following variables and provide proxy port number and node name;\n`HOSTNAME_PORT=\"localhost-and-port-number\"` `NODE_NAME=\"The-Name-Of-Node-To-Extract-Configuration\" from the output of \"kubectl get nodes\"`\n\n \n```\nkubectl proxy --port=8001 \u0026\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"\n\n```","impactStatement":"","defaultValue":"See the Azure AKS documentation for the default value."},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-3.2.2 Ensure that the --authorization-mode argument is not set to AlwaysAllow","description":"","long_description":"Kubelets can be configured to allow all authenticated requests (even anonymous ones) without needing explicit authorization checks from the apiserver. You should restrict this behavior and only allow explicitly authorized requests.","remediation":"**Remediation Method 1:**\n\n If configuring via the Kubelet config file, you first need to locate the file.\n\n To do this, SSH to each node and execute the following command to find the kubelet process:\n\n \n```\nps -ef | grep kubelet\n\n```\n The output of the above command provides details of the active kubelet process, from which we can see the location of the configuration file provided to the kubelet service with the `--config` argument. The file can be viewed with a command such as `more` or `less`, like so:\n\n \n```\nsudo less /path/to/kubelet-config.json\n\n```\n Enable Webhook Authentication by setting the following parameter:\n\n \n```\n\"authentication\": { \"webhook\": { \"enabled\": true } }\n\n```\n Next, set the Authorization Mode to `Webhook` by setting the following parameter:\n\n \n```\n\"authorization\": { \"mode\": \"Webhook }\n\n```\n Finer detail of the `authentication` and `authorization` fields can be found in the [Kubelet Configuration documentation](https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/).\n\n **Remediation Method 2:**\n\n If using executable arguments, edit the kubelet service file on each worker node and ensure the below parameters are part of the `KUBELET_ARGS` variable string.\n\n For systems using `systemd`, such as the Amazon EKS Optimised Amazon Linux or Bottlerocket AMIs, then this file can be found at `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf`. Otherwise, you may need to look up documentation for your chosen operating system to determine which service manager is configured:\n\n \n```\n--authentication-token-webhook\n--authorization-mode=Webhook\n\n```\n **For Both Remediation Steps:**\n\n Based on your system, restart the `kubelet` service and check the service status.\n\n The following example is for operating systems using `systemd`, such as the Amazon EKS Optimised Amazon Linux or Bottlerocket AMIs, and invokes the `systemctl` command. If `systemctl` is not available then you will need to look up documentation for your chosen operating system to determine which service manager is configured:\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\nsystemctl status kubelet -l\n\n```","references":["https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/","https://kubernetes.io/docs/reference/access-authn-authz/kubelet-authn-authz/#kubelet-authentication","https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/"],"manualCheck":"**Audit Method 1:**\n\n Kubelets can accept configuration via a configuration file and in some cases via command line arguments. It is important to note that parameters provided as command line arguments will override their counterpart parameters in the configuration file (see `--config` details in the [Kubelet CLI Reference](https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/) for more info, where you can also find out which configuration parameters can be supplied as a command line argument).\n\n With this in mind, it is important to check for the existence of command line arguments as well as configuration file entries when auditing Kubelet configuration.\n\n Firstly, SSH to each node and execute the following command to find the Kubelet process:\n\n \n```\nps -ef | grep kubelet\n\n```\n The output of the above command provides details of the active Kubelet process, from which we can see the command line arguments provided to the process. Also note the location of the configuration file, provided with the `--config` argument, as this will be needed to verify configuration. The file can be viewed with a command such as `more` or `less`, like so:\n\n \n```\nsudo less /path/to/kubelet-config.json\n\n```\n Verify that Webhook Authentication is enabled. This may be enabled as a command line argument to the kubelet service with `--authentication-token-webhook` or in the kubelet configuration file via `\"authentication\": { \"webhook\": { \"enabled\": true } }`.\n\n Verify that the Authorization Mode is set to `WebHook`. This may be set as a command line argument to the kubelet service with `--authorization-mode=Webhook` or in the configuration file via `\"authorization\": { \"mode\": \"Webhook }`.\n\n **Audit Method 2:**\n\n It is also possible to review the running configuration of a Kubelet via the /configz endpoint of the Kubernetes API. This can be achieved using `kubectl` to proxy your requests to the API.\n\n Discover all nodes in your cluster by running the following command:\n\n \n```\nkubectl get nodes\n\n```\n Next, initiate a proxy with kubectl on a local port of your choice. In this example we will use 8080:\n\n \n```\nkubectl proxy --port=8080\n\n```\n With this running, in a separate terminal run the following command for each node:\n\n \n```\nexport NODE_NAME=my-node-name\ncurl http://localhost:8080/api/v1/nodes/${NODE_NAME}/proxy/configz    \n\n```\n The curl command will return the API response which will be a JSON formatted string representing the Kubelet configuration.\n\n Verify that Webhook Authentication is enabled with `\"authentication\": { \"webhook\": { \"enabled\": true } }` in the API response.\n\n Verify that the Authorization Mode is set to `WebHook` with `\"authorization\": { \"mode\": \"Webhook }` in the API response.","impactStatement":"","defaultValue":"See the EKS documentation for the default value."},{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-4.2.2 Ensure that the --authorization-mode argument is not set to AlwaysAllow","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126668/recommendations/1838640"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the following command on each node:\n\n \n```\nps -ef | grep kubelet\n\n```\n If the `--authorization-mode` argument is present check that it is not set to `AlwaysAllow`. If it is not present check that there is a Kubelet config file specified by `--config`, and that file sets `authorization: mode` to something other than `AlwaysAllow`.\n\n It is also possible to review the running configuration of a Kubelet via the `/configz` endpoint on the Kubelet API port (typically `10250/TCP`). Accessing these with appropriate credentials will provide details of the Kubelet's configuration.","configuration":[]},{"controlDetailsID":"C-0174","name":"Ensure that the --client-ca-file argument is set as appropriate","description":"Enable Kubelet authentication using certificates.","category":"Control plane","subCategory":"","remediation":"If using a Kubelet config file, edit the file to set `authentication: x509: clientCAFile` to the location of the client CA file.\n\n If using command line arguments, edit the kubelet service file `/etc/kubernetes/kubelet.conf` on each worker node and set the below parameter in `KUBELET_AUTHZ_ARGS` variable.\n\n \n```\n--client-ca-file=\u003cpath/to/client-ca-file\u003e\n\n```\n Based on your system, restart the `kubelet` service. For example:\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n\n```","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0","cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-3.2.3 Ensure that the --client-ca-file argument is set as appropriate","description":"","long_description":"","remediation":"**Remediation Method 1:**\n\n If modifying the Kubelet config file, edit the kubelet-config.json file `/etc/kubernetes/kubelet/kubelet-config.json` and set the below parameter to false\n\n \n```\n\"authentication\": { \"x509\": {\"clientCAFile:\" to the location of the client CA file.\n\n```\n **Remediation Method 2:**\n\n If using executable arguments, edit the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` on each worker node and add the below parameter at the end of the `KUBELET_ARGS` variable string.\n\n \n```\n--client-ca-file=\u003cpath/to/client-ca-file\u003e\n\n```\n **Remediation Method 3:**\n\n If using the api configz endpoint consider searching for the status of `\"authentication.*x509\":(\"clientCAFile\":\"/etc/kubernetes/pki/ca.crt\"` by extracting the live configuration from the nodes running kubelet.\n\n \\*\\*See detailed step-by-step configmap procedures in [Reconfigure a Node's Kubelet in a Live Cluster](https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/), and then rerun the curl statement from audit process to check for kubelet configuration changes\n\n \n```\nkubectl proxy --port=8001 \u0026\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"\n\n```\n **For all three remediations:**\nBased on your system, restart the `kubelet` service and check status\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\nsystemctl status kubelet -l\n\n```","references":["\u003chttps://kubernetes.io/docs/admin/kubelet/\u003e\n\n  \u003chttps://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-authentication-authorization/\u003e\n\n  \u003chttps://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/\u003e\n\n  \u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-data-protection#dp-4-encrypt-sensitive-information-in-transit\u003e"],"manualCheck":"**Audit Method 1:**\n\n If using a Kubelet configuration file, check that there is an entry for `\"x509\": {\"clientCAFile:\"` set to the location of the client certificate authority file.\n\n First, SSH to the relevant node:\n\n Run the following command on each node to find the appropriate Kubelet config file:\n\n \n```\nps -ef | grep kubelet\n\n```\n The output of the above command should return something similar to `--config /etc/kubernetes/kubelet/kubelet-config.json` which is the location of the Kubelet config file.\n\n Open the Kubelet config file:\n\n \n```\nsudo more /etc/kubernetes/kubelet/kubelet-config.json\n\n```\n Verify that the `\"x509\": {\"clientCAFile:\"` argument exists and is set to the location of the client certificate authority file.\n\n If the `\"x509\": {\"clientCAFile:\"` argument is not present, check that there is a Kubelet config file specified by `--config`, and that the file sets `\"authentication\": { \"x509\": {\"clientCAFile:\"` to the location of the client certificate authority file.\n\n **Audit Method 2:**\n\n If using the api configz endpoint consider searching for the status of `authentication.. x509\":(\"clientCAFile\":\"/etc/kubernetes/pki/ca.crt` by extracting the live configuration from the nodes running kubelet.\n\n Set the local proxy port and the following variables and provide proxy port number and node name;\n`HOSTNAME_PORT=\"localhost-and-port-number\"` `NODE_NAME=\"The-Name-Of-Node-To-Extract-Configuration\" from the output of \"kubectl get nodes\"`\n\n \n```\nkubectl proxy --port=8001 \u0026\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"\n\n```","impactStatement":"","defaultValue":"See the Azure AKS documentation for the default value."},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-3.2.3 Ensure that a Client CA File is Configured","description":"","long_description":"","remediation":"**Remediation Method 1:**\n\n If configuring via the Kubelet config file, you first need to locate the file.\n\n To do this, SSH to each node and execute the following command to find the kubelet process:\n\n \n```\nps -ef | grep kubelet\n\n```\n The output of the above command provides details of the active kubelet process, from which we can see the location of the configuration file provided to the kubelet service with the `--config` argument. The file can be viewed with a command such as `more` or `less`, like so:\n\n \n```\nsudo less /path/to/kubelet-config.json\n\n```\n Configure the client certificate authority file by setting the following parameter appropriately:\n\n \n```\n\"authentication\": { \"x509\": {\"clientCAFile\": \u003cpath/to/client-ca-file\u003e } }\"\n\n```\n **Remediation Method 2:**\n\n If using executable arguments, edit the kubelet service file on each worker node and ensure the below parameters are part of the `KUBELET_ARGS` variable string.\n\n For systems using `systemd`, such as the Amazon EKS Optimised Amazon Linux or Bottlerocket AMIs, then this file can be found at `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf`. Otherwise, you may need to look up documentation for your chosen operating system to determine which service manager is configured:\n\n \n```\n--client-ca-file=\u003cpath/to/client-ca-file\u003e\n\n```\n **For Both Remediation Steps:**\n\n Based on your system, restart the `kubelet` service and check the service status.\n\n The following example is for operating systems using `systemd`, such as the Amazon EKS Optimised Amazon Linux or Bottlerocket AMIs, and invokes the `systemctl` command. If `systemctl` is not available then you will need to look up documentation for your chosen operating system to determine which service manager is configured:\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\nsystemctl status kubelet -l\n\n```","references":["https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/","https://kubernetes.io/docs/reference/access-authn-authz/kubelet-authn-authz/#kubelet-authentication","https://kubernetes.io/docs/reference/config-api/kubelet-config.v1beta1/"],"manualCheck":"**Audit Method 1:**\n\n Kubelets can accept configuration via a configuration file and in some cases via command line arguments. It is important to note that parameters provided as command line arguments will override their counterpart parameters in the configuration file (see `--config` details in the [Kubelet CLI Reference](https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/) for more info, where you can also find out which configuration parameters can be supplied as a command line argument).\n\n With this in mind, it is important to check for the existence of command line arguments as well as configuration file entries when auditing Kubelet configuration.\n\n Firstly, SSH to each node and execute the following command to find the Kubelet process:\n\n \n```\nps -ef | grep kubelet\n\n```\n The output of the above command provides details of the active Kubelet process, from which we can see the command line arguments provided to the process. Also note the location of the configuration file, provided with the `--config` argument, as this will be needed to verify configuration. The file can be viewed with a command such as `more` or `less`, like so:\n\n \n```\nsudo less /path/to/kubelet-config.json\n\n```\n Verify that a client certificate authority file is configured. This may be configured using a command line argument to the kubelet service with `--client-ca-file` or in the kubelet configuration file via `\"authentication\": { \"x509\": {\"clientCAFile\": \u003cpath/to/client-ca-file\u003e } }\"`.\n\n **Audit Method 2:**\n\n It is also possible to review the running configuration of a Kubelet via the /configz endpoint of the Kubernetes API. This can be achieved using `kubectl` to proxy your requests to the API.\n\n Discover all nodes in your cluster by running the following command:\n\n \n```\nkubectl get nodes\n\n```\n Next, initiate a proxy with kubectl on a local port of your choice. In this example we will use 8080:\n\n \n```\nkubectl proxy --port=8080\n\n```\n With this running, in a separate terminal run the following command for each node:\n\n \n```\nexport NODE_NAME=my-node-name\ncurl http://localhost:8080/api/v1/nodes/${NODE_NAME}/proxy/configz    \n\n```\n The curl command will return the API response which will be a JSON formatted string representing the Kubelet configuration.\n\n Verify that a client certificate authority file is configured with `\"authentication\": { \"x509\": {\"clientCAFile\": \u003cpath/to/client-ca-file\u003e } }\"` in the API response.","impactStatement":"","defaultValue":"See the EKS documentation for the default value."},{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-4.2.3 Ensure that the --client-ca-file argument is set as appropriate","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126668/recommendations/1838643"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the following command on each node:\n\n \n```\nps -ef | grep kubelet\n\n```\n Verify that the `--client-ca-file` argument exists and is set to the location of the client certificate authority file.\n\n If the `--client-ca-file` argument is not present, check that there is a Kubelet config file specified by `--config`, and that the file sets `authentication: x509: clientCAFile` to the location of the client certificate authority file.","configuration":[]},{"controlDetailsID":"C-0175","name":"Verify that the --read-only-port argument is set to 0","description":"Disable the read-only port.","category":"Control plane","subCategory":"","remediation":"If using a Kubelet config file, edit the file to set `readOnlyPort` to `0`.\n\n If using command line arguments, edit the kubelet service file `/etc/kubernetes/kubelet.conf` on each worker node and set the below parameter in `KUBELET_SYSTEM_PODS_ARGS` variable.\n\n \n```\n--read-only-port=0\n\n```\n Based on your system, restart the `kubelet` service. For example:\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n\n```","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0","cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":4,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-3.2.4 Ensure that the --read-only-port is secured","description":"","long_description":"","remediation":"If modifying the Kubelet config file, edit the kubelet-config.json file `/etc/kubernetes/kubelet/kubelet-config.json` and set the below parameter to false\n\n \n```\nreadOnlyPort to 0\n\n```\n If using executable arguments, edit the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` on each worker node and add the below parameter at the end of the `KUBELET_ARGS` variable string.\n\n \n```\n--read-only-port=0\n\n```\n For all remediations:\nBased on your system, restart the `kubelet` service and check status\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\nsystemctl status kubelet -l\n\n```","references":["\u003chttps://kubernetes.io/docs/admin/kubelet/\u003e\n\n  \u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-posture-vulnerability-management#pv-3-establish-secure-configurations-for-compute-resources\u003e"],"manualCheck":"If using a Kubelet configuration file, check that there is an entry for `authentication: anonymous: enabled` set to `0`.\n\n First, SSH to the relevant node:\n\n Run the following command on each node to find the appropriate Kubelet config file:\n\n \n```\nps -ef | grep kubelet\n\n```\n The output of the above command should return something similar to `--config /etc/kubernetes/kubelet/kubelet-config.json` which is the location of the Kubelet config file.\n\n Open the Kubelet config file:\n\n \n```\ncat /etc/kubernetes/kubelet/kubelet-config.json\n\n```\n Verify that the `--read-only-port` argument exists and is set to `0`.\n\n If the `--read-only-port` argument is not present, check that there is a Kubelet config file specified by `--config`. Check that if there is a `readOnlyPort` entry in the file, it is set to `0`.","impactStatement":"","defaultValue":"See the Azure AKS documentation for the default value."},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-3.2.4 Ensure that the --read-only-port is disabled","description":"","long_description":"","remediation":"If modifying the Kubelet config file, edit the kubelet-config.json file `/etc/kubernetes/kubelet/kubelet-config.json` and set the below parameter to 0\n\n \n```\n\"readOnlyPort\": 0\n\n```\n If using executable arguments, edit the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` on each worker node and add the below parameter at the end of the `KUBELET_ARGS` variable string.\n\n \n```\n--read-only-port=0\n\n```\n For each remediation:\nBased on your system, restart the `kubelet` service and check status\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\nsystemctl status kubelet -l\n\n```","references":["https://kubernetes.io/docs/admin/kubelet/"],"manualCheck":"If using a Kubelet configuration file, check that there is an entry for `authentication: anonymous: enabled` set to `0`.\n\n First, SSH to the relevant node:\n\n Run the following command on each node to find the appropriate Kubelet config file:\n\n \n```\nps -ef | grep kubelet\n\n```\n The output of the above command should return something similar to `--config /etc/kubernetes/kubelet/kubelet-config.json` which is the location of the Kubelet config file.\n\n Open the Kubelet config file:\n\n \n```\ncat /etc/kubernetes/kubelet/kubelet-config.json\n\n```\n Verify that the `--read-only-port` argument exists and is set to `0`.\n\n If the `--read-only-port` argument is not present, check that there is a Kubelet config file specified by `--config`. Check that if there is a `readOnlyPort` entry in the file, it is set to `0`.","impactStatement":"","defaultValue":"See the Amazon EKS documentation for the default value."},{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-4.2.4 Verify that the --read-only-port argument is set to 0","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126668/recommendations/1838645"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the following command on each node:\n\n \n```\nps -ef | grep kubelet\n\n```\n Verify that the `--read-only-port` argument exists and is set to `0`.\n\n If the `--read-only-port` argument is not present, check that there is a Kubelet config file specified by `--config`. Check that if there is a `readOnlyPort` entry in the file, it is set to `0`.","configuration":[]},{"controlDetailsID":"C-0176","name":"Ensure that the --streaming-connection-idle-timeout argument is not set to 0","description":"Do not disable timeouts on streaming connections.","category":"Control plane","subCategory":"","remediation":"If using a Kubelet config file, edit the file to set `streamingConnectionIdleTimeout` to a value other than 0.\n\n If using command line arguments, edit the kubelet service file `/etc/kubernetes/kubelet.conf` on each worker node and set the below parameter in `KUBELET_SYSTEM_PODS_ARGS` variable.\n\n \n```\n--streaming-connection-idle-timeout=5m\n\n```\n Based on your system, restart the `kubelet` service. For example:\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n\n```","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0","cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":3,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-3.2.5 Ensure that the --streaming-connection-idle-timeout argument is not set to 0","description":"","long_description":"","remediation":"**Remediation Method 1:**\n\n If modifying the Kubelet config file, edit the kubelet-config.json file `/etc/kubernetes/kubelet/kubelet-config.json` and set the below parameter to a non-zero value in the format of #h#m#s\n\n \n```\n\"streamingConnectionIdleTimeout\": \"4h0m0s\"\n\n```\n You should ensure that the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` does not specify a `--streaming-connection-idle-timeout` argument because it would override the Kubelet config file.\n\n **Remediation Method 2:**\n\n If using executable arguments, edit the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` on each worker node and add the below parameter at the end of the `KUBELET_ARGS` variable string.\n\n \n```\n--streaming-connection-idle-timeout=4h0m0s\n\n```\n **Remediation Method 3:**\n\n If using the api configz endpoint consider searching for the status of `\"streamingConnectionIdleTimeout\":` by extracting the live configuration from the nodes running kubelet.\n\n \\*\\*See detailed step-by-step configmap procedures in [Reconfigure a Node's Kubelet in a Live Cluster](https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/), and then rerun the curl statement from audit process to check for kubelet configuration changes\n\n \n```\nkubectl proxy --port=8001 \u0026\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"\n\n```\n **For all three remediations:**\nBased on your system, restart the `kubelet` service and check status\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\nsystemctl status kubelet -l\n\n```","references":["\u003chttps://kubernetes.io/docs/admin/kubelet/\u003e\n\n  \u003chttps://github.com/kubernetes/kubernetes/pull/18552\u003e\n\n  \u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-posture-vulnerability-management#pv-3-establish-secure-configurations-for-compute-resources\u003e"],"manualCheck":"**Audit Method 1:**\n\n First, SSH to the relevant node:\n\n Run the following command on each node to find the running kubelet process:\n\n \n```\nps -ef | grep kubelet\n\n```\n If the command line for the process includes the argument `streaming-connection-idle-timeout` verify that it is not set to 0.\n\n If the `streaming-connection-idle-timeout` argument is not present in the output of the above command, refer instead to the `config` argument that specifies the location of the Kubelet config file e.g. `--config /etc/kubernetes/kubelet/kubelet-config.json`.\n\n Open the Kubelet config file:\n\n \n```\ncat /etc/kubernetes/kubelet/kubelet-config.json\n\n```\n Verify that the `streamingConnectionIdleTimeout` argument is not set to `0`.\n\n **Audit Method 2:**\n\n If using the api configz endpoint consider searching for the status of `\"streamingConnectionIdleTimeout\":\"4h0m0s\"` by extracting the live configuration from the nodes running kubelet.\n\n Set the local proxy port and the following variables and provide proxy port number and node name;\n`HOSTNAME_PORT=\"localhost-and-port-number\"` `NODE_NAME=\"The-Name-Of-Node-To-Extract-Configuration\" from the output of \"kubectl get nodes\"`\n\n \n```\nkubectl proxy --port=8001 \u0026\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"\n\n```","impactStatement":"","defaultValue":"See the Azure AKS documentation for the default value."},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-3.2.5 Ensure that the --streaming-connection-idle-timeout argument is not set to 0","description":"","long_description":"","remediation":"**Remediation Method 1:**\n\n If modifying the Kubelet config file, edit the kubelet-config.json file `/etc/kubernetes/kubelet/kubelet-config.json` and set the below parameter to a non-zero value in the format of #h#m#s\n\n \n```\n\"streamingConnectionIdleTimeout\": \"4h0m0s\"\n\n```\n You should ensure that the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` does not specify a `--streaming-connection-idle-timeout` argument because it would override the Kubelet config file.\n\n **Remediation Method 2:**\n\n If using executable arguments, edit the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` on each worker node and add the below parameter at the end of the `KUBELET_ARGS` variable string.\n\n \n```\n--streaming-connection-idle-timeout=4h0m0s\n\n```\n **Remediation Method 3:**\n\n If using the api configz endpoint consider searching for the status of `\"streamingConnectionIdleTimeout\":` by extracting the live configuration from the nodes running kubelet.\n\n \\*\\*See detailed step-by-step configmap procedures in [Reconfigure a Node's Kubelet in a Live Cluster](https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/), and then rerun the curl statement from audit process to check for kubelet configuration changes\n\n \n```\nkubectl proxy --port=8001 \u0026\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=ip-192.168.31.226.ec2.internal (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"\n\n```\n **For all three remediations:**\nBased on your system, restart the `kubelet` service and check status\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\nsystemctl status kubelet -l\n\n```","references":["https://kubernetes.io/docs/admin/kubelet/","https://github.com/kubernetes/kubernetes/pull/18552"],"manualCheck":"**Audit Method 1:**\n\n First, SSH to the relevant node:\n\n Run the following command on each node to find the running kubelet process:\n\n \n```\nps -ef | grep kubelet\n\n```\n If the command line for the process includes the argument `streaming-connection-idle-timeout` verify that it is not set to 0.\n\n If the `streaming-connection-idle-timeout` argument is not present in the output of the above command, refer instead to the `config` argument that specifies the location of the Kubelet config file e.g. `--config /etc/kubernetes/kubelet/kubelet-config.json`.\n\n Open the Kubelet config file:\n\n \n```\ncat /etc/kubernetes/kubelet/kubelet-config.json\n\n```\n Verify that the `streamingConnectionIdleTimeout` argument is not set to `0`.\n\n **Audit Method 2:**\n\n If using the api configz endpoint consider searching for the status of `\"streamingConnectionIdleTimeout\":\"4h0m0s\"` by extracting the live configuration from the nodes running kubelet.\n\n Set the local proxy port and the following variables and provide proxy port number and node name;\n`HOSTNAME_PORT=\"localhost-and-port-number\"` `NODE_NAME=\"The-Name-Of-Node-To-Extract-Configuration\" from the output of \"kubectl get nodes\"`\n\n \n```\nkubectl proxy --port=8001 \u0026\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=ip-192.168.31.226.ec2.internal (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"\n\n```","impactStatement":"","defaultValue":"See the EKS documentation for the default value."},{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-4.2.5 Ensure that the --streaming-connection-idle-timeout argument is not set to 0","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126668/recommendations/1838646"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the following command on each node:\n\n \n```\nps -ef | grep kubelet\n\n```\n Verify that the `--streaming-connection-idle-timeout` argument is not set to `0`.\n\n If the argument is not present, and there is a Kubelet config file specified by `--config`, check that it does not set `streamingConnectionIdleTimeout` to 0.","configuration":[]},{"controlDetailsID":"C-0177","name":"Ensure that the --protect-kernel-defaults argument is set to true","description":"Protect tuned kernel parameters from overriding kubelet default kernel parameter values.","category":"Control plane","subCategory":"","remediation":"If using a Kubelet config file, edit the file to set `protectKernelDefaults: true`.\n\n If using command line arguments, edit the kubelet service file `/etc/kubernetes/kubelet.conf` on each worker node and set the below parameter in `KUBELET_SYSTEM_PODS_ARGS` variable.\n\n \n```\n--protect-kernel-defaults=true\n\n```\n Based on your system, restart the `kubelet` service. For example:\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n\n```","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0","cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":2,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-3.2.6 Ensure that the --protect-kernel-defaults argument is set to true","description":"","long_description":"","remediation":"**Remediation Method 1:**\n\n If modifying the Kubelet config file, edit the kubelet-config.json file `/etc/kubernetes/kubelet/kubelet-config.json` and set the below parameter to true\n\n \n```\n\"protectKernelDefaults\": \n\n```\n **Remediation Method 2:**\n\n If using executable arguments, edit the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` on each worker node and add the below parameter at the end of the `KUBELET_ARGS` variable string.\n\n \n```\n----protect-kernel-defaults=true\n\n```\n **Remediation Method 3:**\n\n If using the api configz endpoint consider searching for the status of `\"protectKernelDefaults\":` by extracting the live configuration from the nodes running kubelet.\n\n \\*\\*See detailed step-by-step configmap procedures in [Reconfigure a Node's Kubelet in a Live Cluster](https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/), and then rerun the curl statement from audit process to check for kubelet configuration changes\n\n \n```\nkubectl proxy --port=8001 \u0026\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"\n\n```\n **For all three remediations:**\nBased on your system, restart the `kubelet` service and check status\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\nsystemctl status kubelet -l\n\n```","references":["\u003chttps://kubernetes.io/docs/admin/kubelet/\u003e\n\n  \u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-posture-vulnerability-management#pv-3-establish-secure-configurations-for-compute-resources\u003e"],"manualCheck":"**Audit Method 1:**\n\n Run the following command on each node to find the kubelet process:\n\n \n```\nps -ef | grep kubelet\n\n```\n Verify that the command line for kubelet includes this argument set to `true`:\n\n \n```\n--protect-kernel-defaults=true\n\n```\n If the `--protect-kernel-defaults` argument is not present, check that there is a Kubelet config file specified by `--config`, and that the file sets `protectKernelDefaults` to `true`.\n\n **Audit Method 2:**\n\n If using the api configz endpoint consider searching for the status of `\"protectKernelDefaults\"` by extracting the live configuration from the nodes running kubelet.\n\n Set the local proxy port and the following variables and provide proxy port number and node name;\n`HOSTNAME_PORT=\"localhost-and-port-number\"` `NODE_NAME=\"The-Name-Of-Node-To-Extract-Configuration\" from the output of \"kubectl get nodes\"`\n\n \n```\nkubectl proxy --port=8001 \u0026\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"\n\n```","impactStatement":"","defaultValue":"See the Azure AKS documentation for the default value."},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-3.2.6 Ensure that the --protect-kernel-defaults argument is set to true","description":"","long_description":"","remediation":"**Remediation Method 1:**\n\n If modifying the Kubelet config file, edit the kubelet-config.json file `/etc/kubernetes/kubelet/kubelet-config.json` and set the below parameter to true\n\n \n```\n\"protectKernelDefaults\": \n\n```\n **Remediation Method 2:**\n\n If using executable arguments, edit the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` on each worker node and add the below parameter at the end of the `KUBELET_ARGS` variable string.\n\n \n```\n----protect-kernel-defaults=true\n\n```\n **Remediation Method 3:**\n\n If using the api configz endpoint consider searching for the status of `\"protectKernelDefaults\":` by extracting the live configuration from the nodes running kubelet.\n\n \\*\\*See detailed step-by-step configmap procedures in [Reconfigure a Node's Kubelet in a Live Cluster](https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/), and then rerun the curl statement from audit process to check for kubelet configuration changes\n\n \n```\nkubectl proxy --port=8001 \u0026\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=ip-192.168.31.226.ec2.internal (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"\n\n```\n **For all three remediations:**\nBased on your system, restart the `kubelet` service and check status\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\nsystemctl status kubelet -l\n\n```","references":["https://kubernetes.io/docs/admin/kubelet/"],"manualCheck":"**Audit Method 1:**\n\n Run the following command on each node to find the kubelet process:\n\n \n```\nps -ef | grep kubelet\n\n```\n Verify that the command line for kubelet includes this argument set to `true`:\n\n \n```\n--protect-kernel-defaults=true\n\n```\n If the `--protect-kernel-defaults` argument is not present, check that there is a Kubelet config file specified by `--config`, and that the file sets `protectKernelDefaults` to `true`.\n\n **Audit Method 2:**\n\n If using the api configz endpoint consider searching for the status of `\"protectKernelDefaults\"` by extracting the live configuration from the nodes running kubelet.\n\n Set the local proxy port and the following variables and provide proxy port number and node name;\n`HOSTNAME_PORT=\"localhost-and-port-number\"` `NODE_NAME=\"The-Name-Of-Node-To-Extract-Configuration\" from the output of \"kubectl get nodes\"`\n\n \n```\nkubectl proxy --port=8001 \u0026\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=ip-192.168.31.226.ec2.internal (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"\n\n```","impactStatement":"","defaultValue":"See the EKS documentation for the default value."},{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-4.2.6 Ensure that the --protect-kernel-defaults argument is set to true","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126668/recommendations/1838648"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the following command on each node:\n\n \n```\nps -ef | grep kubelet\n\n```\n Verify that the `--protect-kernel-defaults` argument is set to `true`.\n\n If the `--protect-kernel-defaults` argument is not present, check that there is a Kubelet config file specified by `--config`, and that the file sets `protectKernelDefaults` to `true`.","configuration":[]},{"controlDetailsID":"C-0178","name":"Ensure that the --make-iptables-util-chains argument is set to true","description":"Allow Kubelet to manage iptables.","category":"Control plane","subCategory":"","remediation":"If using a Kubelet config file, edit the file to set `makeIPTablesUtilChains: true`.\n\n If using command line arguments, edit the kubelet service file `/etc/kubernetes/kubelet.conf` on each worker node and remove the `--make-iptables-util-chains` argument from the `KUBELET_SYSTEM_PODS_ARGS` variable.\n\n Based on your system, restart the `kubelet` service. For example:\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n\n```","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0","cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":3,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-3.2.7 Ensure that the --make-iptables-util-chains argument is set to true","description":"","long_description":"","remediation":"**Remediation Method 1:**\n\n If modifying the Kubelet config file, edit the kubelet-config.json file `/etc/kubernetes/kubelet/kubelet-config.json` and set the below parameter to false\n\n \n```\n\"makeIPTablesUtilChains\": true\n\n```\n **Remediation Method 2:**\n\n If using executable arguments, edit the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` on each worker node and add the below parameter at the end of the `KUBELET_ARGS` variable string.\n\n \n```\n--make-iptables-util-chains:true\n\n```\n **Remediation Method 3:**\n\n If using the api configz endpoint consider searching for the status of `\"makeIPTablesUtilChains\": true` by extracting the live configuration from the nodes running kubelet.\n\n \\*\\*See detailed step-by-step configmap procedures in [Reconfigure a Node's Kubelet in a Live Cluster](https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/), and then rerun the curl statement from audit process to check for kubelet configuration changes\n\n \n```\nkubectl proxy --port=8001 \u0026\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"\n\n```\n **For all three remediations:**\nBased on your system, restart the `kubelet` service and check status\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\nsystemctl status kubelet -l\n\n```","references":["\u003chttps://kubernetes.io/docs/admin/kubelet/\u003e\n\n  \u003chttps://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/\u003e\n\n  \u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-network-security#ns-1-implement-security-for-internal-traffic\u003e"],"manualCheck":"**Audit Method 1:**\n\n If using a Kubelet configuration file, check that there is an entry for `makeIPTablesUtilChains` set to `true`.\n\n First, SSH to the relevant node:\n\n Run the following command on each node to find the appropriate Kubelet config file:\n\n \n```\nps -ef | grep kubelet\n\n```\n The output of the above command should return something similar to `--config /etc/kubernetes/kubelet/kubelet-config.json` which is the location of the Kubelet config file.\n\n Open the Kubelet config file:\n\n \n```\ncat /etc/kubernetes/kubelet/kubelet-config.json\n\n```\n Verify that if the `makeIPTablesUtilChains` argument exists then it is set to `true`.\n\n If the `--make-iptables-util-chains` argument does not exist, and there is a Kubelet config file specified by `--config`, verify that the file does not set `makeIPTablesUtilChains` to `false`.\n\n **Audit Method 2:**\n\n If using the api configz endpoint consider searching for the status of `authentication... \"makeIPTablesUtilChains\":true` by extracting the live configuration from the nodes running kubelet.\n\n Set the local proxy port and the following variables and provide proxy port number and node name;\n`HOSTNAME_PORT=\"localhost-and-port-number\"` `NODE_NAME=\"The-Name-Of-Node-To-Extract-Configuration\" from the output of \"kubectl get nodes\"`\n\n \n```\nkubectl proxy --port=8001 \u0026\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"\n\n```","impactStatement":"","defaultValue":"See the Azure AKS documentation for the default value."},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-3.2.7 Ensure that the --make-iptables-util-chains argument is set to true","description":"","long_description":"","remediation":"**Remediation Method 1:**\n\n If modifying the Kubelet config file, edit the kubelet-config.json file `/etc/kubernetes/kubelet/kubelet-config.json` and set the below parameter to true\n\n \n```\n\"makeIPTablesUtilChains\": true\n\n```\n Ensure that `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` does not set the `--make-iptables-util-chains` argument because that would override your Kubelet config file.\n\n **Remediation Method 2:**\n\n If using executable arguments, edit the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` on each worker node and add the below parameter at the end of the `KUBELET_ARGS` variable string.\n\n \n```\n--make-iptables-util-chains:true\n\n```\n **Remediation Method 3:**\n\n If using the api configz endpoint consider searching for the status of `\"makeIPTablesUtilChains.: true` by extracting the live configuration from the nodes running kubelet.\n\n \\*\\*See detailed step-by-step configmap procedures in [Reconfigure a Node's Kubelet in a Live Cluster](https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/), and then rerun the curl statement from audit process to check for kubelet configuration changes\n\n \n```\nkubectl proxy --port=8001 \u0026\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=ip-192.168.31.226.ec2.internal (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"\n\n```\n **For all three remediations:**\nBased on your system, restart the `kubelet` service and check status\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\nsystemctl status kubelet -l\n\n```","references":["https://kubernetes.io/docs/admin/kubelet/","https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/"],"manualCheck":"**Audit Method 1:**\n\n First, SSH to each node:\n\n Run the following command on each node to find the Kubelet process:\n\n \n```\nps -ef | grep kubelet\n\n```\n If the output of the above command includes the argument `--make-iptables-util-chains` then verify it is set to true.\n\n If the `--make-iptables-util-chains` argument does not exist, and there is a Kubelet config file specified by `--config`, verify that the file does not set `makeIPTablesUtilChains` to `false`.\n\n **Audit Method 2:**\n\n If using the api configz endpoint consider searching for the status of `authentication... \"makeIPTablesUtilChains.:true` by extracting the live configuration from the nodes running kubelet.\n\n Set the local proxy port and the following variables and provide proxy port number and node name;\n`HOSTNAME_PORT=\"localhost-and-port-number\"` `NODE_NAME=\"The-Name-Of-Node-To-Extract-Configuration\" from the output of \"kubectl get nodes\"`\n\n \n```\nkubectl proxy --port=8001 \u0026\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=ip-192.168.31.226.ec2.internal (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"\n\n```","impactStatement":"","defaultValue":"See the Amazon EKS documentation for the default value."},{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-4.2.7 Ensure that the --make-iptables-util-chains argument is set to true","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126668/recommendations/1838651"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the following command on each node:\n\n \n```\nps -ef | grep kubelet\n\n```\n Verify that if the `--make-iptables-util-chains` argument exists then it is set to `true`.\n\n If the `--make-iptables-util-chains` argument does not exist, and there is a Kubelet config file specified by `--config`, verify that the file does not set `makeIPTablesUtilChains` to `false`.","configuration":[]},{"controlDetailsID":"C-0179","name":"Ensure that the --hostname-override argument is not set","description":"Do not override node hostnames.","category":"Control plane","subCategory":"","remediation":"Edit the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubeadm.conf` on each worker node and remove the `--hostname-override` argument from the `KUBELET_SYSTEM_PODS_ARGS` variable.\n\n Based on your system, restart the `kubelet` service. For example:\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n\n```","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0","cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":3,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-3.2.8 Ensure that the --hostname-override argument is not set","description":"","long_description":"Overriding hostnames could potentially break TLS setup between the kubelet and the apiserver. Additionally, with overridden hostnames, it becomes increasingly difficult to associate logs with a particular node and process them for security analytics. Hence, you should setup your kubelet nodes with resolvable FQDNs and avoid overriding the hostnames with IPs. Usage of --hostname-override also may have some undefined/unsupported behaviours.","remediation":"**Remediation Method 1:**\n\n If using executable arguments, edit the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` on each worker node and remove the below parameter from the `KUBELET_ARGS` variable string.\n\n \n```\n--hostname-override\n\n```\n Based on your system, restart the `kubelet` service and check status. The example below is for systemctl:\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\nsystemctl status kubelet -l\n\n```","references":["\u003chttps://kubernetes.io/docs/admin/kubelet/\u003e\n\n  \u003chttps://github.com/kubernetes/kubernetes/issues/22063\u003e\n\n  \u003chttps://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/\u003e\n\n  \u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-posture-vulnerability-management#pv-3-establish-secure-configurations-for-compute-resources\u003e"],"manualCheck":"**Audit Method 1:**\n\n SSH to each node:\n\n Run the following command on each node to find the Kubelet process:\n\n \n```\nps -ef | grep kubelet\n\n```\n Verify that `--hostname-override` argument does not exist in the output of the above command.\n\n **Note** This setting is not configurable via the Kubelet config file.","impactStatement":"--hostname-override may not take when the kubelet also has --cloud-provider aws","defaultValue":"See the Azure AKS documentation for the default value."},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-3.2.8 Ensure that the --hostname-override argument is not set","description":"","long_description":"Overriding hostnames could potentially break TLS setup between the kubelet and the apiserver. Additionally, with overridden hostnames, it becomes increasingly difficult to associate logs with a particular node and process them for security analytics. Hence, you should setup your kubelet nodes with resolvable FQDNs and avoid overriding the hostnames with IPs. Usage of --hostname-override also may have some undefined/unsupported behaviours.","remediation":"**Remediation Method 1:**\n\n If using executable arguments, edit the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` on each worker node and remove the below parameter from the `KUBELET_ARGS` variable string.\n\n \n```\n--hostname-override\n\n```\n Based on your system, restart the `kubelet` service and check status. The example below is for systemctl:\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\nsystemctl status kubelet -l\n\n```","references":["https://kubernetes.io/docs/admin/kubelet/","https://github.com/kubernetes/kubernetes/issues/22063","https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/"],"manualCheck":"**Audit Method 1:**\n\n SSH to each node:\n\n Run the following command on each node to find the Kubelet process:\n\n \n```\nps -ef | grep kubelet\n\n```\n Verify that `--hostname-override` argument does not exist in the output of the above command.\n\n **Note** This setting is not configurable via the Kubelet config file.","impactStatement":"--hostname-override may not take when the kubelet also has --cloud-provider aws","defaultValue":"See the Amazon EKS documentation for the default value."},{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-4.2.8 Ensure that the --hostname-override argument is not set","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126668/recommendations/1838654"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the following command on each node:\n\n \n```\nps -ef | grep kubelet\n\n```\n Verify that `--hostname-override` argument does not exist.\n\n **Note** This setting is not configurable via the Kubelet config file.","configuration":[]},{"controlDetailsID":"C-0180","name":"Ensure that the --event-qps argument is set to 0 or a level which ensures appropriate event capture","description":"Security relevant information should be captured. The `--event-qps` flag on the Kubelet can be used to limit the rate at which events are gathered. Setting this too low could result in relevant events not being logged, however the unlimited setting of `0` could result in a denial of service on the kubelet.","category":"Control plane","subCategory":"","remediation":"If using a Kubelet config file, edit the file to set `eventRecordQPS:` to an appropriate level.\n\n If using command line arguments, edit the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubeadm.conf` on each worker node and set the below parameter in `KUBELET_SYSTEM_PODS_ARGS` variable.\n\n Based on your system, restart the `kubelet` service. For example:\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n\n```","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0","cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":2,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-3.2.9 Ensure that the --eventRecordQPS argument is set to 0 or a level which ensures appropriate event capture","description":"Security relevant information should be captured. The `--eventRecordQPS` flag on the Kubelet can be used to limit the rate at which events are gathered. Setting this too low could result in relevant events not being logged, however the unlimited setting of `0` could result in a denial of service on the kubelet.","long_description":"","remediation":"**Remediation Method 1:**\n\n If modifying the Kubelet config file, edit the kubelet-config.json file `/etc/kubernetes/kubelet/kubelet-config.json` and set the below parameter to 5 or a value greater or equal to 0\n\n \n```\n\"eventRecordQPS\": 5\n\n```\n Check that `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` does not define an executable argument for `eventRecordQPS` because this would override your Kubelet config.\n\n **Remediation Method 2:**\n\n If using executable arguments, edit the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` on each worker node and add the below parameter at the end of the `KUBELET_ARGS` variable string.\n\n \n```\n--eventRecordQPS=5\n\n```\n **Remediation Method 3:**\n\n If using the api configz endpoint consider searching for the status of `\"eventRecordQPS\"` by extracting the live configuration from the nodes running kubelet.\n\n \\*\\*See detailed step-by-step configmap procedures in [Reconfigure a Node's Kubelet in a Live Cluster](https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/), and then rerun the curl statement from audit process to check for kubelet configuration changes\n\n \n```\nkubectl proxy --port=8001 \u0026\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"\n\n```\n **For all three remediations:**\nBased on your system, restart the `kubelet` service and check status\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\nsystemctl status kubelet -l\n\n```","references":["\u003chttps://kubernetes.io/docs/admin/kubelet/\u003e\n\n  \u003chttps://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/kubeletconfig/v1beta1/types.go\u003e\n\n  \u003chttps://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/\u003e\n\n  \u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-logging-threat-detection\u003e"],"manualCheck":"**Audit Method 1:**\n\n First, SSH to each node.\n\n Run the following command on each node to find the Kubelet process:\n\n \n```\nps -ef | grep kubelet\n\n```\n In the output of the above command review the value set for the `--eventRecordQPS` argument and determine whether this has been set to an appropriate level for the cluster. The value of `0` can be used to ensure that all events are captured.\n\n If the `--eventRecordQPS` argument does not exist, check that there is a Kubelet config file specified by `--config` and review the value in this location.\nThe output of the above command should return something similar to `--config /etc/kubernetes/kubelet/kubelet-config.json` which is the location of the Kubelet config file.\n\n Open the Kubelet config file:\n\n \n```\ncat /etc/kubernetes/kubelet/kubelet-config.json\n\n```\n If there is an entry for `eventRecordQPS` check that it is set to 0 or an appropriate level for the cluster.\n\n **Audit Method 2:**\n\n If using the api configz endpoint consider searching for the status of `eventRecordQPS` by extracting the live configuration from the nodes running kubelet.\n\n Set the local proxy port and the following variables and provide proxy port number and node name;\n`HOSTNAME_PORT=\"localhost-and-port-number\"` `NODE_NAME=\"The-Name-Of-Node-To-Extract-Configuration\" from the output of \"kubectl get nodes\"`\n\n \n```\nkubectl proxy --port=8001 \u0026\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"\n\n```","impactStatement":"","defaultValue":"See the AKS documentation for the default value."},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-3.2.9 Ensure that the --eventRecordQPS argument is set to 0 or a level which ensures appropriate event capture","description":"Security relevant information should be captured. The `--eventRecordQPS` flag on the Kubelet can be used to limit the rate at which events are gathered. Setting this too low could result in relevant events not being logged, however the unlimited setting of `0` could result in a denial of service on the kubelet.","long_description":"","remediation":"**Remediation Method 1:**\n\n If modifying the Kubelet config file, edit the kubelet-config.json file `/etc/kubernetes/kubelet/kubelet-config.json` and set the below parameter to 5 or a value greater or equal to 0\n\n \n```\n\"eventRecordQPS\": 5\n\n```\n Check that `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` does not define an executable argument for `eventRecordQPS` because this would override your Kubelet config.\n\n **Remediation Method 2:**\n\n If using executable arguments, edit the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` on each worker node and add the below parameter at the end of the `KUBELET_ARGS` variable string.\n\n \n```\n--eventRecordQPS=5\n\n```\n **Remediation Method 3:**\n\n If using the api configz endpoint consider searching for the status of `\"eventRecordQPS\"` by extracting the live configuration from the nodes running kubelet.\n\n \\*\\*See detailed step-by-step configmap procedures in [Reconfigure a Node's Kubelet in a Live Cluster](https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/), and then rerun the curl statement from audit process to check for kubelet configuration changes\n\n \n```\nkubectl proxy --port=8001 \u0026\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=ip-192.168.31.226.ec2.internal (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"\n\n```\n **For all three remediations:**\nBased on your system, restart the `kubelet` service and check status\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\nsystemctl status kubelet -l\n\n```","references":["https://kubernetes.io/docs/admin/kubelet/","https://github.com/kubernetes/kubernetes/blob/master/pkg/kubelet/apis/kubeletconfig/v1beta1/types.go","https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/"],"manualCheck":"**Audit Method 1:**\n\n First, SSH to each node.\n\n Run the following command on each node to find the Kubelet process:\n\n \n```\nps -ef | grep kubelet\n\n```\n In the output of the above command review the value set for the `--eventRecordQPS` argument and determine whether this has been set to an appropriate level for the cluster. The value of `0` can be used to ensure that all events are captured.\n\n If the `--eventRecordQPS` argument does not exist, check that there is a Kubelet config file specified by `--config` and review the value in this location.\nThe output of the above command should return something similar to `--config /etc/kubernetes/kubelet/kubelet-config.json` which is the location of the Kubelet config file.\n\n Open the Kubelet config file:\n\n \n```\ncat /etc/kubernetes/kubelet/kubelet-config.json\n\n```\n If there is an entry for `eventRecordQPS` check that it is set to 0 or an appropriate level for the cluster.\n\n **Audit Method 2:**\n\n If using the api configz endpoint consider searching for the status of `eventRecordQPS` by extracting the live configuration from the nodes running kubelet.\n\n Set the local proxy port and the following variables and provide proxy port number and node name;\n`HOSTNAME_PORT=\"localhost-and-port-number\"` `NODE_NAME=\"The-Name-Of-Node-To-Extract-Configuration\" from the output of \"kubectl get nodes\"`\n\n \n```\nkubectl proxy --port=8001 \u0026\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=ip-192.168.31.226.ec2.internal (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"\n\n```","impactStatement":"","defaultValue":"See the Amazon EKS documentation for the default value."},{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-4.2.9 Ensure that the --event-qps argument is set to 0 or a level which ensures appropriate event capture","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126668/recommendations/1838656"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the following command on each node:\n\n \n```\nps -ef | grep kubelet\n\n```\n Review the value set for the `--event-qps` argument and determine whether this has been set to an appropriate level for the cluster. The value of `0` can be used to ensure that all events are captured.\n\n If the `--event-qps` argument does not exist, check that there is a Kubelet config file specified by `--config` and review the value in this location.","configuration":[]},{"controlDetailsID":"C-0181","name":"Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate","description":"Setup TLS connection on the Kubelets.","category":"Control plane","subCategory":"","remediation":"If using a Kubelet config file, edit the file to set tlsCertFile to the location of the certificate file to use to identify this Kubelet, and tlsPrivateKeyFile to the location of the corresponding private key file.\n\n If using command line arguments, edit the kubelet service file /etc/kubernetes/kubelet.conf on each worker node and set the below parameters in KUBELET\\_CERTIFICATE\\_ARGS variable.\n\n --tls-cert-file=\u003cpath/to/tls-certificate-file\u003e --tls-private-key-file=\u003cpath/to/tls-key-file\u003e\nBased on your system, restart the kubelet service. For example:\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n\n```","framework":["cis-eks-t1.2.0","cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"cis-eks-t1.2.0","name":"CIS-3.2.10 Ensure that the --rotate-certificates argument is not present or is set to true","description":"Enable kubelet client certificate rotation.","long_description":"The `--rotate-certificates` setting causes the kubelet to rotate its client certificates by creating new CSRs as its existing credentials expire. This automated periodic rotation ensures that the there is no downtime due to expired certificates and thus addressing availability in the CIA (Confidentiality, Integrity, and Availability) security triad.\n\n **Note:** This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to implement rotation yourself.\n\n **Note:** This feature also requires the `RotateKubeletClientCertificate` feature gate to be enabled.","remediation":"**Remediation Method 1:**\n\n If modifying the Kubelet config file, edit the kubelet-config.json file `/etc/kubernetes/kubelet/kubelet-config.json` and set the below parameter to true\n\n \n```\n\"RotateCertificate\":true\n\n```\n Additionally, ensure that the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf does not set the --RotateCertificate executable argument to false because this would override the Kubelet config file.\n\n **Remediation Method 2:**\n\n If using executable arguments, edit the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` on each worker node and add the below parameter at the end of the `KUBELET_ARGS` variable string.\n\n \n```\n--RotateCertificate=true\n\n```","references":["https://github.com/kubernetes/kubernetes/pull/41912","https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/#kubelet-configuration","https://kubernetes.io/docs/imported/release/notes/","https://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/","https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/"],"manualCheck":"**Audit Method 1:**\n\n SSH to each node and run the following command to find the Kubelet process:\n\n \n```\nps -ef | grep kubelet\n\n```\n If the output of the command above includes the `--RotateCertificate` executable argument, verify that it is set to true.\nIf the output of the command above does not include the `--RotateCertificate` executable argument then check the Kubelet config file. The output of the above command should return something similar to `--config /etc/kubernetes/kubelet/kubelet-config.json` which is the location of the Kubelet config file.\n\n Open the Kubelet config file:\n\n \n```\ncat /etc/kubernetes/kubelet/kubelet-config.json\n\n```\n Verify that the `RotateCertificate` argument is not present, or is set to `true`.","impactStatement":"None","defaultValue":"See the Amazon EKS documentation for the default value."},{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-4.2.10 Ensure that the --tls-cert-file and --tls-private-key-file arguments are set as appropriate","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126668/recommendations/1838657"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the following command on each node:\n\n \n```\nps -ef | grep kubelet\n\n```\n Verify that the --tls-cert-file and --tls-private-key-file arguments exist and they are set as appropriate.\n\n If these arguments are not present, check that there is a Kubelet config specified by --config and that it contains appropriate settings for tlsCertFile and tlsPrivateKeyFile.","configuration":[]},{"controlDetailsID":"C-0182","name":"Ensure that the --rotate-certificates argument is not set to false","description":"Enable kubelet client certificate rotation.","category":"Control plane","subCategory":"","remediation":"If using a Kubelet config file, edit the file to add the line `rotateCertificates: true` or remove it altogether to use the default value.\n\n If using command line arguments, edit the kubelet service file `/etc/kubernetes/kubelet.conf` on each worker node and remove `--rotate-certificates=false` argument from the `KUBELET_CERTIFICATE_ARGS` variable.\n\n Based on your system, restart the `kubelet` service. For example:\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n\n```","framework":["cis-aks-t1.2.0","cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-3.2.10 Ensure that the --rotate-certificates argument is not set to false","description":"","long_description":"The `--rotate-certificates` setting causes the kubelet to rotate its client certificates by creating new CSRs as its existing credentials expire. This automated periodic rotation ensures that the there is no downtime due to expired certificates and thus addressing availability in the CIA (Confidentiality, Integrity, and Availability) security triad.\n\n **Note:** This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to implement rotation yourself.\n\n **Note:** This feature also requires the `RotateKubeletClientCertificate` feature gate to be enabled.","remediation":"**Remediation Method 1:**\n\n If modifying the Kubelet config file, edit the kubelet-config.json file `/etc/kubernetes/kubelet/kubelet-config.json` and set the below parameter to true\n\n \n```\n\"RotateCertificate\":true\n\n```\n Additionally, ensure that the kubelet service file /etc/systemd/system/kubelet.service.d/10-kubelet-args.conf does not set the --RotateCertificate executable argument to false because this would override the Kubelet config file.\n\n **Remediation Method 2:**\n\n If using executable arguments, edit the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` on each worker node and add the below parameter at the end of the `KUBELET_ARGS` variable string.\n\n \n```\n--RotateCertificate=true\n\n```","references":["\u003chttps://github.com/kubernetes/kubernetes/pull/41912\u003e\n\n  \u003chttps://kubernetes.io/docs/reference/command-line-tools-reference/kubelet-tls-bootstrapping/#kubelet-configuration\u003e\n\n  \u003chttps://kubernetes.io/docs/imported/release/notes/\u003e\n\n  \u003chttps://kubernetes.io/docs/reference/command-line-tools-reference/feature-gates/\u003e\n\n  \u003chttps://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/\u003e\n\n  \u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-data-protection#dp-4-encrypt-sensitive-information-in-transit\u003e"],"manualCheck":"**Audit Method 1:**\n\n SSH to each node and run the following command to find the Kubelet process:\n\n \n```\nps -ef | grep kubelet\n\n```\n If the output of the command above includes the `--RotateCertificate` executable argument, verify that it is set to true.\nIf the output of the command above does not include the `--RotateCertificate` executable argument then check the Kubelet config file. The output of the above command should return something similar to `--config /etc/kubernetes/kubelet/kubelet-config.json` which is the location of the Kubelet config file.\n\n Open the Kubelet config file:\n\n \n```\ncat /etc/kubernetes/kubelet/kubelet-config.json\n\n```\n Verify that the `RotateCertificate` argument is not present, or is set to `true`.","impactStatement":"","defaultValue":"See the AKS documentation for the default value."},{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-4.2.11 Ensure that the --rotate-certificates argument is not set to false","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126668/recommendations/1838658"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Run the following command on each node:\n\n \n```\nps -ef | grep kubelet\n\n```\n Verify that the `--rotate-certificates` argument is not present, or is set to `true`.\n\n If the `--rotate-certificates` argument is not present, verify that if there is a Kubelet config file specified by `--config`, that file does not contain `rotateCertificates: false`.","configuration":[]},{"controlDetailsID":"C-0183","name":"Verify that the RotateKubeletServerCertificate argument is set to true","description":"Enable kubelet server certificate rotation.","category":"Control plane","subCategory":"","remediation":"Edit the kubelet service file `/etc/kubernetes/kubelet.conf` on each worker node and set the below parameter in `KUBELET_CERTIFICATE_ARGS` variable.\n\n \n```\n--feature-gates=RotateKubeletServerCertificate=true\n\n```\n Based on your system, restart the `kubelet` service. For example:\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n\n```","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0","cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-3.2.11 Ensure that the RotateKubeletServerCertificate argument is set to true","description":"","long_description":"","remediation":"**Remediation Method 1:**\n\n If modifying the Kubelet config file, edit the kubelet-config.json file `/etc/kubernetes/kubelet/kubelet-config.json` and set the below parameter to true\n\n \n```\n\"RotateKubeletServerCertificate\":true\n\n```\n **Remediation Method 2:**\n\n If using a Kubelet config file, edit the file to set `RotateKubeletServerCertificate to true`.\n\n If using executable arguments, edit the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` on each worker node and add the below parameter at the end of the `KUBELET_ARGS` variable string.\n\n \n```\n--rotate-kubelet-server-certificate=true\n\n```\n **Remediation Method 3:**\n\n If using the api configz endpoint consider searching for the status of `\"RotateKubeletServerCertificate\":` by extracting the live configuration from the nodes running kubelet.\n\n \\*\\*See detailed step-by-step configmap procedures in [Reconfigure a Node's Kubelet in a Live Cluster](https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/), and then rerun the curl statement from audit process to check for kubelet configuration changes\n\n \n```\nkubectl proxy --port=8001 \u0026\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"\n\n```\n **For all three remediations:**\nBased on your system, restart the `kubelet` service and check status\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\nsystemctl status kubelet -l\n\n```","references":["\u003chttps://github.com/kubernetes/kubernetes/pull/45059\u003e\n\n  \u003chttps://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#kubelet-configuration\u003e"],"manualCheck":"**Audit Method 1:**\n\n If using a Kubelet configuration file, check that there is an entry for `RotateKubeletServerCertificate` is set to `true`.\n\n First, SSH to the relevant node:\n\n Run the following command on each node to find the appropriate Kubelet config file:\n\n \n```\nps -ef | grep kubelet\n\n```\n The output of the above command should return something similar to `--config /etc/kubernetes/kubelet/kubelet-config.json` which is the location of the Kubelet config file.\n\n Open the Kubelet config file:\n\n \n```\ncat /etc/kubernetes/kubelet/kubelet-config.json\n\n```\n Verify that `RotateKubeletServerCertificate` argument exists and is set to `true`.\n\n **Audit Method 2:**\n\n If using the api configz endpoint consider searching for the status of `\"RotateKubeletServerCertificate\":true` by extracting the live configuration from the nodes running kubelet.\n\n Set the local proxy port and the following variables and provide proxy port number and node name;\n`HOSTNAME_PORT=\"localhost-and-port-number\"` `NODE_NAME=\"The-Name-Of-Node-To-Extract-Configuration\" from the output of \"kubectl get nodes\"`\n\n \n```\nkubectl proxy --port=8001 \u0026\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=ip-192.168.31.226.aks.internal (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"\n\n```","impactStatement":"","defaultValue":"See the AKS documentation for the default value."},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-3.2.11 Ensure that the RotateKubeletServerCertificate argument is set to true","description":"","long_description":"`RotateKubeletServerCertificate` causes the kubelet to both request a serving certificate after bootstrapping its client credentials and rotate the certificate as its existing credentials expire. This automated periodic rotation ensures that the there are no downtimes due to expired certificates and thus addressing availability in the CIA (Confidentiality, Integrity, and Availability) security triad.\n\n Note: This recommendation only applies if you let kubelets get their certificates from the API server. In case your kubelet certificates come from an outside authority/tool (e.g. Vault) then you need to implement rotation yourself.","remediation":"**Remediation Method 1:**\n\n If modifying the Kubelet config file, edit the kubelet-config.json file `/etc/kubernetes/kubelet/kubelet-config.json` and set the below parameter to true\n\n \n```\n\"featureGates\": {\n  \"RotateKubeletServerCertificate\":true\n},\n\n```\n Additionally, ensure that the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` does not set the `--rotate-kubelet-server-certificate` executable argument to false because this would override the Kubelet config file.\n\n **Remediation Method 2:**\n\n If using executable arguments, edit the kubelet service file `/etc/systemd/system/kubelet.service.d/10-kubelet-args.conf` on each worker node and add the below parameter at the end of the `KUBELET_ARGS` variable string.\n\n \n```\n--rotate-kubelet-server-certificate=true\n\n```\n **Remediation Method 3:**\n\n If using the api configz endpoint consider searching for the status of `\"RotateKubeletServerCertificate\":` by extracting the live configuration from the nodes running kubelet.\n\n \\*\\*See detailed step-by-step configmap procedures in [Reconfigure a Node's Kubelet in a Live Cluster](https://kubernetes.io/docs/tasks/administer-cluster/reconfigure-kubelet/), and then rerun the curl statement from audit process to check for kubelet configuration changes\n\n \n```\nkubectl proxy --port=8001 \u0026\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=ip-192.168.31.226.ec2.internal (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"\n\n```\n **For all three remediation methods:**\nRestart the `kubelet` service and check status. The example below is for when using systemctl to manage services:\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\nsystemctl status kubelet -l\n\n```","references":["https://github.com/kubernetes/kubernetes/pull/45059","https://kubernetes.io/docs/admin/kubelet-tls-bootstrapping/#kubelet-configuration"],"manualCheck":"**Audit Method 1:**\n\n First, SSH to each node:\n\n Run the following command on each node to find the Kubelet process:\n\n \n```\nps -ef | grep kubelet\n\n```\n If the output of the command above includes the `--rotate-kubelet-server-certificate` executable argument verify that it is set to true.\n\n If the process does not have the `--rotate-kubelet-server-certificate` executable argument then check the Kubelet config file. The output of the above command should return something similar to `--config /etc/kubernetes/kubelet/kubelet-config.json` which is the location of the Kubelet config file.\n\n Open the Kubelet config file:\n\n \n```\ncat /etc/kubernetes/kubelet/kubelet-config.json\n\n```\n Verify that `RotateKubeletServerCertificate` argument exists in the `featureGates` section and is set to `true`.\n\n **Audit Method 2:**\n\n If using the api configz endpoint consider searching for the status of `\"RotateKubeletServerCertificate\":true` by extracting the live configuration from the nodes running kubelet.\n\n Set the local proxy port and the following variables and provide proxy port number and node name;\n`HOSTNAME_PORT=\"localhost-and-port-number\"` `NODE_NAME=\"The-Name-Of-Node-To-Extract-Configuration\" from the output of \"kubectl get nodes\"`\n\n \n```\nkubectl proxy --port=8001 \u0026\n\nexport HOSTNAME_PORT=localhost:8001 (example host and port number)\nexport NODE_NAME=ip-192.168.31.226.ec2.internal (example node name from \"kubectl get nodes\")\n\ncurl -sSL \"http://${HOSTNAME_PORT}/api/v1/nodes/${NODE_NAME}/proxy/configz\"\n\n```","impactStatement":"","defaultValue":"See the Amazon EKS documentation for the default value."},{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-4.2.12 Verify that the RotateKubeletServerCertificate argument is set to true","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126668/recommendations/1838661"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Ignore this check if serverTLSBootstrap is true in the kubelet config file or if the --rotate-server-certificates parameter is set on kubelet\n\n Run the following command on each node:\n\n \n```\nps -ef | grep kubelet\n\n```\n Verify that `RotateKubeletServerCertificate` argument exists and is set to `true`.","configuration":[]},{"controlDetailsID":"C-0184","name":"Ensure that the Kubelet only makes use of Strong Cryptographic Ciphers","description":"Ensure that the Kubelet is configured to only use strong cryptographic ciphers.","category":"Control plane","subCategory":"","remediation":"If using a Kubelet config file, edit the file to set `TLSCipherSuites:` to `TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256` or to a subset of these values.\n\n If using executable arguments, edit the kubelet service file `/etc/kubernetes/kubelet.conf` on each worker node and set the `--tls-cipher-suites` parameter as follows, or to a subset of these values.\n\n \n```\n --tls-cipher-suites=TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256,TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384,TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305,TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_256_GCM_SHA384,TLS_RSA_WITH_AES_128_GCM_SHA256\n\n```\n Based on your system, restart the `kubelet` service. For example:\n\n \n```\nsystemctl daemon-reload\nsystemctl restart kubelet.service\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-4.2.13 Ensure that the Kubelet only makes use of Strong Cryptographic Ciphers","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126668/recommendations/1838663"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"The set of cryptographic ciphers currently considered secure is the following:\n\n * `TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256`\n* `TLS_ECDHE_RSA_WITH_AES_128_GCM_SHA256`\n* `TLS_ECDHE_ECDSA_WITH_CHACHA20_POLY1305`\n* `TLS_ECDHE_RSA_WITH_AES_256_GCM_SHA384`\n* `TLS_ECDHE_RSA_WITH_CHACHA20_POLY1305`\n* `TLS_ECDHE_ECDSA_WITH_AES_256_GCM_SHA384`\n* `TLS_RSA_WITH_AES_256_GCM_SHA384`\n* `TLS_RSA_WITH_AES_128_GCM_SHA256`\n\n Run the following command on each node:\n\n \n```\nps -ef | grep kubelet\n\n```\n If the `--tls-cipher-suites` argument is present, ensure it only contains values included in this set.\n\n If it is not present check that there is a Kubelet config file specified by `--config`, and that file sets `TLSCipherSuites:` to only include values from this set.","configuration":[]},{"controlDetailsID":"C-0185","name":"Ensure that the cluster-admin role is only used where required","description":"The RBAC role `cluster-admin` provides wide-ranging powers over the environment and should be used only where and when needed.","category":"Access control","subCategory":"","remediation":"Identify all clusterrolebindings to the cluster-admin role. Check if they are used and if they need this role or if they could use a role with fewer privileges.\n\n Where possible, first bind users to a lower privileged role and then remove the clusterrolebinding to the cluster-admin role :\n\n \n```\nkubectl delete clusterrolebinding [name]\n\n```","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0","cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-4.1.1 Ensure that the cluster-admin role is only used where required","description":"","long_description":"","remediation":"","references":["\u003chttps://kubernetes.io/docs/admin/authorization/rbac/#user-facing-roles\u003e\n\n  \u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-privileged-access#pa-7-follow-just-enough-administration-least-privilege-principle\u003e"],"manualCheck":"Obtain a list of the principals who have access to the `cluster-admin` role by reviewing the `clusterrolebinding` output for each role binding that has access to the `cluster-admin` role.\n\n kubectl get clusterrolebindings -o=custom-columns=NAME:.metadata.name,ROLE:.roleRef.name,SUBJECT:.subjects[\\*].name\n\n Review each principal listed and ensure that `cluster-admin` privilege is required for it.","impactStatement":"","defaultValue":""},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-4.1.1 Ensure that the cluster-admin role is only used where required","description":"","long_description":"","remediation":"","references":["https://kubernetes.io/docs/admin/authorization/rbac/#user-facing-roles"],"manualCheck":"Obtain a list of the principals who have access to the `cluster-admin` role by reviewing the `clusterrolebinding` output for each role binding that has access to the `cluster-admin` role.\n\n kubectl get clusterrolebindings -o=custom-columns=NAME:.metadata.name,ROLE:.roleRef.name,SUBJECT:.subjects[\\*].name\n\n Review each principal listed and ensure that `cluster-admin` privilege is required for it.","impactStatement":"","defaultValue":""},{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-5.1.1 Ensure that the cluster-admin role is only used where required","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126661/recommendations/1838588"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Role","ClusterRole","ClusterRoleBinding"],"testCriteria":"Check which subjects have are bound to the cluster-admin role with a clusterrolebinding.","manualCheck":"Obtain a list of the principals who have access to the `cluster-admin` role by reviewing the `clusterrolebinding` output for each role binding that has access to the `cluster-admin` role.\n\n \n```\nkubectl get clusterrolebindings -o=custom-columns=NAME:.metadata.name,ROLE:.roleRef.name,SUBJECT:.subjects[*].name\n\n```\n Review each principal listed and ensure that `cluster-admin` privilege is required for it.","configuration":[]},{"controlDetailsID":"C-0186","name":"Minimize access to secrets","description":"The Kubernetes API stores secrets, which may be service account tokens for the Kubernetes API or credentials used by workloads in the cluster. Access to these secrets should be restricted to the smallest possible group of users to reduce the risk of privilege escalation.","category":"Access control","subCategory":"","remediation":"Where possible, remove `get`, `list` and `watch` access to `secret` objects in the cluster.","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0","cis-v1.23-t1.0.1","SOC2"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-4.1.2 Minimize access to secrets","description":"","long_description":"","remediation":"","references":["\u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-identity-management#im-7-eliminate-unintended-credential-exposure\u003e"],"manualCheck":"","impactStatement":"","defaultValue":"By default, the following list of principals have `get` privileges on `secret` objects\n\n \n```\nCLUSTERROLEBINDING                                    SUBJECT                             TYPE            SA-NAMESPACE\ncluster-admin                                         system:masters                      Group           \nsystem:controller:clusterrole-aggregation-controller  clusterrole-aggregation-controller  ServiceAccount  kube-system\nsystem:controller:expand-controller                   expand-controller                   ServiceAccount  kube-system\nsystem:controller:generic-garbage-collector           generic-garbage-collector           ServiceAccount  kube-system\nsystem:controller:namespace-controller                namespace-controller                ServiceAccount  kube-system\nsystem:controller:persistent-volume-binder            persistent-volume-binder            ServiceAccount  kube-system\nsystem:kube-controller-manager                        system:kube-controller-manager      User \n\n```"},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-4.1.2 Minimize access to secrets","description":"","long_description":"","remediation":"","references":[],"manualCheck":"","impactStatement":"","defaultValue":"By default, the following list of principals have `get` privileges on `secret` objects\n\n \n```\nCLUSTERROLEBINDING                                    SUBJECT                             TYPE            SA-NAMESPACE\ncluster-admin                                         system:masters                      Group           \nsystem:controller:clusterrole-aggregation-controller  clusterrole-aggregation-controller  ServiceAccount  kube-system\nsystem:controller:expand-controller                   expand-controller                   ServiceAccount  kube-system\nsystem:controller:generic-garbage-collector           generic-garbage-collector           ServiceAccount  kube-system\nsystem:controller:namespace-controller                namespace-controller                ServiceAccount  kube-system\nsystem:controller:persistent-volume-binder            persistent-volume-binder            ServiceAccount  kube-system\nsystem:kube-controller-manager                        system:kube-controller-manager      User \n\n```"},{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-5.1.2 Minimize access to secrets","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126661/recommendations/1838590"],"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"SOC2","name":"Cryptographic key management - minimize access to secrets (CC6.1,CC6.6,CC6.7)","description":"Encryption keys used to protect data at rest and in transit are stored and managed in accordance with the organization's cryptography policy. Access to encryption keys are restricted to authorized personnel.","long_description":"Encryption keys used to protect data at rest and in transit are stored and managed in accordance with the organization's cryptography policy. Access to encryption keys are restricted to authorized personnel.","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Role","ClusterRole","ClusterRoleBinding","RoleBinding"],"testCriteria":"Check which subjects have RBAC permissions to get, list or watch Kubernetes secrets.","manualCheck":"Review the users who have `get`, `list` or `watch` access to `secrets` objects in the Kubernetes API.","configuration":[]},{"controlDetailsID":"C-0187","name":"Minimize wildcard use in Roles and ClusterRoles","description":"Kubernetes Roles and ClusterRoles provide access to resources based on sets of objects and actions that can be taken on those objects. It is possible to set either of these to be the wildcard \"\\*\" which matches all items.\n\n Use of wildcards is not optimal from a security perspective as it may allow for inadvertent access to be granted when new resources are added to the Kubernetes API either as CRDs or in later versions of the product.","category":"Access control","subCategory":"","remediation":"Where possible replace any use of wildcards in clusterroles and roles with specific objects or actions.","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0","cis-v1.23-t1.0.1","ClusterScan"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-4.1.3 Minimize wildcard use in Roles and ClusterRoles","description":"","long_description":"","remediation":"","references":["\u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-privileged-access#pa-7-follow-just-enough-administration-least-privilege-principle\u003e"],"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-4.1.3 Minimize wildcard use in Roles and ClusterRoles","description":"","long_description":"","remediation":"","references":[],"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-5.1.3 Minimize wildcard use in Roles and ClusterRoles","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126661/recommendations/1838591"],"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ClusterScan","name":"Minimize wildcard use in Roles and ClusterRoles","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Role","ClusterRole","ClusterRoleBinding","RoleBinding"],"testCriteria":"Check which subjects have wildcard RBAC permissions.","manualCheck":"Retrieve the roles defined across each namespaces in the cluster and review for wildcards\n\n \n```\nkubectl get roles --all-namespaces -o yaml\n\n```\n Retrieve the cluster roles defined in the cluster and review for wildcards\n\n \n```\nkubectl get clusterroles -o yaml\n\n```","configuration":[]},{"controlDetailsID":"C-0188","name":"Minimize access to create pods","description":"The ability to create pods in a namespace can provide a number of opportunities for privilege escalation, such as assigning privileged service accounts to these pods or mounting hostPaths with access to sensitive data (unless Pod Security Policies are implemented to restrict this access)\n\n As such, access to create new pods should be restricted to the smallest possible group of users.","category":"Access control","subCategory":"","remediation":"Where possible, remove `create` access to `pod` objects in the cluster.","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0","cis-v1.23-t1.0.1","ClusterScan"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-4.1.4 Minimize access to create pods","description":"","long_description":"","remediation":"","references":["\u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-privileged-access#pa-7-follow-just-enough-administration-least-privilege-principle\u003e"],"manualCheck":"","impactStatement":"","defaultValue":"By default, the following list of principals have `create` privileges on `pod` objects\n\n \n```\nCLUSTERROLEBINDING                                    SUBJECT                             TYPE            SA-NAMESPACE\ncluster-admin                                         system:masters                      Group           \nsystem:controller:clusterrole-aggregation-controller  clusterrole-aggregation-controller  ServiceAccount  kube-system\nsystem:controller:daemon-set-controller               daemon-set-controller               ServiceAccount  kube-system\nsystem:controller:job-controller                      job-controller                      ServiceAccount  kube-system\nsystem:controller:persistent-volume-binder            persistent-volume-binder            ServiceAccount  kube-system\nsystem:controller:replicaset-controller               replicaset-controller               ServiceAccount  kube-system\nsystem:controller:replication-controller              replication-controller              ServiceAccount  kube-system\nsystem:controller:statefulset-controller              statefulset-controller              ServiceAccount  kube-system\n\n```"},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-4.1.4 Minimize access to create pods","description":"","long_description":"","remediation":"","references":[],"manualCheck":"","impactStatement":"","defaultValue":"By default, the following list of principals have `create` privileges on `pod` objects\n\n \n```\nCLUSTERROLEBINDING                                    SUBJECT                             TYPE            SA-NAMESPACE\ncluster-admin                                         system:masters                      Group           \nsystem:controller:clusterrole-aggregation-controller  clusterrole-aggregation-controller  ServiceAccount  kube-system\nsystem:controller:daemon-set-controller               daemon-set-controller               ServiceAccount  kube-system\nsystem:controller:job-controller                      job-controller                      ServiceAccount  kube-system\nsystem:controller:persistent-volume-binder            persistent-volume-binder            ServiceAccount  kube-system\nsystem:controller:replicaset-controller               replicaset-controller               ServiceAccount  kube-system\nsystem:controller:replication-controller              replication-controller              ServiceAccount  kube-system\nsystem:controller:statefulset-controller              statefulset-controller              ServiceAccount  kube-system\n\n```"},{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-5.1.4 Minimize access to create pods","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126661/recommendations/1838592"],"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ClusterScan","name":"Minimize access to create pods","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Role","ClusterRole","ClusterRoleBinding","RoleBinding"],"testCriteria":"Check which subjects have RBAC permissions to create pods.","manualCheck":"Review the users who have create access to pod objects in the Kubernetes API.","configuration":[]},{"controlDetailsID":"C-0189","name":"Ensure that default service accounts are not actively used","description":"The `default` service account should not be used to ensure that rights granted to applications can be more easily audited and reviewed.","category":"Workload","subCategory":"","remediation":"Create explicit service accounts wherever a Kubernetes workload requires specific access to the Kubernetes API server.\n\n Modify the configuration of each default service account to include this value\n\n \n```\nautomountServiceAccountToken: false\n\n```","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0","cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-4.1.5 Ensure that default service accounts are not actively used.","description":"","long_description":"","remediation":"Create explicit service accounts wherever a Kubernetes workload requires specific access to the Kubernetes API server.\n\n Modify the configuration of each default service account to include this value\n\n \n```\nautomountServiceAccountToken: false\n\n```\n Automatic remediation for the default account:\n\n `kubectl patch serviceaccount default -p $'automountServiceAccountToken: false'`","references":["\u003chttps://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\u003e\n\n  \u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-identity-management#im-2-manage-application-identities-securely-and-automatically\u003e"],"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-4.1.5 Ensure that default service accounts are not actively used.","description":"","long_description":"","remediation":"Create explicit service accounts wherever a Kubernetes workload requires specific access to the Kubernetes API server.\n\n Modify the configuration of each default service account to include this value\n\n \n```\nautomountServiceAccountToken: false\n\n```\n Automatic remediation for the default account:\n\n `kubectl patch serviceaccount default -p $'automountServiceAccountToken: false'`","references":["https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/","https://aws.github.io/aws-eks-best-practices/iam/#disable-auto-mounting-of-service-account-tokens"],"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-5.1.5 Ensure that default service accounts are not actively used","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126661/recommendations/1838594"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Namespace","ServiceAccount"],"testCriteria":"Checks that each namespace has at least one service account that isn't the default, and checks that the default service accounts have 'automountServiceAccountToken: false' set","manualCheck":"For each namespace in the cluster, review the rights assigned to the default service account and ensure that it has no roles or cluster roles bound to it apart from the defaults.\n\n Additionally ensure that the `automountServiceAccountToken: false` setting is in place for each default service account.","configuration":[]},{"controlDetailsID":"C-0190","name":"Ensure that Service Account Tokens are only mounted where necessary","description":"Service accounts tokens should not be mounted in pods except where the workload running in the pod explicitly needs to communicate with the API server","category":"Workload","subCategory":"","remediation":"Modify the definition of pods and service accounts which do not need to mount service account tokens to disable it.","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0","cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-4.1.6 Ensure that Service Account Tokens are only mounted where necessary","description":"","long_description":"","remediation":"","references":["\u003chttps://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/\u003e\n\n  \u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-identity-management#im-2-manage-application-identities-securely-and-automatically\u003e"],"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-4.1.6 Ensure that Service Account Tokens are only mounted where necessary","description":"","long_description":"","remediation":"","references":["https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/"],"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-5.1.6 Ensure that Service Account Tokens are only mounted where necessary","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126661/recommendations/1838595"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","ServiceAccount","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"Check that all service accounts and workloads disable automount of service account tokens.","manualCheck":"Review pod and service account objects in the cluster and ensure that the option below is set, unless the resource explicitly requires this access.\n\n \n```\nautomountServiceAccountToken: false\n\n```","configuration":[]},{"controlDetailsID":"C-0191","name":"Limit use of the Bind, Impersonate and Escalate permissions in the Kubernetes cluster","description":"Cluster roles and roles with the impersonate, bind or escalate permissions should not be granted unless strictly required. Each of these permissions allow a particular subject to escalate their privileges beyond those explicitly granted by cluster administrators","category":"Access control","subCategory":"","remediation":"Where possible, remove the impersonate, bind and escalate rights from subjects.","framework":["cis-eks-t1.2.0","cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-eks-t1.2.0","name":"CIS-4.1.8 Limit use of the Bind, Impersonate and Escalate permissions in the Kubernetes cluster","description":"","long_description":"","remediation":"","references":["https://www.impidio.com/blog/kubernetes-rbac-security-pitfalls","https://raesene.github.io/blog/2020/12/12/Escalating_Away/","https://raesene.github.io/blog/2021/01/16/Getting-Into-A-Bind-with-Kubernetes/"],"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-5.1.8 Limit use of the Bind, Impersonate and Escalate permissions in the Kubernetes cluster","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126661/recommendations/1838597"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Role","ClusterRole","ClusterRoleBinding","RoleBinding"],"testCriteria":"","manualCheck":"Review the users who have access to cluster roles or roles which provide the impersonate, bind or escalate privileges.","configuration":[]},{"controlDetailsID":"C-0192","name":"Ensure that the cluster has at least one active policy control mechanism in place","description":"Every Kubernetes cluster should have at least one policy control mechanism in place to enforce the other requirements in this section. This could be the in-built Pod Security Admission controller, or a third party policy control system.","category":"Network","subCategory":"","remediation":"Ensure that either Pod Security Admission or an external policy control system is in place for every namespace which contains user workloads.","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":4,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-5.2.1 Ensure that the cluster has at least one active policy control mechanism in place","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126662/recommendations/1838600"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Namespace","MutatingWebhookConfiguration"],"testCriteria":"Checks that every namespace enabled pod security admission, or if there are external policies applied for namespaced resources (validating/mutating webhooks)","manualCheck":"Pod Security Admission is enabled by default on all clusters using Kubernetes 1.23 or higher. To assess what controls, if any, are in place using this mechanism, review the namespaces in the cluster to see if the[required labels](https://kubernetes.io/docs/concepts/security/pod-security-admission/#pod-security-admission-labels-for-namespaces) have been applied\n\n \n```\nkubectl get namespaces -o yaml\n\n```\n To confirm if any external policy control system is in use, review the cluster for the presence of `validatingadmissionwebhook` and `mutatingadmissionwebhook` objects.\n\n \n```\nkubectl get validatingwebhookconfigurations\n\n```\n \n```\nkubectl get mutatingwebhookconfigurations\n\n```","configuration":[]},{"controlDetailsID":"C-0193","name":"Minimize the admission of privileged containers","description":"Do not generally permit containers to be run with the `securityContext.privileged` flag set to `true`.","category":"Workload","subCategory":"","remediation":"Add policies to each namespace in the cluster which has user workloads to restrict the admission of privileged containers.","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-5.2.2 Minimize the admission of privileged containers","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126662/recommendations/1838601"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Namespace","MutatingWebhookConfiguration"],"testCriteria":"","manualCheck":"List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of privileged containers.","configuration":[]},{"controlDetailsID":"C-0194","name":"Minimize the admission of containers wishing to share the host process ID namespace","description":"Do not generally permit containers to be run with the `hostPID` flag set to true.","category":"Workload","subCategory":"","remediation":"Add policies to each namespace in the cluster which has user workloads to restrict the admission of `hostPID` containers.","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-5.2.3 Minimize the admission of containers wishing to share the host process ID namespace","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126662/recommendations/1838602"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Namespace","MutatingWebhookConfiguration"],"testCriteria":"","manualCheck":"List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of `hostPID` containers","configuration":[]},{"controlDetailsID":"C-0195","name":"Minimize the admission of containers wishing to share the host IPC namespace","description":"Do not generally permit containers to be run with the `hostIPC` flag set to true.","category":"Workload","subCategory":"","remediation":"Add policies to each namespace in the cluster which has user workloads to restrict the admission of `hostIPC` containers.","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-5.2.4 Minimize the admission of containers wishing to share the host IPC namespace","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126662/recommendations/1838605"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Namespace","MutatingWebhookConfiguration"],"testCriteria":"","manualCheck":"List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of `hostIPC` containers","configuration":[]},{"controlDetailsID":"C-0196","name":"Minimize the admission of containers wishing to share the host network namespace","description":"Do not generally permit containers to be run with the `hostNetwork` flag set to true.","category":"Workload","subCategory":"","remediation":"Add policies to each namespace in the cluster which has user workloads to restrict the admission of `hostNetwork` containers.","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-5.2.5 Minimize the admission of containers wishing to share the host network namespace","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126662/recommendations/1838610"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Namespace","MutatingWebhookConfiguration"],"testCriteria":"","manualCheck":"List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of `hostNetwork` containers","configuration":[]},{"controlDetailsID":"C-0197","name":"Minimize the admission of containers with allowPrivilegeEscalation","description":"Do not generally permit containers to be run with the `allowPrivilegeEscalation` flag set to true. Allowing this right can lead to a process running a container getting more rights than it started with.\n\n It's important to note that these rights are still constrained by the overall container sandbox, and this setting does not relate to the use of privileged containers.","category":"Workload","subCategory":"","remediation":"Add policies to each namespace in the cluster which has user workloads to restrict the admission of conatiners with `.spec.allowPrivilegeEscalation`set to `true`.","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-5.2.6 Minimize the admission of containers with allowPrivilegeEscalation","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126662/recommendations/1838612"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Namespace","ValidatingWebhookConfiguration"],"testCriteria":"","manualCheck":"List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of containers which allow privilege escalation.","configuration":[]},{"controlDetailsID":"C-0198","name":"Minimize the admission of root containers","description":"Do not generally permit containers to be run as the root user.","category":"Workload","subCategory":"","remediation":"Create a policy for each namespace in the cluster, ensuring that either `MustRunAsNonRoot` or `MustRunAs` with the range of UIDs not including 0, is set.","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-5.2.7 Minimize the admission of root containers","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126662/recommendations/1838615"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Namespace","ValidatingWebhookConfiguration"],"testCriteria":"","manualCheck":"List the policies in use for each namespace in the cluster, ensure that each policy restricts the use of root containers by setting `MustRunAsNonRoot` or `MustRunAs` with the range of UIDs not including 0.","configuration":[]},{"controlDetailsID":"C-0199","name":"Minimize the admission of containers with the NET_RAW capability","description":"Do not generally permit containers with the potentially dangerous NET\\_RAW capability.","category":"Workload","subCategory":"","remediation":"Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers with the `NET_RAW` capability.","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-5.2.8 Minimize the admission of containers with the NET_RAW capability","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126662/recommendations/1838617"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Namespace","MutatingWebhookConfiguration"],"testCriteria":"","manualCheck":"List the policies in use for each namespace in the cluster, ensure that at least one policy disallows the admission of containers with the `NET_RAW` capability.","configuration":[]},{"controlDetailsID":"C-0200","name":"Minimize the admission of containers with added capabilities","description":"Do not generally permit containers with capabilities assigned beyond the default set.","category":"Workload","subCategory":"","remediation":"Ensure that `allowedCapabilities` is not present in policies for the cluster unless it is set to an empty array.","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-5.2.9 Minimize the admission of containers with added capabilities","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126662/recommendations/1838621"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Namespace","ValidatingWebhookConfiguration"],"testCriteria":"","manualCheck":"List the policies in use for each namespace in the cluster, ensure that policies are present which prevent `allowedCapabilities` to be set to anything other than an empty array.","configuration":[]},{"controlDetailsID":"C-0201","name":"Minimize the admission of containers with capabilities assigned","description":"Do not generally permit containers with capabilities","category":"Workload","subCategory":"","remediation":"Review the use of capabilites in applications runnning on your cluster. Where a namespace contains applicaions which do not require any Linux capabities to operate consider adding a policy which forbids the admission of containers which do not drop all capabilities.","framework":["cis-aks-t1.2.0","cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-4.2.8 Minimize the admission of containers with capabilities assigned","description":"","long_description":"","remediation":"Review the use of capabilities in applications running on your cluster. Where a namespace contains applications which do not require any Linux capabilities to operate consider adding a PSP which forbids the admission of containers which do not drop all capabilities.","references":["\u003chttps://kubernetes.io/docs/concepts/policy/pod-security-policy/#enabling-pod-security-policies\u003e\n\n  \u003chttps://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged-linux-containers/\u003e\n\n  \u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-privileged-access#pa-7-follow-just-enough-administration-least-privilege-principle\u003e"],"manualCheck":"Get the set of PSPs with the following command:\n\n \n```\nkubectl get psp\n\n```\n For each PSP, check whether capabilities have been forbidden:\n\n \n```\nkubectl get psp \u003cname\u003e -o=jsonpath='{.spec.requiredDropCapabilities}'\n\n```","impactStatement":"","defaultValue":"By default, PodSecurityPolicies are not defined."},{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-5.2.10 Minimize the admission of containers with capabilities assigned","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126662/recommendations/1838622"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Namespace","ValidatingWebhookConfiguration"],"testCriteria":"","manualCheck":"List the policies in use for each namespace in the cluster, ensure that at least one policy requires that capabilities are dropped by all containers.","configuration":[]},{"controlDetailsID":"C-0202","name":"Minimize the admission of Windows HostProcess Containers","description":"Do not generally permit Windows containers to be run with the `hostProcess` flag set to true.","category":"Workload","subCategory":"","remediation":"Add policies to each namespace in the cluster which has user workloads to restrict the admission of `hostProcess` containers.","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-5.2.11 Minimize the admission of Windows HostProcess Containers","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126662/recommendations/1838623"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Namespace","MutatingWebhookConfiguration"],"testCriteria":"","manualCheck":"List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of `hostProcess` containers","configuration":[]},{"controlDetailsID":"C-0203","name":"Minimize the admission of HostPath volumes","description":"Do not generally admit containers which make use of `hostPath` volumes.","category":"Workload","subCategory":"","remediation":"Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers which use `hostPath` volumes.","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-5.2.12 Minimize the admission of HostPath volumes","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126662/recommendations/1838625"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Namespace","MutatingWebhookConfiguration"],"testCriteria":"","manualCheck":"List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of containers with `hostPath` volumes.","configuration":[]},{"controlDetailsID":"C-0204","name":"Minimize the admission of containers which use HostPorts","description":"Do not generally permit containers which require the use of HostPorts.","category":"Workload","subCategory":"","remediation":"Add policies to each namespace in the cluster which has user workloads to restrict the admission of containers which use `hostPort` sections.","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":4,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-5.2.13 Minimize the admission of containers which use HostPorts","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126662/recommendations/1838626"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Namespace","MutatingWebhookConfiguration"],"testCriteria":"","manualCheck":"List the policies in use for each namespace in the cluster, ensure that each policy disallows the admission of containers which have `hostPort` sections.","configuration":[]},{"controlDetailsID":"C-0205","name":"Ensure that the CNI in use supports Network Policies","description":"There are a variety of CNI plugins available for Kubernetes. If the CNI in use does not support Network Policies it may not be possible to effectively restrict traffic in the cluster.","category":"Network","subCategory":"","remediation":"If the CNI plugin in use does not support network policies, consideration should be given to making use of a different plugin, or finding an alternate mechanism for restricting traffic in the Kubernetes cluster.","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0","cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":4,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-4.4.1 Ensure latest CNI version is used","description":"","long_description":"","remediation":"As with RBAC policies, network policies should adhere to the policy of least privileged access. Start by creating a deny all policy that restricts all inbound and outbound traffic from a namespace or create a global policy using Calico.","references":["\u003chttps://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/\u003e\n\n  \u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-network-security#ns-1-implement-security-for-internal-traffic\u003e"],"manualCheck":"Ensure CNI plugin supports network policies.","impactStatement":"None.","defaultValue":""},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-4.3.1 Ensure CNI plugin supports network policies.","description":"","long_description":"","remediation":"As with RBAC policies, network policies should adhere to the policy of least privileged access. Start by creating a deny all policy that restricts all inbound and outbound traffic from a namespace or create a global policy using Calico.","references":["https://kubernetes.io/docs/concepts/extend-kubernetes/compute-storage-net/network-plugins/","https://aws.github.io/aws-eks-best-practices/network/"],"manualCheck":"Review the documentation of CNI plugin in use by the cluster, and confirm that it supports network policies.","impactStatement":"None.","defaultValue":""},{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-5.3.1 Ensure that the CNI in use supports Network Policies","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126664/recommendations/1838627"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Review the documentation of CNI plugin in use by the cluster, and confirm that it supports Ingress and Egress network policies.","configuration":[]},{"controlDetailsID":"C-0206","name":"Ensure that all Namespaces have Network Policies defined","description":"Use network policies to isolate traffic in your cluster network.","category":"Network","subCategory":"","remediation":"Follow the documentation and create `NetworkPolicy` objects as you need them.","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0","cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":4,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-4.4.2 Ensure that all Namespaces have Network Policies defined","description":"","long_description":"Running different applications on the same Kubernetes cluster creates a risk of one compromised application attacking a neighboring application. Network segmentation is important to ensure that containers can communicate only with those they are supposed to. A network policy is a specification of how selections of pods are allowed to communicate with each other and other network endpoints.\n\n Once there is any Network Policy in a namespace selecting a particular pod, that pod will reject any connections that are not allowed by any Network Policy. Other pods in the namespace that are not selected by any Network Policy will continue to accept all traffic\"","remediation":"","references":["\u003chttps://kubernetes.io/docs/concepts/services-networking/networkpolicies/\u003e\n\n  \u003chttps://octetz.com/posts/k8s-network-policy-apis\u003e\n\n  \u003chttps://kubernetes.io/docs/tasks/configure-pod-container/declare-network-policy/\u003e\n\n  \u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-network-security#ns-1-implement-security-for-internal-traffic\u003e"],"manualCheck":"Run the below command and review the `NetworkPolicy` objects created in the cluster.\n\n \n```\nkubectl get networkpolicy --all-namespaces\n\n```\n Ensure that each namespace defined in the cluster has at least one Network Policy.","impactStatement":"Once there is any Network Policy in a namespace selecting a particular pod, that pod will reject any connections that are not allowed by any Network Policy. Other pods in the namespace that are not selected by any Network Policy will continue to accept all traffic\"","defaultValue":""},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-4.3.2 Ensure that all Namespaces have Network Policies defined","description":"","long_description":"Running different applications on the same Kubernetes cluster creates a risk of one compromised application attacking a neighboring application. Network segmentation is important to ensure that containers can communicate only with those they are supposed to. A network policy is a specification of how selections of pods are allowed to communicate with each other and other network endpoints.\n\n Once there is any Network Policy in a namespace selecting a particular pod, that pod will reject any connections that are not allowed by any Network Policy. Other pods in the namespace that are not selected by any Network Policy will continue to accept all traffic\"","remediation":"","references":["https://kubernetes.io/docs/concepts/services-networking/networkpolicies/","https://octetz.com/posts/k8s-network-policy-apis","https://kubernetes.io/docs/tasks/configure-pod-container/declare-network-policy/"],"manualCheck":"Run the below command and review the `NetworkPolicy` objects created in the cluster.\n\n \n```\nkubectl get networkpolicy --all-namespaces\n\n```\n Ensure that each namespace defined in the cluster has at least one Network Policy.","impactStatement":"Once there is any Network Policy in a namespace selecting a particular pod, that pod will reject any connections that are not allowed by any Network Policy. Other pods in the namespace that are not selected by any Network Policy will continue to accept all traffic\"","defaultValue":""},{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-5.3.2 Ensure that all Namespaces have Network Policies defined","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126664/recommendations/1838628"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Namespace","NetworkPolicy"],"testCriteria":"Check for each namespace if there is a network policy defined.","manualCheck":"Run the below command and review the `NetworkPolicy` objects created in the cluster.\n\n \n```\nkubectl --all-namespaces get networkpolicy\n\n```\n Ensure that each namespace defined in the cluster has at least one Network Policy.","configuration":[]},{"controlDetailsID":"C-0207","name":"Prefer using secrets as files over secrets as environment variables","description":"Kubernetes supports mounting secrets as data volumes or as environment variables. Minimize the use of environment variable secrets.","category":"Workload","subCategory":"Secrets","remediation":"If possible, rewrite application code to read secrets from mounted secret files, rather than from environment variables.","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0","cis-v1.23-t1.0.1","WorkloadScan"],"prerequisites":{"cloudProviders":null},"severity":4,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-4.5.1 Prefer using secrets as files over secrets as environment variables","description":"","long_description":"","remediation":"","references":["\u003chttps://kubernetes.io/docs/concepts/configuration/secret/#using-secrets\u003e\n\n  \u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-identity-management#im-7-eliminate-unintended-credential-exposure\u003e"],"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-4.4.1 Prefer using secrets as files over secrets as environment variables","description":"","long_description":"","remediation":"","references":["https://kubernetes.io/docs/concepts/configuration/secret/#using-secrets"],"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-5.4.1 Prefer using secrets as files over secrets as environment variables","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126665/recommendations/1838630"],"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"WorkloadScan","name":"Prefer using secrets as files over secrets as environment variables","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"Check if pods have secrets in their environment variables","manualCheck":"Run the following command to find references to objects which use environment variables defined from secrets.\n\n \n```\nkubectl get all -o jsonpath='{range .items[?(@..secretKeyRef)]} {.kind} {.metadata.name} {\"\\n\"}{end}' -A\n\n```","configuration":[]},{"controlDetailsID":"C-0208","name":"Consider external secret storage","description":"Consider the use of an external secrets storage and management system, instead of using Kubernetes Secrets directly, if you have more complex secret management needs. Ensure the solution requires authentication to access secrets, has auditing of access to and use of secrets, and encrypts secrets. Some solutions also make it easier to rotate secrets.","category":"Control plane","subCategory":"","remediation":"Refer to the secrets management options offered by your cloud provider or a third-party secrets management solution.","framework":["cis-aks-t1.2.0","cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-4.5.2 Consider external secret storage","description":"","long_description":"Kubernetes supports secrets as first-class objects, but care needs to be taken to ensure that access to secrets is carefully limited. Using an external secrets provider can ease the management of access to secrets, especially where secrests are used across both Kubernetes and non-Kubernetes environments.","remediation":"","references":["\u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-identity-management#im-7-eliminate-unintended-credential-exposure\u003e"],"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-5.4.2 Consider external secret storage","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126665/recommendations/1838631"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"Checking encryption configuration to see if secrets are managed externally by kms using aws, azure, or akeyless vault","manualCheck":"Review your secrets management implementation.","configuration":[]},{"controlDetailsID":"C-0209","name":"Create administrative boundaries between resources using namespaces","description":"Use namespaces to isolate your Kubernetes objects.","category":"Workload","subCategory":"","remediation":"Follow the documentation and create namespaces for objects in your deployment as you need them.","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0","cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-4.7.1 Create administrative boundaries between resources using namespaces","description":"","long_description":"Limiting the scope of user permissions can reduce the impact of mistakes or malicious activities. A Kubernetes namespace allows you to partition created resources into logically named groups. Resources created in one namespace can be hidden from other namespaces. By default, each resource created by a user in an Azure AKS cluster runs in a default namespace, called `default`. You can create additional namespaces and attach resources and users to them. You can use Kubernetes Authorization plugins to create policies that segregate access to namespace resources between different users.","remediation":"","references":["\u003chttps://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/\u003e\n\n  \u003chttp://blog.kubernetes.io/2016/08/security-best-practices-kubernetes-deployment.html\u003e\n\n  \u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-governance-strategy#gs-1-define-asset-management-and-data-protection-strategy\u003e\n\n  \u003chttps://docs.microsoft.com/en-us/azure/aks/concepts-clusters-workloads#:~:text=Kubernetes%20resources%2C%20such%20as%20pods,or%20manage%20access%20to%20resources.\u0026text=When%20you%20interact%20with%20the,used%20when%20none%20is%20specified\u003e."],"manualCheck":"","impactStatement":"","defaultValue":"When you create an AKS cluster, the following namespaces are available:\n\n NAMESPACES\nNamespace Description\ndefault Where pods and deployments are created by default when none is provided. In smaller environments, you can deploy applications directly into the default namespace without creating additional logical separations. When you interact with the Kubernetes API, such as with kubectl get pods, the default namespace is used when none is specified.\nkube-system Where core resources exist, such as network features like DNS and proxy, or the Kubernetes dashboard. You typically don't deploy your own applications into this namespace.\nkube-public Typically not used, but can be used for resources to be visible across the whole cluster, and can be viewed by any user."},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-4.6.1 Create administrative boundaries between resources using namespaces","description":"","long_description":"Limiting the scope of user permissions can reduce the impact of mistakes or malicious activities. A Kubernetes namespace allows you to partition created resources into logically named groups. Resources created in one namespace can be hidden from other namespaces. By default, each resource created by a user in an Amazon EKS cluster runs in a default namespace, called `default`. You can create additional namespaces and attach resources and users to them. You can use Kubernetes Authorization plugins to create policies that segregate access to namespace resources between different users.","remediation":"","references":["https://kubernetes.io/docs/concepts/overview/working-with-objects/namespaces/"],"manualCheck":"","impactStatement":"","defaultValue":"By default, Kubernetes starts with two initial namespaces:\n\n 1. `default` - The default namespace for objects with no other namespace\n2. `kube-system` - The namespace for objects created by the Kubernetes system\n3. `kube-public` - The namespace for public-readable ConfigMap\n4. `kube-node-lease` - The namespace for associated lease object for each node"},{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-5.7.1 Create administrative boundaries between resources using namespaces","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126667/recommendations/1838633"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Namespace"],"testCriteria":"Lists all namespaces in cluster for user to review","manualCheck":"Run the below command and review the namespaces created in the cluster.\n\n \n```\nkubectl get namespaces\n\n```\n Ensure that these namespaces are the ones you need and are adequately administered as per your requirements.","configuration":[]},{"controlDetailsID":"C-0210","name":"Ensure that the seccomp profile is set to docker/default in your pod definitions","description":"Enable `docker/default` seccomp profile in your pod definitions.","category":"Workload","subCategory":"","remediation":"Use security context to enable the `docker/default` seccomp profile in your pod definitions. An example is as below:\n\n \n```\n  securityContext:\n    seccompProfile:\n      type: RuntimeDefault\n\n```","framework":["cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":4,"frameworkOverrides":[{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-5.7.2 Ensure that the seccomp profile is set to docker/default in your pod definitions","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126667/recommendations/1838635"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"Checks if seccomp profile is defined as type RuntimeDefault in security context of workload or container level","manualCheck":"Review the pod definitions in your cluster. It should create a line as below:\n\n \n```\n  securityContext:\n    seccompProfile:\n      type: RuntimeDefault\n\n```","configuration":[]},{"controlDetailsID":"C-0211","name":"Apply Security Context to Your Pods and Containers","description":"Apply Security Context to Your Pods and Containers","category":"Workload","subCategory":"","remediation":"Follow the Kubernetes documentation and apply security contexts to your pods. For a suggested list of security contexts, you may refer to the CIS Security Benchmark for Docker Containers.","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0","cis-v1.23-t1.0.1","security"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-4.7.2 Apply Security Context to Your Pods and Containers","description":"","long_description":"","remediation":"As a best practice we recommend that you scope the binding for privileged pods to service accounts within a particular namespace, e.g. kube-system, and limiting access to that namespace. For all other serviceaccounts/namespaces, we recommend implementing a more restrictive policy such as this:\n\n \n```\napiVersion: policy/v1beta1\nkind: PodSecurityPolicy\nmetadata:\n    name: restricted\n    annotations:\n    seccomp.security.alpha.kubernetes.io/allowedProfileNames: 'docker/default,runtime/default'\n    apparmor.security.beta.kubernetes.io/allowedProfileNames: 'runtime/default'\n    seccomp.security.alpha.kubernetes.io/defaultProfileName:  'runtime/default'\n    apparmor.security.beta.kubernetes.io/defaultProfileName:  'runtime/default'\nspec:\n    privileged: false\n    # Required to prevent escalations to root.\n    allowPrivilegeEscalation: false\n    # This is redundant with non-root + disallow privilege escalation,\n    # but we can provide it for defense in depth.\n    requiredDropCapabilities:\n    - ALL\n    # Allow core volume types.\n    volumes:\n    - 'configMap'\n    - 'emptyDir'\n    - 'projected'\n    - 'secret'\n    - 'downwardAPI'\n    # Assume that persistentVolumes set up by the cluster admin are safe to use.\n    - 'persistentVolumeClaim'\n    hostNetwork: false\n    hostIPC: false\n    hostPID: false\n    runAsUser:\n    # Require the container to run without root privileges.\n    rule: 'MustRunAsNonRoot'\n    seLinux:\n    # This policy assumes the nodes are using AppArmor rather than SELinux.\n    rule: 'RunAsAny'\n    supplementalGroups:\n    rule: 'MustRunAs'\n    ranges:\n        # Forbid adding the root group.\n        - min: 1\n        max: 65535\n    fsGroup:\n    rule: 'MustRunAs'\n    ranges:\n        # Forbid adding the root group.\n        - min: 1\n        max: 65535\n    readOnlyRootFilesystem: false\n\n```\n This policy prevents pods from running as privileged or escalating privileges. It also restricts the types of volumes that can be mounted and the root supplemental groups that can be added.\n\n Another, albeit similar, approach is to start with policy that locks everything down and incrementally add exceptions for applications that need looser restrictions such as logging agents which need the ability to mount a host path.","references":["\u003chttps://kubernetes.io/docs/concepts/policy/security-context/\u003e\n\n  \u003chttps://learn.cisecurity.org/benchmarks\u003e\n\n  \u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-posture-vulnerability-management#pv-3-establish-secure-configurations-for-compute-resources\u003e"],"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-4.6.2 Apply Security Context to Your Pods and Containers","description":"","long_description":"A security context defines the operating system security settings (uid, gid, capabilities, SELinux role, etc..) applied to a container. When designing your containers and pods, make sure that you configure the security context for your pods, containers, and volumes. A security context is a property defined in the deployment yaml. It controls the security parameters that will be assigned to the pod/container/volume. There are two levels of security context: pod level security context, and container level security context.","remediation":"","references":["https://kubernetes.io/docs/tasks/configure-pod-container/security-context/"],"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-5.7.3 Apply Security Context to Your Pods and Containers","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126667/recommendations/1838636"],"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"security","name":"Apply Security Context to Your Pods and Containers","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob","PodSecurityPolicy"],"testCriteria":"Check that pod and container security context fields according to recommendations in CIS Security Benchmark for Docker Containers","manualCheck":"Review the pod definitions in your cluster and verify that you have security contexts defined as appropriate.","configuration":[]},{"controlDetailsID":"C-0212","name":"The default namespace should not be used","description":"Kubernetes provides a default namespace, where objects are placed if no namespace is specified for them. Placing objects in this namespace makes application of RBAC and other controls more difficult.","category":"Workload","subCategory":"","remediation":"Ensure that namespaces are created to allow for appropriate segregation of Kubernetes resources and that all new resources are created in a specific namespace.","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0","cis-v1.23-t1.0.1"],"prerequisites":{"cloudProviders":null},"severity":4,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-4.7.3 The default namespace should not be used","description":"","long_description":"","remediation":"","references":["\u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-posture-vulnerability-management#pv-3-establish-secure-configurations-for-compute-resources\u003e"],"manualCheck":"Run this command to list objects in default namespace\n\n \n```\nkubectl get all -n default\n\n```\n The only entries there should be system managed resources such as the `kubernetes` service","impactStatement":"","defaultValue":""},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-4.6.3 The default namespace should not be used","description":"","long_description":"","remediation":"","references":[],"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"cis-v1.23-t1.0.1","name":"CIS-5.7.4 The default namespace should not be used","description":"","long_description":"","remediation":"","references":["https://workbench.cisecurity.org/sections/1126667/recommendations/1838637"],"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Secret"],"testCriteria":"Lists all resources in default namespace for user to review and approve.","manualCheck":"Run this command to list objects in default namespace\n\n \n```\nkubectl get $(kubectl api-resources --verbs=list --namespaced=true -o name | paste -sd, -) --ignore-not-found -n default\n\n```\n The only entries there should be system managed resources such as the `kubernetes` service","configuration":[]},{"controlDetailsID":"C-0213","name":"Minimize the admission of privileged containers","description":"Do not generally permit containers to be run with the `securityContext.privileged` flag set to `true`.","category":"","subCategory":"","remediation":"Create a PSP as described in the Kubernetes documentation, ensuring that the `.spec.privileged` field is set to `false`.","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-4.2.1 Minimize the admission of privileged containers","description":"","long_description":"","remediation":"Create a PSP as described in the Kubernetes documentation, ensuring that the `.spec.privileged` field is omitted or set to `false`.","references":["\u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-governance-strategy#gs-2-define-enterprise-segmentation-strategy\u003e"],"manualCheck":"Get the set of PSPs with the following command:\n\n \n```\nkubectl get psp\n\n```\n as an alternative AZ CLI can be used:\n\n \n```\naz aks list --output yaml\n\n```\n For each PSP, check whether privileged is enabled:\n\n \n```\nkubectl get psp -o json\n\n```\n Verify that there is at least one PSP which does not return `true`.\n\n `kubectl get psp \u003cname\u003e -o=jsonpath='{.spec.privileged}'`","impactStatement":"","defaultValue":"By default, when you provision an AKS cluster, the value of \"enablePodSecurityPolicy\" is null."},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-4.2.1 Minimize the admission of privileged containers","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["PodSecurityPolicy"],"testCriteria":"","manualCheck":"Get the set of PSPs with the following command:\n\n \n```\nkubectl get psp\n\n```\n For each PSP, check whether privileged is enabled:\n\n \n```\nkubectl get psp -o json\n\n```\n Verify that there is at least one PSP which does not return `true`.\n\n `kubectl get psp \u003cname\u003e -o=jsonpath='{.spec.privileged}'`","configuration":[]},{"controlDetailsID":"C-0214","name":"Minimize the admission of containers wishing to share the host process ID namespace","description":"Do not generally permit containers to be run with the `hostPID` flag set to true.","category":"","subCategory":"","remediation":"Create a PSP as described in the Kubernetes documentation, ensuring that the `.spec.hostPID` field is omitted or set to false.","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-4.2.2 Minimize the admission of containers wishing to share the host process ID namespace","description":"","long_description":"","remediation":"","references":["\u003chttps://kubernetes.io/docs/concepts/policy/pod-security-policy\u003e\n\n  \u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-data-protection#dp-2-protect-sensitive-data\u003e"],"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-4.2.2 Minimize the admission of containers wishing to share the host process ID namespace","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["PodSecurityPolicy"],"testCriteria":"","manualCheck":"Get the set of PSPs with the following command:\n\n \n```\nkubectl get psp\n\n```\n For each PSP, check whether privileged is enabled:\n\n \n```\nkubectl get psp \u003cname\u003e -o=jsonpath='{.spec.hostPID}'\n\n```\n Verify that there is at least one PSP which does not return true.","configuration":[]},{"controlDetailsID":"C-0215","name":"Minimize the admission of containers wishing to share the host IPC namespace","description":"Do not generally permit containers to be run with the `hostIPC` flag set to true.","category":"","subCategory":"","remediation":"Create a PSP as described in the Kubernetes documentation, ensuring that the `.spec.hostIPC` field is omitted or set to false.","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-4.2.3 Minimize the admission of containers wishing to share the host IPC namespace","description":"","long_description":"","remediation":"","references":["\u003chttps://kubernetes.io/docs/concepts/policy/pod-security-policy\u003e\n\n  \u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-data-protection#dp-2-protect-sensitive-data\u003e"],"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-4.2.3 Minimize the admission of containers wishing to share the host IPC namespace","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["PodSecurityPolicy"],"testCriteria":"","manualCheck":"Get the set of PSPs with the following command:\n\n \n```\nkubectl get psp\n\n```\n For each PSP, check whether privileged is enabled:\n\n \n```\nkubectl get psp \u003cname\u003e -o=jsonpath='{.spec.hostIPC}'\n\n```\n Verify that there is at least one PSP which does not return true.","configuration":[]},{"controlDetailsID":"C-0216","name":"Minimize the admission of containers wishing to share the host network namespace","description":"Do not generally permit containers to be run with the `hostNetwork` flag set to true.","category":"","subCategory":"","remediation":"Create a PSP as described in the Kubernetes documentation, ensuring that the `.spec.hostNetwork` field is omitted or set to false.","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-4.2.4 Minimize the admission of containers wishing to share the host network namespace","description":"","long_description":"","remediation":"","references":["\u003chttps://kubernetes.io/docs/concepts/policy/pod-security-policy\u003e\n\n  \u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-data-protection#dp-2-protect-sensitive-data\u003e"],"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-4.2.4 Minimize the admission of containers wishing to share the host network namespace","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["PodSecurityPolicy"],"testCriteria":"","manualCheck":"Get the set of PSPs with the following command:\n\n \n```\nkubectl get psp\n\n```\n For each PSP, check whether privileged is enabled:\n\n \n```\nkubectl get psp \u003cname\u003e -o=jsonpath='{.spec.hostNetwork}'\n\n```\n Verify that there is at least one PSP which does not return true.","configuration":[]},{"controlDetailsID":"C-0217","name":"Minimize the admission of containers with allowPrivilegeEscalation","description":"Do not generally permit containers to be run with the `allowPrivilegeEscalation` flag set to true.","category":"","subCategory":"","remediation":"Create a PSP as described in the Kubernetes documentation, ensuring that the `.spec.allowPrivilegeEscalation` field is omitted or set to false.","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-4.2.5 Minimize the admission of containers with allowPrivilegeEscalation","description":"","long_description":"","remediation":"","references":["\u003chttps://kubernetes.io/docs/concepts/policy/pod-security-policy\u003e\n\n  \u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-privileged-access#pa-7-follow-just-enough-administration-least-privilege-principle\u003e"],"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-4.2.5 Minimize the admission of containers with allowPrivilegeEscalation","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["PodSecurityPolicy"],"testCriteria":"","manualCheck":"Get the set of PSPs with the following command:\n\n \n```\nkubectl get psp\n\n```\n For each PSP, check whether privileged is enabled:\n\n \n```\nkubectl get psp \u003cname\u003e -o=jsonpath='{.spec.allowPrivilegeEscalation}'\n\n```\n Verify that there is at least one PSP which does not return true.","configuration":[]},{"controlDetailsID":"C-0218","name":"Minimize the admission of root containers","description":"Do not generally permit containers to be run as the root user.","category":"","subCategory":"","remediation":"Create a PSP as described in the Kubernetes documentation, ensuring that the `.spec.runAsUser.rule` is set to either `MustRunAsNonRoot` or `MustRunAs` with the range of UIDs not including 0.","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-4.2.6 Minimize the admission of root containers","description":"","long_description":"","remediation":"","references":["\u003chttps://kubernetes.io/docs/concepts/policy/pod-security-policy/#enabling-pod-security-policies\u003e\n\n  \u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-privileged-access#pa-7-follow-just-enough-administration-least-privilege-principle\u003e"],"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-4.2.6 Minimize the admission of root containers","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["PodSecurityPolicy"],"testCriteria":"","manualCheck":"Get the set of PSPs with the following command:\n\n \n```\nkubectl get psp\n\n```\n For each PSP, check whether running containers as root is enabled:\n\n \n```\nkubectl get psp \u003cname\u003e -o=jsonpath='{.spec.runAsUser.rule}'\n\n```\n Verify that there is at least one PSP which returns `MustRunAsNonRoot` or `MustRunAs` with the range of UIDs not including 0.","configuration":[]},{"controlDetailsID":"C-0219","name":"Minimize the admission of containers with added capabilities","description":"Do not generally permit containers with capabilities assigned beyond the default set.","category":"","subCategory":"","remediation":"Ensure that `allowedCapabilities` is not present in PSPs for the cluster unless it is set to an empty array.","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-4.2.7 Minimize the admission of containers with added capabilities","description":"","long_description":"","remediation":"","references":["\u003chttps://kubernetes.io/docs/concepts/policy/pod-security-policy/#enabling-pod-security-policies\u003e\n\n  \u003chttps://www.nccgroup.trust/uk/our-research/abusing-privileged-and-unprivileged-linux-containers/\u003e\n\n  \u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-privileged-access#pa-7-follow-just-enough-administration-least-privilege-principle\u003e"],"manualCheck":"","impactStatement":"","defaultValue":"By default, PodSecurityPolicies are not defined."},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-4.2.7 Minimize the admission of containers with added capabilities","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["PodSecurityPolicy"],"testCriteria":"","manualCheck":"Get the set of PSPs with the following command:\n\n \n```\nkubectl get psp\n\n```\n Verify that there are no PSPs present which have `allowedCapabilities` set to anything other than an empty array.","configuration":[]},{"controlDetailsID":"C-0220","name":"Minimize the admission of containers with capabilities assigned","description":"Do not generally permit containers with capabilities","category":"","subCategory":"","remediation":"Review the use of capabilities in applications running on your cluster. Where a namespace contains applications which do not require any Linux capabilities to operate consider adding a PSP which forbids the admission of containers which do not drop all capabilities.","framework":["cis-eks-t1.2.0"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"cis-eks-t1.2.0","name":"CIS-4.2.8 Minimize the admission of containers with capabilities assigned","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["PodSecurityPolicy"],"testCriteria":"","manualCheck":"Get the set of PSPs with the following command:\n\n \n```\nkubectl get psp\n\n```\n For each PSP, check whether capabilities have been forbidden:\n\n \n```\nkubectl get psp \u003cname\u003e -o=jsonpath='{.spec.requiredDropCapabilities}'\n\n```","configuration":[]},{"controlDetailsID":"C-0221","name":"Ensure Image Vulnerability Scanning using Amazon ECR image scanning or a third party provider","description":"Scan images being deployed to Amazon EKS for vulnerabilities.","category":"","subCategory":"","remediation":"To utilize AWS ECR for Image scanning please follow the steps below:\n\n To create a repository configured for scan on push (AWS CLI)\n\n \n```\naws ecr create-repository --repository-name $REPO_NAME --image-scanning-configuration scanOnPush=true --region $REGION_CODE\n\n```\n To edit the settings of an existing repository (AWS CLI)\n\n \n```\naws ecr put-image-scanning-configuration --repository-name $REPO_NAME --image-scanning-configuration scanOnPush=true --region $REGION_CODE\n\n```\n Use the following steps to start a manual image scan using the AWS Management Console.2. Open the Amazon ECR console at\u003chttps://console.aws.amazon.com/ecr/repositories\u003e.\n3. From the navigation bar, choose the Region to create your repository in.\n4. In the navigation pane, choose Repositories.\n5. On the Repositories page, choose the repository that contains the image to scan.\n6. On the Images page, select the image to scan and then choose Scan.","framework":["cis-eks-t1.2.0"],"prerequisites":{"cloudProviders":["EKS"]},"severity":5,"frameworkOverrides":[{"frameworkName":"cis-eks-t1.2.0","name":"CIS-5.1.1 Ensure Image Vulnerability Scanning using Amazon ECR image scanning or a third party provider","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Please follow AWS ECS or your 3rd party image scanning provider's guidelines for enabling Image Scanning.","configuration":[]},{"controlDetailsID":"C-0222","name":"Minimize user access to Amazon ECR","description":"Restrict user access to Amazon ECR, limiting interaction with build images to only authorized personnel and service accounts.","category":"","subCategory":"","remediation":"Before you use IAM to manage access to Amazon ECR, you should understand what IAM features are available to use with Amazon ECR. To get a high-level view of how Amazon ECR and other AWS services work with IAM, see AWS Services That Work with IAM in the IAM User Guide.\n\n **Topics**\n\n * Amazon ECR Identity-Based Policies\n* Amazon ECR Resource-Based Policies\n* Authorization Based on Amazon ECR Tags\n* Amazon ECR IAM Roles\n\n **Amazon ECR Identity-Based Policies**\n\n With IAM identity-based policies, you can specify allowed or denied actions and resources as well as the conditions under which actions are allowed or denied. Amazon ECR supports specific actions, resources, and condition keys. To learn about all of the elements that you use in a JSON policy, see IAM JSON Policy Elements Reference in the IAM User Guide.\n\n **Actions**\nThe Action element of an IAM identity-based policy describes the specific action or actions that will be allowed or denied by the policy. Policy actions usually have the same name as the associated AWS API operation. The action is used in a policy to grant permissions to perform the associated operation.\n\n Policy actions in Amazon ECR use the following prefix before the action: ecr:. For example, to grant someone permission to create an Amazon ECR repository with the Amazon ECR CreateRepository API operation, you include the ecr:CreateRepository action in their policy. Policy statements must include either an Action or NotAction element. Amazon ECR defines its own set of actions that describe tasks that you can perform with this service.\n\n To specify multiple actions in a single statement, separate them with commas as follows:\n\n `\"Action\": [ \"ecr:action1\", \"ecr:action2\"`\n\n You can specify multiple actions using wildcards (\\*). For example, to specify all actions that begin with the word Describe, include the following action:\n\n `\"Action\": \"ecr:Describe*\"`\n\n To see a list of Amazon ECR actions, see Actions, Resources, and Condition Keys for Amazon Elastic Container Registry in the IAM User Guide.\n\n **Resources**\nThe Resource element specifies the object or objects to which the action applies. Statements must include either a Resource or a NotResource element. You specify a resource using an ARN or using the wildcard (\\*) to indicate that the statement applies to all resources.\n\n An Amazon ECR repository resource has the following ARN:\n\n `arn:${Partition}:ecr:${Region}:${Account}:repository/${Repository-name}`\n\n For more information about the format of ARNs, see Amazon Resource Names (ARNs) and AWS Service Namespaces.\n\n For example, to specify the my-repo repository in the us-east-1 Region in your statement, use the following ARN:\n\n `\"Resource\": \"arn:aws:ecr:us-east-1:123456789012:repository/my-repo\"`\n\n To specify all repositories that belong to a specific account, use the wildcard (\\*):\n\n `\"Resource\": \"arn:aws:ecr:us-east-1:123456789012:repository/*\"`\n\n To specify multiple resources in a single statement, separate the ARNs with commas.\n\n `\"Resource\": [ \"resource1\", \"resource2\"`\n\n To see a list of Amazon ECR resource types and their ARNs, see Resources Defined by Amazon Elastic Container Registry in the IAM User Guide. To learn with which actions you can specify the ARN of each resource, see Actions Defined by Amazon Elastic Container Registry.\n\n **Condition Keys**\nThe Condition element (or Condition block) lets you specify conditions in which a statement is in effect. The Condition element is optional. You can build conditional expressions that use condition operators, such as equals or less than, to match the condition in the policy with values in the request.\n\n If you specify multiple Condition elements in a statement, or multiple keys in a single Condition element, AWS evaluates them using a logical AND operation. If you specify multiple values for a single condition key, AWS evaluates the condition using a logical OR operation. All of the conditions must be met before the statement's permissions are granted.\n\n You can also use placeholder variables when you specify conditions. For example, you can grant an IAM user permission to access a resource only if it is tagged with their IAM user name. For more information, see IAM Policy Elements: Variables and Tags in the IAM User Guide.\n\n Amazon ECR defines its own set of condition keys and also supports using some global condition keys. To see all AWS global condition keys, see AWS Global Condition Context Keys in the IAM User Guide.\n\n Most Amazon ECR actions support the aws:ResourceTag and ecr:ResourceTag condition keys. For more information, see Using Tag-Based Access Control.\n\n To see a list of Amazon ECR condition keys, see Condition Keys Defined by Amazon Elastic Container Registry in the IAM User Guide. To learn with which actions and resources you can use a condition key, see Actions Defined by Amazon Elastic Container Registry.","framework":["cis-eks-t1.2.0"],"prerequisites":{"cloudProviders":["EKS"]},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-eks-t1.2.0","name":"CIS-5.1.2 Minimize user access to Amazon ECR","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0223","name":"Minimize cluster access to read-only for Amazon ECR","description":"Configure the Cluster Service Account with Storage Object Viewer Role to only allow read-only access to Amazon ECR.","category":"","subCategory":"","remediation":"You can use your Amazon ECR images with Amazon EKS, but you need to satisfy the following prerequisites.\n\n The Amazon EKS worker node IAM role (NodeInstanceRole) that you use with your worker nodes must possess the following IAM policy permissions for Amazon ECR.\n\n \n```\n{\n    \"Version\": \"2012-10-17\",\n    \"Statement\": [\n        {\n            \"Effect\": \"Allow\",\n            \"Action\": [\n                \"ecr:BatchCheckLayerAvailability\",\n                \"ecr:BatchGetImage\",\n                \"ecr:GetDownloadUrlForLayer\",\n                \"ecr:GetAuthorizationToken\"\n            ],\n            \"Resource\": \"*\"\n        }\n    ]\n}\n\n```","framework":["cis-eks-t1.2.0"],"prerequisites":{"cloudProviders":["EKS"]},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-eks-t1.2.0","name":"CIS-5.1.3 Minimize cluster access to read-only for Amazon ECR","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Review AWS ECS worker node IAM role (NodeInstanceRole) IAM Policy Permissions to verify that they are set and the minimum required level.\n\n If utilizing a 3rd party tool to scan images utilize the minimum required permission level required to interact with the cluster - generally this should be read-only.","configuration":[]},{"controlDetailsID":"C-0225","name":"Prefer using dedicated EKS Service Accounts","description":"Kubernetes workloads should not use cluster node service accounts to authenticate to Amazon EKS APIs. Each Kubernetes workload that needs to authenticate to other AWS services using AWS IAM should be provisioned with a dedicated Service account.","category":"","subCategory":"","remediation":"With IAM roles for service accounts on Amazon EKS clusters, you can associate an IAM role with a Kubernetes service account. This service account can then provide AWS permissions to the containers in any pod that uses that service account. With this feature, you no longer need to provide extended permissions to the worker node IAM role so that pods on that node can call AWS APIs.\n\n Applications must sign their AWS API requests with AWS credentials. This feature provides a strategy for managing credentials for your applications, similar to the way that Amazon EC2 instance profiles provide credentials to Amazon EC2 instances. Instead of creating and distributing your AWS credentials to the containers or using the Amazon EC2 instance’s role, you can associate an IAM role with a Kubernetes service account. The applications in the pod’s containers can then use an AWS SDK or the AWS CLI to make API requests to authorized AWS services.\n\n The IAM roles for service accounts feature provides the following benefits:\n\n * Least privilege — By using the IAM roles for service accounts feature, you no longer need to provide extended permissions to the worker node IAM role so that pods on that node can call AWS APIs. You can scope IAM permissions to a service account, and only pods that use that service account have access to those permissions. This feature also eliminates the need for third-party solutions such as kiam or kube2iam.\n* Credential isolation — A container can only retrieve credentials for the IAM role that is associated with the service account to which it belongs. A container never has access to credentials that are intended for another container that belongs to another pod.\n* Audit-ability — Access and event logging is available through CloudTrail to help ensure retrospective auditing.\n\n To get started, see list text hereEnabling IAM roles for service accounts on your cluster.\n\n For an end-to-end walkthrough using eksctl, see Walkthrough: Updating a DaemonSet to use IAM for service accounts.","framework":["cis-eks-t1.2.0"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"cis-eks-t1.2.0","name":"CIS-5.2.1 Prefer using dedicated EKS Service Accounts","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["ServiceAccount"],"testCriteria":"","manualCheck":"For each namespace in the cluster, review the rights assigned to the default service account and ensure that it has no roles or cluster roles bound to it apart from the defaults.\n\n Additionally ensure that the automountServiceAccountToken: false setting is in place for each default service account.","configuration":[]},{"controlDetailsID":"C-0226","name":"Prefer using a container-optimized OS when possible","description":"A container-optimized OS is an operating system image that is designed for secure managed hosting of containers on compute instances.\n\n Use cases for container-optimized OSes might include:\n\n * Docker container or Kubernetes support with minimal setup.\n* A small-secure container footprint.\n* An OS that is tested, hardened and verified for running Kubernetes nodes in your compute instances.","category":"","subCategory":"","remediation":"","framework":["cis-eks-t1.2.0"],"prerequisites":{"cloudProviders":["EKS"]},"severity":3,"frameworkOverrides":[{"frameworkName":"cis-eks-t1.2.0","name":"CIS-3.3.1 Prefer using a container-optimized OS when possible","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Node"],"testCriteria":"","manualCheck":"If a container-optimized OS is required examine the nodes in EC2 and click on their AMI to ensure that it is a container-optimized OS like Amazon Bottlerocket; or connect to the worker node and check its OS.","configuration":[]},{"controlDetailsID":"C-0227","name":"Restrict Access to the Control Plane Endpoint","description":"Enable Endpoint Private Access to restrict access to the cluster's control plane to only an allowlist of authorized IPs.","category":"","subCategory":"","remediation":"By enabling private endpoint access to the Kubernetes API server, all communication between your nodes and the API server stays within your VPC. You can also limit the IP addresses that can access your API server from the internet, or completely disable internet access to the API server.\n\n With this in mind, you can update your cluster accordingly using the AWS CLI to ensure that Private Endpoint Access is enabled.\n\n If you choose to also enable Public Endpoint Access then you should also configure a list of allowable CIDR blocks, resulting in restricted access from the internet. If you specify no CIDR blocks, then the public API server endpoint is able to receive and process requests from all IP addresses by defaulting to ['0.0.0.0/0'].\n\n For example, the following command would enable private access to the Kubernetes API as well as limited public access over the internet from a single IP address (noting the /32 CIDR suffix):\n\n `aws eks update-cluster-config --region $AWS_REGION --name $CLUSTER_NAME --resources-vpc-config endpointPrivateAccess=true,endpointPublicAccess=true,publicAccessCidrs=\"203.0.113.5/32\"`\n\n Note:\n\n The CIDR blocks specified cannot include reserved addresses.\nThere is a maximum number of CIDR blocks that you can specify. For more information, see the EKS Service Quotas link in the references section.\nFor more detailed information, see the EKS Cluster Endpoint documentation link in the references section.","framework":["cis-eks-t1.2.0"],"prerequisites":{"cloudProviders":["EKS"]},"severity":8,"frameworkOverrides":[{"frameworkName":"cis-eks-t1.2.0","name":"CIS-5.4.1 Restrict Access to the Control Plane Endpoint","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0228","name":"Ensure clusters are created with Private Endpoint Enabled and Public Access Disabled","description":"Disable access to the Kubernetes API from outside the node network if it is not required.","category":"","subCategory":"","remediation":"By enabling private endpoint access to the Kubernetes API server, all communication between your nodes and the API server stays within your VPC.\n\n With this in mind, you can update your cluster accordingly using the AWS CLI to ensure that Private Endpoint Access is enabled.\n\n For example, the following command would enable private access to the Kubernetes API and ensure that no public access is permitted:\n\n `aws eks update-cluster-config --region $AWS_REGION --name $CLUSTER_NAME --resources-vpc-config endpointPrivateAccess=true, endpointPublicAccess=false`\n\n Note: For more detailed information, see the EKS Cluster Endpoint documentation link in the references section.","framework":["cis-eks-t1.2.0"],"prerequisites":{"cloudProviders":["EKS"]},"severity":8,"frameworkOverrides":[{"frameworkName":"cis-eks-t1.2.0","name":"CIS-5.4.2 Ensure clusters are created with Private Endpoint Enabled and Public Access Disabled","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Check for private endpoint access to the Kubernetes API server","configuration":[]},{"controlDetailsID":"C-0229","name":"Ensure clusters are created with Private Nodes","description":"Disable public IP addresses for cluster nodes, so that they only have private IP addresses. Private Nodes are nodes with no public IP addresses.","category":"","subCategory":"","remediation":"\n```\naws eks update-cluster-config \\\n    --region region-code \\\n    --name my-cluster \\\n    --resources-vpc-config endpointPublicAccess=true,publicAccessCidrs=\"203.0.113.5/32\",endpointPrivateAccess=true\n\n```","framework":["cis-eks-t1.2.0"],"prerequisites":{"cloudProviders":["EKS"]},"severity":8,"frameworkOverrides":[{"frameworkName":"cis-eks-t1.2.0","name":"CIS-5.4.3 Ensure clusters are created with Private Nodes","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0230","name":"Ensure Network Policy is Enabled and set as appropriate","description":"Amazon EKS provides two ways to implement network policy. You choose a network policy option when you create an EKS cluster. The policy option can't be changed after the cluster is created:\nCalico Network Policies, an open-source network and network security solution founded by Tigera.\nBoth implementations use Linux IPTables to enforce the specified policies. Policies are translated into sets of allowed and disallowed IP pairs. These pairs are then programmed as IPTable filter rules.","category":"","subCategory":"","remediation":"","framework":["cis-eks-t1.2.0"],"prerequisites":{"cloudProviders":["EKS"]},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-eks-t1.2.0","name":"CIS-5.4.4 Ensure Network Policy is Enabled and set as appropriate","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0231","name":"Encrypt traffic to HTTPS load balancers with TLS certificates","description":"Encrypt traffic to HTTPS load balancers using TLS certificates.","category":"","subCategory":"","remediation":"","framework":["cis-eks-t1.2.0"],"prerequisites":{"cloudProviders":["EKS"]},"severity":5,"frameworkOverrides":[{"frameworkName":"cis-eks-t1.2.0","name":"CIS-5.4.5 Encrypt traffic to HTTPS load balancers with TLS certificates","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Service"],"testCriteria":"","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0232","name":"Manage Kubernetes RBAC users with AWS IAM Authenticator for Kubernetes or Upgrade to AWS CLI v1.16.156","description":"Amazon EKS uses IAM to provide authentication to your Kubernetes cluster through the AWS IAM Authenticator for Kubernetes. You can configure the stock kubectl client to work with Amazon EKS by installing the AWS IAM Authenticator for Kubernetes and modifying your kubectl configuration file to use it for authentication.","category":"","subCategory":"","remediation":"Refer to the '[Managing users or IAM roles for your cluster](https://docs.aws.amazon.com/eks/latest/userguide/add-user-role.html)' in Amazon EKS documentation.\n\n Note: If using AWS CLI version 1.16.156 or later there is no need to install the AWS IAM Authenticator anymore.\n\n The relevant AWS CLI commands, depending on the use case, are:\n\n \n```\naws eks update-kubeconfig\naws eks get-token\n\n```","framework":["cis-eks-t1.2.0"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"cis-eks-t1.2.0","name":"CIS-5.5.1 Manage Kubernetes RBAC users with AWS IAM Authenticator for Kubernetes or Upgrade to AWS CLI v1.16.156","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Role"],"testCriteria":"","manualCheck":"To Audit access to the namespace $NAMESPACE, assume the IAM role yourIAMRoleName for a user that you created, and then run the following command:\n\n \n```\n$ kubectl get role -n $NAMESPACE\n\n```\n The response lists the RBAC role that has access to this Namespace.","configuration":[]},{"controlDetailsID":"C-0233","name":"Consider Fargate for running untrusted workloads","description":"It is Best Practice to restrict or fence untrusted workloads when running in a multi-tenant environment.","category":"","subCategory":"","remediation":"**Create a Fargate profile for your cluster**\nBefore you can schedule pods running on Fargate in your cluster, you must define a Fargate profile that specifies which pods should use Fargate when they are launched. For more information, see AWS Fargate profile.\n\n **Note**\nIf you created your cluster with eksctl using the --fargate option, then a Fargate profile has already been created for your cluster with selectors for all pods in the kube-system and default namespaces. Use the following procedure to create Fargate profiles for any other namespaces you would like to use with Fargate.\n\n **via eksctl CLI**\nCreate your Fargate profile with the following eksctl command, replacing the variable text with your own values. You must specify a namespace, but the labels option is not required.\n\n \n```\neksctl create fargateprofile --cluster cluster_name --name fargate_profile_name --namespace kubernetes_namespace --labels key=value\n\n```\n **via AWS Management Console**\n\n To create a Fargate profile for a cluster with the AWS Management Console\n\n 1. Open the Amazon EKS console at \u003chttps://console.aws.amazon.com/eks/home#/clusters\u003e.\n2. Choose the cluster to create a Fargate profile for.\n3. Under Fargate profiles, choose Add Fargate profile.\n4. On the Configure Fargate profile page, enter the following information and choose Next.\n\n * For Name, enter a unique name for your Fargate profile.\n* For Pod execution role, choose the pod execution role to use with your Fargate profile. Only IAM roles with the eks-fargate-pods.amazonaws.com service principal are shown. If you do not see any roles listed here, you must create one. For more information, see Pod execution role.\n* For Subnets, choose the subnets to use for your pods. By default, all subnets in your cluster's VPC are selected. Only private subnets are supported for pods running on Fargate; you must deselect any public subnets.\n* For Tags, you can optionally tag your Fargate profile. These tags do not propagate to other resources associated with the profile, such as its pods.\n\n 5. On the Configure pods selection page, enter the following information and choose Next.\n\n * list text hereFor Namespace, enter a namespace to match for pods, such as kube-system or default.\n* Add Kubernetes labels to the selector that pods in the specified namespace must have to match the selector. For example, you could add the label infrastructure: fargate to the selector so that only pods in the specified namespace that also have the infrastructure: fargate Kubernetes label match the selector.\n\n 6. On the Review and create page, review the information for your Fargate profile and choose Create.","framework":["cis-eks-t1.2.0"],"prerequisites":{"cloudProviders":null},"severity":3,"frameworkOverrides":[{"frameworkName":"cis-eks-t1.2.0","name":"CIS-5.6.1 Consider Fargate for running untrusted workloads","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Node"],"testCriteria":"","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0234","name":"Consider external secret storage","description":"Consider the use of an external secrets storage and management system, instead of using Kubernetes Secrets directly, if you have more complex secret management needs. Ensure the solution requires authentication to access secrets, has auditing of access to and use of secrets, and encrypts secrets. Some solutions also make it easier to rotate secrets.","category":"","subCategory":"","remediation":"Refer to the secrets management options offered by your cloud provider or a third-party secrets management solution.","framework":["cis-eks-t1.2.0"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-eks-t1.2.0","name":"CIS-4.4.2 Consider external secret storage","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"","manualCheck":"Review your secrets management implementation.","configuration":[]},{"controlDetailsID":"C-0235","name":"Ensure that the kubelet configuration file has permissions set to 644 or more restrictive","description":"Ensure that if the kubelet refers to a configuration file with the `--config` argument, that file has permissions of 644 or more restrictive.","category":"","subCategory":"","remediation":"Run the following command (using the config file location identified in the Audit step)\n\n \n```\nchmod 644 /etc/kubernetes/kubelet/kubelet-config.json\n\n```","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-3.1.3 Ensure that the kubelet configuration file has permissions set to 644 or more restrictive","description":"","long_description":"","remediation":"","references":["\u003chttps://kubernetes.io/docs/tasks/administer-cluster/kubelet-config-file/\u003e\n\n  \u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-posture-vulnerability-management#pv-3-establish-secure-configurations-for-compute-resources\u003e"],"manualCheck":"","impactStatement":"","defaultValue":"See the Azure AKS documentation for the default value."},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-3.1.3 Ensure that the kubelet configuration file has permissions set to 644 or more restrictive","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"First, SSH to the relevant worker node:\n\n To check to see if the Kubelet Service is running:\n\n \n```\nsudo systemctl status kubelet\n\n```\n The output should return `Active: active (running) since..`\n\n Run the following command on each node to find the appropriate Kubelet config file:\n\n \n```\nps -ef | grep kubelet\n\n```\n The output of the above command should return something similar to `--config /etc/kubernetes/kubelet/kubelet-config.json` which is the location of the Kubelet config file.\n\n Run the following command:\n\n \n```\nstat -c %a /etc/kubernetes/kubelet/kubelet-config.json\n\n```\n The output of the above command is the Kubelet config file's permissions. Verify that the permissions are `644` or more restrictive.","configuration":[]},{"controlDetailsID":"C-0236","name":"Verify image signature","description":"Verifies the signature of each image with given public keys","category":"Workload","subCategory":"Supply chain","remediation":"Replace the image with an image that is signed correctly","framework":["ArmoBest"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"ArmoBest","name":"Verify image signature","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"","manualCheck":"","configuration":[{"path":"settings.postureControlInputs.trustedCosignPublicKeys","name":"Trusted Cosign public keys","description":"A list of trusted Cosign public keys that are used for validating container image signatures."}]},{"controlDetailsID":"C-0237","name":"Check if signature exists","description":"Ensures that all images contain some signature","category":"Workload","subCategory":"Supply chain","remediation":"Replace the image with a signed image","framework":["ArmoBest"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"ArmoBest","name":"Check if signature exists","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0238","name":"Ensure that the kubeconfig file permissions are set to 644 or more restrictive","description":"If kubelet is running, and if it is configured by a kubeconfig file, ensure that the proxy kubeconfig file has permissions of 644 or more restrictive.","category":"","subCategory":"","remediation":"Run the below command (based on the file location on your system) on the each worker\nnode. For example,\n\n \n```\nchmod 644 \u003ckubeconfig file\u003e\n\n```","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-3.1.1 Ensure that the kubeconfig file permissions are set to 644 or more restrictive","description":"If `kubelet` is running, and if it is configured by a kubeconfig file, ensure that the proxy kubeconfig file has permissions of 644 or more restrictive.","long_description":"","remediation":"","references":["\u003chttps://kubernetes.io/docs/admin/kube-proxy/\u003e\n\n  \u003chttps://docs.microsoft.com/security/benchmark/azure/security-controls-v2-posture-vulnerability-management#pv-3-establish-secure-configurations-for-compute-resources\u003e"],"manualCheck":"","impactStatement":"","defaultValue":"See the Azure AKS documentation for the default value."},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-3.1.1 Ensure that the kubeconfig file permissions are set to 644 or more restrictive","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"SSH to the worker nodes\n\n To check to see if the Kubelet Service is running:\n\n \n```\nsudo systemctl status kubelet\n\n```\n The output should return `Active: active (running) since..`\n\n Run the following command on each node to find the appropriate kubeconfig file:\n\n \n```\nps -ef | grep kubelet\n\n```\n The output of the above command should return something similar to `--kubeconfig /var/lib/kubelet/kubeconfig` which is the location of the kubeconfig file.\n\n Run this command to obtain the kubeconfig file permissions:\n\n \n```\nstat -c %a /var/lib/kubelet/kubeconfig\n\n```\n The output of the above command gives you the kubeconfig file's permissions.\n\n Verify that if a file is specified and it exists, the permissions are `644` or more restrictive.","configuration":[]},{"controlDetailsID":"C-0239","name":"Prefer using dedicated AKS Service Accounts","description":"Kubernetes workloads should not use cluster node service accounts to authenticate to Azure AKS APIs. Each Kubernetes workload that needs to authenticate to other Azure Web Services using IAM should be provisioned with a dedicated Service account.","category":"","subCategory":"","remediation":"Azure Active Directory integration\nThe security of AKS clusters can be enhanced with the integration of Azure Active Directory (AD). Built on decades of enterprise identity management, Azure AD is a multi-tenant, cloud-based directory, and identity management service that combines core directory services, application access management, and identity protection. With Azure AD, you can integrate on-premises identities into AKS clusters to provide a single source for account management and security.\n\n Azure Active Directory integration with AKS clusters\n\n With Azure AD-integrated AKS clusters, you can grant users or groups access to Kubernetes resources within a namespace or across the cluster. To obtain a kubectl configuration context, a user can run the az aks get-credentials command. When a user then interacts with the AKS cluster with kubectl, they're prompted to sign in with their Azure AD credentials. This approach provides a single source for user account management and password credentials. The user can only access the resources as defined by the cluster administrator.\n\n Azure AD authentication is provided to AKS clusters with OpenID Connect. OpenID Connect is an identity layer built on top of the OAuth 2.0 protocol. For more information on OpenID Connect, see the Open ID connect documentation. From inside of the Kubernetes cluster, Webhook Token Authentication is used to verify authentication tokens. Webhook token authentication is configured and managed as part of the AKS cluster.","framework":["cis-aks-t1.2.0"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-5.2.1 Prefer using dedicated AKS Service Accounts","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["ClusterRoleBinding","RoleBinding"],"testCriteria":"","manualCheck":"For each namespace in the cluster, review the rights assigned to the default service account and ensure that it has no roles or cluster roles bound to it apart from the defaults.","configuration":[]},{"controlDetailsID":"C-0240","name":"Ensure Network Policy is Enabled and set as appropriate","description":"When you run modern, microservices-based applications in Kubernetes, you often want to control which components can communicate with each other. The principle of least privilege should be applied to how traffic can flow between pods in an Azure Kubernetes Service (AKS) cluster. Let's say you likely want to block traffic directly to back-end applications. The Network Policy feature in Kubernetes lets you define rules for ingress and egress traffic between pods in a cluster.","category":"","subCategory":"","remediation":"","framework":["cis-aks-t1.2.0"],"prerequisites":{"cloudProviders":["AKS"]},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-5.4.4 Ensure Network Policy is Enabled and set as appropriate","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0241","name":"Use Azure RBAC for Kubernetes Authorization.","description":"The ability to manage RBAC for Kubernetes resources from Azure gives you the choice to manage RBAC for the cluster resources either using Azure or native Kubernetes mechanisms.","category":"","subCategory":"","remediation":"Set Azure RBAC as access system.","framework":["cis-aks-t1.2.0"],"prerequisites":{"cloudProviders":["AKS"]},"severity":7,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-5.2.2 Use Azure RBAC for Kubernetes Authorization","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0242","name":"Hostile multi-tenant workloads","description":"Currently, Kubernetes environments aren't safe for hostile multi-tenant usage. Extra security features, like Pod Security Policies or Kubernetes RBAC for nodes, efficiently block exploits. For true security when running hostile multi-tenant workloads, only trust a hypervisor. The security domain for Kubernetes becomes the entire cluster, not an individual node.\n\n For these types of hostile multi-tenant workloads, you should use physically isolated clusters. For more information on ways to isolate workloads, see Best practices for cluster isolation in AKS.","category":"","subCategory":"","remediation":"","framework":["cis-aks-t1.2.0","cis-eks-t1.2.0"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-5.6.2 Hostile multi-tenant workloads","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"cis-eks-t1.2.0","name":"CIS-5.6.2 Hostile multi-tenant workloads","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0243","name":"Ensure Image Vulnerability Scanning using Azure Defender image scanning or a third party provider","description":"Scan images being deployed to Azure (AKS) for vulnerabilities.\n\n Vulnerability scanning for images stored in Azure Container Registry is generally available in Azure Security Center. This capability is powered by Qualys, a leading provider of information security.\n\n When you push an image to Container Registry, Security Center automatically scans it, then checks for known vulnerabilities in packages or dependencies defined in the file.\n\n When the scan completes (after about 10 minutes), Security Center provides details and a security classification for each vulnerability detected, along with guidance on how to remediate issues and protect vulnerable attack surfaces.","category":"","subCategory":"","remediation":"","framework":["cis-aks-t1.2.0"],"prerequisites":{"cloudProviders":["AKS"]},"severity":5,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-5.1.1 Ensure Image Vulnerability Scanning using Azure Defender image scanning or a third party provider","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0244","name":"Ensure Kubernetes Secrets are encrypted","description":"Encryption at Rest is a common security requirement. In Azure, organizations can encrypt data at rest without the risk or cost of a custom key management solution. Organizations have the option of letting Azure completely manage Encryption at Rest. Additionally, organizations have various options to closely manage encryption or encryption keys.","category":"","subCategory":"","remediation":"","framework":["cis-aks-t1.2.0"],"prerequisites":{"cloudProviders":["AKS","EKS","GKE"]},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-5.3.1 Ensure Kubernetes Secrets are encrypted","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0245","name":"Encrypt traffic to HTTPS load balancers with TLS certificates","description":"Encrypt traffic to HTTPS load balancers using TLS certificates.","category":"","subCategory":"","remediation":"","framework":["cis-aks-t1.2.0"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-5.4.5 Encrypt traffic to HTTPS load balancers with TLS certificates","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Service","Ingress"],"testCriteria":"","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0246","name":"Avoid use of system:masters group","description":"The special group `system:masters` should not be used to grant permissions to any user or service account, except where strictly necessary (e.g. bootstrapping access prior to RBAC being fully available)","category":"","subCategory":"","remediation":"Remove the `system:masters` group from all users in the cluster.","framework":["cis-eks-t1.2.0"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"cis-eks-t1.2.0","name":"CIS-4.1.7 Avoid use of system:masters group","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"Review a list of all credentials which have access to the cluster and ensure that the group `system:masters` is not used.","configuration":[]},{"controlDetailsID":"C-0247","name":"Restrict Access to the Control Plane Endpoint","description":"Enable Endpoint Private Access to restrict access to the cluster's control plane to only an allowlist of authorized IPs.","category":"","subCategory":"","remediation":"","framework":["cis-aks-t1.2.0"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-5.4.1 Restrict Access to the Control Plane Endpoint","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0248","name":"Ensure clusters are created with Private Nodes","description":"Disable public IP addresses for cluster nodes, so that they only have private IP addresses. Private Nodes are nodes with no public IP addresses.","category":"","subCategory":"","remediation":"\n```\naz aks create \\\n--resource-group \u003cprivate-cluster-resource-group\u003e \\\n--name \u003cprivate-cluster-name\u003e \\\n--load-balancer-sku standard \\\n--enable-private-cluster \\\n--network-plugin azure \\\n--vnet-subnet-id \u003csubnet-id\u003e \\\n--docker-bridge-address \\\n--dns-service-ip \\\n--service-cidr \n\n```\n Where `--enable-private-cluster` is a mandatory flag for a private cluster.","framework":["cis-aks-t1.2.0"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-5.4.3 Ensure clusters are created with Private Nodes","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0249","name":"Restrict untrusted workloads","description":"Restricting unstrusted workloads can be achieved by using ACI along with AKS.\n\n What is ACI?\nACI lets you quickly deploy container instances without additional infrastructure overhead. When you connect with AKS, ACI becomes a secured, logical extension of your AKS cluster. The virtual nodes component, which is based on Virtual Kubelet, is installed in your AKS cluster that presents ACI as a virtual Kubernetes node. Kubernetes can then schedule pods that run as ACI instances through virtual nodes, not as pods on VM nodes directly in your AKS cluster.\n\n Your application requires no modification to use virtual nodes. Deployments can scale across AKS and ACI and with no delay as cluster autoscaler deploys new nodes in your AKS cluster.\n\n Virtual nodes are deployed to an additional subnet in the same virtual network as your AKS cluster. This virtual network configuration allows the traffic between ACI and AKS to be secured. Like an AKS cluster, an ACI instance is a secure, logical compute resource that is isolated from other users.","category":"","subCategory":"","remediation":"","framework":["cis-aks-t1.2.0"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-5.6.1 Restrict untrusted workloads","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0250","name":"Minimize cluster access to read-only for Azure Container Registry (ACR)","description":"Configure the Cluster Service Account with Storage Object Viewer Role to only allow read-only access to Azure Container Registry (ACR)","category":"","subCategory":"","remediation":"","framework":["cis-aks-t1.2.0"],"prerequisites":{"cloudProviders":["AKS"]},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-5.1.2 Minimize cluster access to read-only for Azure Container Registry (ACR)","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0251","name":"Minimize user access to Azure Container Registry (ACR)","description":"Restrict user access to Azure Container Registry (ACR), limiting interaction with build images to only authorized personnel and service accounts.","category":"","subCategory":"","remediation":"Azure Container Registry\nIf you use Azure Container Registry (ACR) as your container image store, you need to grant permissions to the service principal for your AKS cluster to read and pull images. Currently, the recommended configuration is to use the az aks create or az aks update command to integrate with a registry and assign the appropriate role for the service principal. For detailed steps, see Authenticate with Azure Container Registry from Azure Kubernetes Service.\n\n To avoid needing an Owner or Azure account administrator role, you can configure a service principal manually or use an existing service principal to authenticate ACR from AKS. For more information, see ACR authentication with service principals or Authenticate from Kubernetes with a pull secret.","framework":["cis-aks-t1.2.0"],"prerequisites":{"cloudProviders":["AKS"]},"severity":6,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-5.1.3 Minimize user access to Azure Container Registry (ACR)","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0252","name":"Ensure clusters are created with Private Endpoint Enabled and Public Access Disabled","description":"Disable access to the Kubernetes API from outside the node network if it is not required.","category":"","subCategory":"","remediation":"To use a private endpoint, create a new private endpoint in your virtual network then create a link between your virtual network and a new private DNS zone","framework":["cis-aks-t1.2.0"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-5.4.2 Ensure clusters are created with Private Endpoint Enabled and Public Access Disabled","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0253","name":"Deprecated Kubernetes image registry","description":"Kubernetes team has deprecated GCR (k8s.gcr.io) registry and recommends pulling Kubernetes components from the new registry (registry.k8s.io). This is mandatory from 1.27","category":"","subCategory":"","remediation":"Change the images to be pulled from the new registry (registry.k8s.io).","framework":["DevOpsBest"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"DevOpsBest","name":"Deprecated Kubernetes image registry","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"Checking images in kube-system namespace, if the registry of the image is from the old registry we raise an alert.","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0254","name":"Enable audit Logs","description":"With Azure Kubernetes Service (AKS), the control plane components such as the kube-apiserver and kube-controller-manager are provided as a managed service. You create and manage the nodes that run the kubelet and container runtime, and deploy your applications through the managed Kubernetes API server. To help troubleshoot your application and services, you may need to view the logs generated by these control plane components.\n\n To help collect and review data from multiple sources, Azure Monitor logs provides a query language and analytics engine that provides insights to your environment. A workspace is used to collate and analyze the data, and can integrate with other Azure services such as Application Insights and Security Center.","category":"","subCategory":"","remediation":"Azure audit logs are enabled and managed in the Azure portal. To enable log collection for the Kubernetes master components in your AKS cluster, open the Azure portal in a web browser and complete the following steps:\n\n 1. Select the resource group for your AKS cluster, such as myResourceGroup. Don't select the resource group that contains your individual AKS cluster resources, such as MC\\_myResourceGroup\\_myAKSCluster\\_eastus.\n2. On the left-hand side, choose Diagnostic settings.\n3. Select your AKS cluster, such as myAKSCluster, then choose to Add diagnostic setting.\n4. Enter a name, such as myAKSClusterLogs, then select the option to Send to Log Analytics.\n5. Select an existing workspace or create a new one. If you create a workspace, provide a workspace name, a resource group, and a location.\n6. In the list of available logs, select the logs you wish to enable. For this example, enable the kube-audit and kube-audit-admin logs. Common logs include the kube-apiserver, kube-controller-manager, and kube-scheduler. You can return and change the collected logs once Log Analytics workspaces are enabled.\n7. When ready, select Save to enable collection of the selected logs.","framework":["cis-aks-t1.2.0"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"cis-aks-t1.2.0","name":"CIS-2.1.1 Enable audit Logs","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":[],"testCriteria":"","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0255","name":"Workload with secret access","description":"This control identifies workloads that have mounted secrets. Workloads with secret access can potentially expose sensitive information and increase the risk of unauthorized access to critical resources.","category":"","subCategory":"","remediation":"Review the workloads identified by this control and assess whether it's necessary to mount these secrets. Remove secret access from workloads that don't require it or ensure appropriate access controls are in place to protect sensitive information.","framework":["security"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"security","name":"Workload with secret access","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Secret","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"Check if any workload has mounted secrets by inspecting their specifications and verifying if secret volumes are defined.","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0256","name":"External facing","description":"This control detect workloads that are exposed on Internet through a Service (NodePort or LoadBalancer) or Ingress. It fails in case it find workloads connected with these resources.","category":"","subCategory":"","remediation":"The user can evaluate its exposed resources and apply relevant changes wherever needed.","framework":["ClusterScan","security"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"ClusterScan","name":"External facing","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"security","name":"External facing","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Service","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob","Ingress"],"testCriteria":"Checks if workloads are exposed through the use of NodePort, LoadBalancer or Ingress","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0257","name":"Workload with PVC access","description":"This control detects workloads that have mounted PVC. Workloads with PVC access can potentially expose sensitive information and elevate the risk of unauthorized access to critical resources.","category":"Workload","subCategory":"Storage","remediation":"Review the workloads identified by this control and assess whether it's necessary to mount these PVCs. Remove PVC access from workloads that don't require it or ensure appropriate access controls are in place to protect sensitive information.","framework":["security","WorkloadScan"],"prerequisites":{"cloudProviders":null},"severity":4,"frameworkOverrides":[{"frameworkName":"security","name":"Workload with PVC access","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"WorkloadScan","name":"Workload with PVC access","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","ConfigMap","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"Check if any workload has mounted PVCs by inspecting their specifications and verifying if PVC volumes are defined","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0258","name":"Workload with ConfigMap access","description":"This control detects workloads that have mounted ConfigMaps. Workloads with ConfigMap access can potentially expose sensitive information and elevate the risk of unauthorized access to critical resources.","category":"","subCategory":"","remediation":"Review the workloads identified by this control and assess whether it's necessary to mount these configMaps. Remove configMaps access from workloads that don't require it or ensure appropriate access controls are in place to protect sensitive information.","framework":["security"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"security","name":"Workload with configMap access","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","ConfigMap","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"Check if any workload has mounted secrets by inspecting their specifications and verifying if secret volumes are defined","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0259","name":"Workload with credential access","description":"This control checks if workloads specifications have sensitive information in their environment variables.","category":"","subCategory":"","remediation":"Use Kubernetes secrets or Key Management Systems to store credentials.","framework":["security"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"security","name":"Workload with credential access","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"Check if the workload has sensitive information in environment variables, by using list of known sensitive key names.","manualCheck":"","configuration":[{"path":"settings.postureControlInputs.sensitiveValues","name":"Sensitive Values","description":"Strings that identify a value that Kubescape believes should be stored in a Secret, and not in a ConfigMap or an environment variable."},{"path":"settings.postureControlInputs.sensitiveValuesAllowed","name":"Allowed Values","description":"Reduce false positives with known values."},{"path":"settings.postureControlInputs.sensitiveKeyNames","name":"Sensitive Keys","description":"Key names that identify a potential value that should be stored in a Secret, and not in a ConfigMap or an environment variable."},{"path":"settings.postureControlInputs.sensitiveKeyNamesAllowed","name":"Allowed Keys","description":"Reduce false positives with known key names."}]},{"controlDetailsID":"C-0260","name":"Missing network policy","description":"This control detects workloads that has no NetworkPolicy configured in labels. If a network policy is not configured, it means that your applications might not have necessary control over the traffic to and from the pods, possibly leading to a security vulnerability.","category":"Network","subCategory":"","remediation":"Review the workloads identified by this control and assess whether it's necessary to configure a network policy for them.","framework":["ClusterScan","security","SOC2","WorkloadScan"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"ClusterScan","name":"Missing network policy","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"security","name":"Missing network policy","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"SOC2","name":"Firewall (CC6.1,CC6.6,CC7.2)","description":"Network is monitored and protected by the following. System firewalls are configured to limit unnecessary ports, protocols and services. Firewall rules are reviewed at least annually by IT management.","long_description":"Network is monitored and protected by the following. System firewalls are configured to limit unnecessary ports, protocols and services. Firewall rules are reviewed at least annually by IT management.","remediation":"Define network policies for all workloads to protect unwanted access","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"WorkloadScan","name":"Missing network policy","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","ConfigMap","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob","NetworkPolicy"],"testCriteria":"Check that all workloads has a network policy configured in labels.","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0261","name":"ServiceAccount token mounted","description":"Potential attacker may gain access to a workload and steal its ServiceAccount token. Therefore, it is recommended to disable automatic mapping of the ServiceAccount tokens in ServiceAccount configuration. Enable it only for workloads that need to use them and ensure that this ServiceAccount is not bound to an unnecessary ClusterRoleBinding or RoleBinding.","category":"","subCategory":"","remediation":"Disable automatic mounting of service account tokens to pods at the workload level, by specifying automountServiceAccountToken: false. Enable it only for workloads that need to use them and ensure that this ServiceAccount doesn't have unnecessary permissions","framework":["security"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"security","name":"ServiceAccount token mounted","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","ServiceAccount","RoleBinding","ClusterRoleBinding","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"test if ServiceAccount token is mounted on workload and it has at least one binding.","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0262","name":"Anonymous user has RoleBinding","description":"Granting permissions to the system:unauthenticated or system:anonymous user is generally not recommended and can introduce security risks. Allowing unauthenticated access to your Kubernetes cluster can lead to unauthorized access, potential data breaches, and abuse of cluster resources.","category":"Control plane","subCategory":"Supply chain","remediation":"Review and modify your cluster's RBAC configuration to ensure that only authenticated and authorized users have appropriate permissions based on their roles and responsibilities within your system.","framework":["AllControls","ClusterScan","security"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Anonymous access enabled","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ClusterScan","name":"Anonymous access enabled","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"security","name":"Anonymous access enabled","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["RoleBinding","ClusterRoleBinding"],"testCriteria":"Checks if ClusterRoleBinding/RoleBinding resources give permissions to anonymous user. Also checks in the apiserver if the --anonymous-auth flag is set to false","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0263","name":"Ingress uses TLS","description":"This control detect Ingress resources that do not use TLS","category":"","subCategory":"","remediation":"The user needs to implement TLS for the Ingress resource in order to encrypt the incoming traffic","framework":["SOC2"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"SOC2","name":"Data in motion encryption - Ingress is TLS encrypted (CC6.1,CC6.6,CC6.7)","description":"Transport Layer Security (TLS) is used to protect the transmission of data sent over the internet to and from the organization's application server.","long_description":"Transport Layer Security (TLS) is used to protect the transmission of data sent over the internet to and from the organization's application server.","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Ingress"],"testCriteria":"Check if the Ingress resource has TLS configured","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0264","name":"PersistentVolume without encyption","description":"This control detects PersistentVolumes without encyption","category":"","subCategory":"","remediation":"Enable encryption on the PersistentVolume using the configuration in StorageClass","framework":["security","SOC2"],"prerequisites":{"cloudProviders":null},"severity":5,"frameworkOverrides":[{"frameworkName":"security","name":"PersistentVolume without encyption","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"SOC2","name":"Data in rest encryption - Persistent Volumes are encrypted (CC1.1,CC6.7)","description":"Transport Layer Security (TLS) is used to protect the transmission of data sent over the internet to and from the organization's application server.","long_description":"Transport Layer Security (TLS) is used to protect the transmission of data sent over the internet to and from the organization's application server.","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["PersistentVolume","StorageClass"],"testCriteria":"Checking all PersistentVolumes via their StorageClass for encryption","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0265","name":"system:authenticated user has elevated roles","description":"Granting permissions to the system:authenticated group is generally not recommended and can introduce security risks. This control ensures that system:authenticated users do not have cluster risking permissions.","category":"Control plane","subCategory":"Supply chain","remediation":"Review and modify your cluster's RBAC configuration to ensure that system:authenticated will have minimal permissions.","framework":["AllControls","ClusterScan","security"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Authenticated user has sensitive permissions","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ClusterScan","name":"Authenticated user has sensitive permissions","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"security","name":"Authenticated user has sensitive permissions","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["RoleBinding","ClusterRoleBinding","Role","ClusterRole"],"testCriteria":"Checks if ClusterRoleBinding/RoleBinding resources give permissions to system:authenticated group.","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0266","name":"Exposure to internet via Gateway API","description":"This control detect workloads that are exposed on Internet through a Gateway API (HTTPRoute,TCPRoute, UDPRoute). It fails in case it find workloads connected with these resources.","category":"","subCategory":"","remediation":"The user can evaluate its exposed resources and apply relevant changes wherever needed.","framework":["security"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"security","name":"Exposure to internet via Gateway API","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Service","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob","HTTPRoute","TCPRoute","UDPRoute"],"testCriteria":"Checks if workloads are exposed through the use of Gateway API (HTTPRoute,TCPRoute, UDPRoute).","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0267","name":"Workload with cluster takeover roles","description":"Cluster takeover roles include workload creation or update and secret access. They can easily lead to super privileges in the cluster. If an attacker can exploit this workload then the attacker can take over the cluster using the RBAC privileges this workload is assigned to.","category":"Workload","subCategory":"","remediation":"You should apply least privilege principle. Make sure each service account has only the permissions that are absolutely necessary.","framework":["security"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"security","name":"Workload with cluster takeover roles","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","ServiceAccount","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob","RoleBinding","ClusterRoleBinding","Role","ClusterRole"],"testCriteria":"Check if the service account used by a workload has cluster takeover roles.","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0268","name":"Ensure CPU requests are set","description":"This control identifies all Pods for which the CPU requests are not set.","category":"Workload","subCategory":"Resource management","remediation":"Set the CPU requests or use exception mechanism to avoid unnecessary notifications.","framework":["DevOpsBest"],"prerequisites":{"cloudProviders":null},"severity":3,"frameworkOverrides":[{"frameworkName":"DevOpsBest","name":"Ensure CPU requests are set","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0269","name":"Ensure memory requests are set","description":"This control identifies all Pods for which the memory requests are not set.","category":"Workload","subCategory":"Resource management","remediation":"Set the memory requests or use exception mechanism to avoid unnecessary notifications.","framework":["DevOpsBest"],"prerequisites":{"cloudProviders":null},"severity":3,"frameworkOverrides":[{"frameworkName":"DevOpsBest","name":"Ensure memory requests are set","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0270","name":"Ensure CPU limits are set","description":"This control identifies all Pods for which the CPU limits are not set.","category":"Workload","subCategory":"Resource management","remediation":"Set the CPU limits or use exception mechanism to avoid unnecessary notifications.","framework":["AllControls","ArmoBest","DevOpsBest","NSA","security","WorkloadScan"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Ensure CPU limits are set","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"Ensure CPU limits are set","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"DevOpsBest","name":"Ensure CPU limits are set","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"NSA","name":"Ensure CPU limits are set","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"security","name":"Ensure CPU limits are set","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"WorkloadScan","name":"Ensure CPU limits are set","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0271","name":"Ensure memory limits are set","description":"This control identifies all Pods for which the memory limits are not set.","category":"Workload","subCategory":"Resource management","remediation":"Set the memory limits or use exception mechanism to avoid unnecessary notifications.","framework":["AllControls","ArmoBest","DevOpsBest","NSA","security","WorkloadScan"],"prerequisites":{"cloudProviders":null},"severity":8,"frameworkOverrides":[{"frameworkName":"AllControls","name":"Ensure memory limits are set","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"ArmoBest","name":"Ensure memory limits are set","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"DevOpsBest","name":"Ensure memory limits are set","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"NSA","name":"Ensure memory limits are set","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"security","name":"Ensure memory limits are set","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""},{"frameworkName":"WorkloadScan","name":"Ensure memory limits are set","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob"],"testCriteria":"","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0272","name":"Workload with administrative roles","description":"This control identifies workloads where the associated service accounts have roles that grant administrative-level access across the cluster. Granting a workload such expansive permissions equates to providing it cluster admin roles. This level of access can pose a significant security risk, as it allows the workload to perform any action on any resource, potentially leading to unauthorized data access or cluster modifications.","category":"Workload","subCategory":"","remediation":"You should apply least privilege principle. Make sure cluster admin permissions are granted only when it is absolutely necessary. Don't use service accounts with such high permissions for daily operations.","framework":["security"],"prerequisites":{"cloudProviders":null},"severity":6,"frameworkOverrides":[{"frameworkName":"security","name":"Workload with administrative roles","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","ServiceAccount","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob","RoleBinding","ClusterRoleBinding","Role","ClusterRole"],"testCriteria":"Check if the service account used by a workload has cluster admin roles, either by being bound to the cluster-admin clusterrole, or by having equivalent high privileges.","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0273","name":"Outdated Kubernetes version","description":"Identifies Kubernetes clusters running on outdated versions. Using old versions can expose clusters to known vulnerabilities, compatibility issues, and miss out on improved features and security patches. Keeping Kubernetes up-to-date is crucial for maintaining security and operational efficiency.","category":"Control plane","subCategory":"","remediation":"Regularly update Kubernetes clusters to the latest stable version to mitigate known vulnerabilities and enhance functionality. Plan and execute upgrades considering workload compatibility, testing in a staging environment before applying changes to production. Follow Kubernetes' best practices for version management and upgrades to ensure a smooth transition and minimal downtime.","framework":["security"],"prerequisites":{"cloudProviders":null},"severity":2,"frameworkOverrides":[{"frameworkName":"security","name":"Outdated Kubernetes version","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Node"],"testCriteria":"Verifies the current Kubernetes version against the latest stable releases.","manualCheck":"","configuration":[]},{"controlDetailsID":"C-0274","name":"Verify Authenticated Service","description":"Verifies if the service is authenticated","category":"Network","subCategory":"","remediation":"Configure the service to require authentication.","framework":["security"],"prerequisites":{"cloudProviders":null},"severity":7,"frameworkOverrides":[{"frameworkName":"security","name":"Verify Authenticated Service","description":"","long_description":"","remediation":"","references":null,"manualCheck":"","impactStatement":"","defaultValue":""}],"relatedResources":["Pod","Service","Deployment","ReplicaSet","DaemonSet","StatefulSet","Job","CronJob","servicesscanresults"],"testCriteria":"","manualCheck":"","configuration":[]}]