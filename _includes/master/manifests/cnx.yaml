{% comment %}
cnx.yaml acccepts the following include flags:

| Name             | Accepted Values          |
|------------------|--------------------------|
| install_type     | openshift                |
| datastore        | kdd, etcd                |

{% endcomment -%}
{%- if include.datastore == "kdd" %}
# This manifest adds the additional CNX Manager components to a cluster
# that has already had the Calico part of CNX deployed.
# - It refers to the calico-config ConfigMap from that file, so if you are
#   not using the provided hosted Calico manifest you must update
#   references to that resource in this file.
# - Update the tigera-cnx-manager-config ConfigMap below before use.
# - Optionally update the cnx-apiserver-certs ConfigMap and caBundle in the
#   apiregistration section below to use your TLS certs for secure communication
#   between the CNX API server and Kubernetes API server.
# - This manifest makes the CNX Manager web server available via a NodePort
#   serving on port 30003.  You may wish to update how this is exposed; do
#   so by editing the tigera-cnx-manager-access Service below.
{%- elsif include.datastore == "etcd" %}
# This manifest adds the additional CNX Manager components to a cluster
# that has already had the Calico part of CNX deployed.
# - It refers to the calico-config ConfigMap and calico-etcd-secrets Secret
#   from that file, so if you are not using the provided hosted Calico
#   manifest you must update references to those resources in this file.
# - Update the tigera-cnx-manager-config ConfigMap below before use.
# - Optionally update the cnx-apiserver-certs ConfigMap and caBundle in the
#   apiregistration section below to use your TLS certs for secure communication
#   between the CNX API server and Kubernetes API server.
# - This manifest makes the CNX Manager web server available via a NodePort
#   serving on port 30003.  You may wish to update how this is exposed; do
#   so by editing the cnx-manager Service below.
{%- endif %}

# Update this ConfigMap with the Google login client id.
kind: ConfigMap
apiVersion: v1
metadata:
  name: tigera-cnx-manager-config
  namespace: kube-system
data:
  # Authentication type.  Must be set to "OIDC", "Basic", "Token", or "OAuth".
{%- if include.install_type == "openshift" %}
  tigera.cnx-manager.authentication-type: "OAuth"
{%- else %}
  tigera.cnx-manager.authentication-type: "Basic"
{%- endif %}
  # The OIDC authority.  Required if authentication-type is OIDC, ignored otherwise.
  tigera.cnx-manager.oidc-authority: "https://accounts.google.com"
  # The OIDC client id to use for OIDC login.  Kubelet must be configured accordingly.
  # Value is ignored if not using OIDC login.
  tigera.cnx-manager.oidc-client-id: "<oidc-client-id>"
  # The OAuth endpoint. Required if authentication-type is OAuth, ignored otherwise.
  tigera.cnx-manager.oauth-authority: "https://<oauth-authority>/oauth/authorize"
{%- if include.datastore == "etcd" %}
  # The OAuth client id to use for OAuth login. Value is ignored if not using OAuth login.
  tigera.cnx-manager.oauth-client-id: "cnx-manager"
{%- endif %}
  # Prometheus server resource path
  tigera.cnx-manager.prometheus-api-url: "/api/v1/namespaces/calico-monitoring/services/calico-node-prometheus:9090/proxy/api/v1"
  # Query api url
  tigera.cnx-manager.query-api-url: "/api/v1/namespaces/kube-system/services/https:cnx-api:8080/proxy"

---

# Optionally update this ConfigMap to enable TLS with certificate verification when CNX API
# server communicates with the Kubernetes API server.
apiVersion: v1
kind: Secret
type: Opaque
metadata:
  name: cnx-apiserver-certs
  namespace: kube-system
data:
  # Populate the following files with apiserver TLS configuration if desired,
  # but leave blank if not using TLS with certificate verification.
  # This self-hosted install expects two files with the following names. The values
  # should be base64 encoded strings of the entire contents of each file.
  #apiserver.key:
  #apiserver.crt:

---

# Optionally update this Service to change how CNX Manager is accessed.
# If using Google login, the URL for the web server must be configured
# as a redirect URI in the Google project.  If the web server will be
# accessed at https://<host>:<port>, add https://<host>:<port>/login/oidc/callback
# to the redirect URI list for the project.
apiVersion: v1
kind: Service
metadata:
  labels:
    k8s-app: cnx-manager
  name: cnx-manager
  namespace: kube-system
spec:
  selector:
    k8s-app: cnx-manager
  ports:
    - port: 8080
      targetPort: 9443
      nodePort: 30003
  type: NodePort

---

# Optionally update this ConfigMap to use your own caBundle.
apiVersion: apiregistration.k8s.io/v1beta1
kind: APIService
metadata:
  name: v3.projectcalico.org
spec:
  group: projectcalico.org
  versionPriority: 200
  groupPriorityMinimum: 200
  service:
    name: cnx-api
    namespace: kube-system
  version: v3
  # Optionally update the caBundle and disable insecureSkipTLSVerify in order
  # to enable TLS certificate verification.
  insecureSkipTLSVerify: true
  #caBundle:

---

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: calico:system:auth-delegator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: system:auth-delegator
subjects:
- kind: ServiceAccount
  name: cnx-apiserver
  namespace: kube-system

---

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: RoleBinding
metadata:
  name: calico-auth-reader
  namespace: kube-system
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: extension-apiserver-authentication-reader
subjects:
- kind: ServiceAccount
  name: cnx-apiserver
  namespace: kube-system

---

kind: ServiceAccount
apiVersion: v1
metadata:
  name: cnx-apiserver
  namespace: kube-system

---

kind: ServiceAccount
apiVersion: v1
metadata:
  name: cnx-manager
  namespace: kube-system

---

{%- if include.datastore == "kdd" %}
# Give cnx-apiserver ServiceAccount permissions needed for
# accessing various backing CRDs and K8s networkpolicies.
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: cnx-apiserver-role
rules:
  - apiGroups: ["extensions","networking.k8s.io",""]
    resources:
      - networkpolicies
      - nodes
      - namespaces
      - pods
    verbs:
      - get
      - list
      - watch
  - apiGroups: ["crd.projectcalico.org"]
    resources:
      - globalnetworkpolicies
      - networkpolicies
      - tiers
      - clusterinformations
      - hostendpoints
      - licensekeys
      - globalnetworksets
    verbs:
      - "*"

---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: cnx-apiserver-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cnx-apiserver-role
subjects:
- kind: ServiceAccount
  name: cnx-apiserver
  namespace: kube-system

---
{%- endif %}

apiVersion: v1
kind: Service
metadata:
  name: cnx-api
  namespace: kube-system
spec:
  ports:
  - name: apiserver
    port: 443
    protocol: TCP
    targetPort: 5443
  - name: queryserver
    port: 8080
    protocol: TCP
    targetPort: 8080
  selector:
    apiserver: "true"

---

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: cnx-apiserver
  namespace: kube-system
  labels:
    apiserver: "true"
    k8s-app: cnx-apiserver
spec:
  replicas: 1
  strategy:
    type: Recreate
  selector:
    matchLabels:
      apiserver: "true"
  template:
    metadata:
      name: cnx-apiserver
      namespace: kube-system
      labels:
        apiserver: "true"
        k8s-app: cnx-apiserver
    spec:
      serviceAccountName: cnx-apiserver
      tolerations:
        - key: node-role.kubernetes.io/master
          effect: NoSchedule
      imagePullSecrets:
        - name: cnx-pull-secret
      containers:
      - name: cnx-apiserver
        image: {{site.imageNames["cnxApiserver"]}}:{{site.data.versions[page.version].first.components["cnx-apiserver"].version}}
        args:
        - "--secure-port=5443"
        env:
{%- if include.datastore == "etcd" %}
  {%- if include.install_type == "openshift" %}
          - name: ETCD_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                name: calico-config
                key: etcd_endpoints
          - name: DATASTORE_TYPE
            value: "etcdv3"
          # Location of the CA certificate for etcd.
           - name: ETCD_CA_CERT_FILE
             valueFrom:
               configMapKeyRef:
                 name: calico-config
                 key: etcd_ca
          # Location of the client key for etcd.
           - name: ETCD_KEY_FILE
             valueFrom:
               configMapKeyRef:
                 name: calico-config
                 key: etcd_key
          # Location of the client certificate for etcd.
           - name: ETCD_CERT_FILE
             valueFrom:
               configMapKeyRef:
                 name: calico-config
                 key: etcd_cert
        volumeMounts:
           - mountPath: /calico-secrets
             name: etcd-certs
           - mountPath: /code/apiserver.local.config/certificates
             name: apiserver-certs
  {%- else %}
          - name: ETCD_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                name: calico-config
                key: etcd_endpoints
          - name: DATASTORE_TYPE
            value: "etcdv3"
          # If you're using TLS enabled etcd uncomment the following.
          # Location of the CA certificate for etcd.
          # - name: ETCD_CA_CERT_FILE
          #   valueFrom:
          #     configMapKeyRef:
          #       name: calico-config
          #       key: etcd_ca
          # Location of the client key for etcd.
          # - name: ETCD_KEY_FILE
          #   valueFrom:
          #     configMapKeyRef:
          #       name: calico-config
          #       key: etcd_key
          # Location of the client certificate for etcd.
          # - name: ETCD_CERT_FILE
          #   valueFrom:
          #     configMapKeyRef:
          #       name: calico-config
          #       key: etcd_cert
        volumeMounts:
          # - mountPath: /calico-secrets
          #   name: etcd-certs
          # - mountPath: /code/apiserver.local.config/certificates
          #   name: apiserver-certs
  {%- endif %}
{%- elsif include.datastore == "kdd" %}
          - name: DATASTORE_TYPE
            value: "kubernetes"
        volumeMounts:
          # - mountPath: /code/apiserver.local.config/certificates
          #   name: apiserver-certs
{%- endif %}
        livenessProbe:
          httpGet:
            path: /apis/
            port: 5443
            scheme: HTTPS
          initialDelaySeconds: 5
          periodSeconds: 3
      - name: cnx-queryserver
        image: {{site.imageNames["cnxQueryserver"]}}:{{site.data.versions[page.version].first.components["cnx-queryserver"].version}}
        env:
          # Set queryserver logging to "info"
{%- if include.datastore == "etcd" %}
  {%- if include.install_type == "openshift" %}
          - name: LOGLEVEL
            value: "info"
          - name: ETCD_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                name: calico-config
                key: etcd_endpoints
          - name: DATASTORE_TYPE
            value: "etcdv3"
          # If you're using TLS enabled etcd uncomment the following.
          # Location of the CA certificate for etcd.
           - name: ETCD_CA_CERT_FILE
             valueFrom:
               configMapKeyRef:
                 name: calico-config
                 key: etcd_ca
          # Location of the client key for etcd.
           - name: ETCD_KEY_FILE
             valueFrom:
               configMapKeyRef:
                 name: calico-config
                 key: etcd_key
          # Location of the client certificate for etcd.
           - name: ETCD_CERT_FILE
             valueFrom:
               configMapKeyRef:
                 name: calico-config
                 key: etcd_cert
        volumeMounts:
           - mountPath: /calico-secrets
             name: etcd-certs
  {%- else %}
          - name: LOGLEVEL
            value: "info"
          - name: ETCD_ENDPOINTS
            valueFrom:
              configMapKeyRef:
                name: calico-config
                key: etcd_endpoints
          - name: DATASTORE_TYPE
            value: "etcdv3"
          # If you're using TLS enabled etcd uncomment the following.
          # Location of the CA certificate for etcd.
          # - name: ETCD_CA_CERT_FILE
          #   valueFrom:
          #     configMapKeyRef:
          #       name: calico-config
          #       key: etcd_ca
          # Location of the client key for etcd.
          # - name: ETCD_KEY_FILE
          #   valueFrom:
          #     configMapKeyRef:
          #       name: calico-config
          #       key: etcd_key
          # Location of the client certificate for etcd.
          # - name: ETCD_CERT_FILE
          #   valueFrom:
          #     configMapKeyRef:
          #       name: calico-config
          #       key: etcd_cert
        volumeMounts:
          # - mountPath: /calico-secrets
          #   name: etcd-certs
  {%- endif %}
{%- elsif include.datastore == "kdd" %}
          - name: LOGLEVEL
            value: "info"
          - name: DATASTORE_TYPE
            value: "kubernetes"
{%- endif %}
        livenessProbe:
          httpGet:
            path: /version
            port: 8080
            scheme: HTTPS
          initialDelaySeconds: 5
          periodSeconds: 3
      volumes:
{%- if include.datastore == "etcd" %}
  {%- if include.install_type == "openshift" %}
        # If you're using TLS enabled etcd uncomment the following.
         - name: etcd-certs
           secret:
             secretName: calico-etcd-secrets
        # If you're using TLS with certificate verification then
        # uncomment the following.
        # - name: apiserver-certs
        #   secret:
        #     secretName: cnx-apiserver-certs
  {%- else %}
        # If you're using TLS enabled etcd uncomment the following.
        # - name: etcd-certs
        #   secret:
        #     secretName: calico-etcd-secrets
        # If you're using TLS with certificate verification then
        # uncomment the following.
        # - name: apiserver-certs
        #   secret:
        #     secretName: cnx-apiserver-certs
  {%- endif %}
{%- elsif include.datastore == "kdd" %}
        # If you're using TLS with certificate verification then
        # uncomment the following.
        # - name: apiserver-certs
        #   secret:
        #     secretName: cnx-apiserver-certs
{%- endif %}

---

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: cnx-manager
  namespace: kube-system
  labels:
    k8s-app: cnx-manager
spec:
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      name: cnx-manager
      namespace: kube-system
      labels:
        k8s-app: cnx-manager
      annotations:
        # Mark this pod as a critical add-on; when enabled, the critical add-on scheduler
        # reserves resources for critical add-on pods so that they can be rescheduled after
        # a failure.  This annotation works in tandem with the toleration below.
        scheduler.alpha.kubernetes.io/critical-pod: ''
    spec:
      serviceAccountName: cnx-manager
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      # Allow this pod to be rescheduled while the node is in "critical add-ons only" mode.
      # This, along with the annotation above marks this pod as a critical add-on.
      - key: CriticalAddonsOnly
        operator: Exists
      imagePullSecrets:
        - name: cnx-pull-secret
      # cnx-manager-init-pem writes a "pem" file into a volume shared by
      # cnx-manager-proxy.
      initContainers:
      - name: cnx-manager-init-pem # create /cache/pem for cnx-manager-proxy
        image: alpine:3.7
        command: ['sh', '-c', 'cat /etc/cnx-manager-web-tls/cert /etc/cnx-manager-web-tls/key > /cache/pem']
        volumeMounts:
        - mountPath: /cache
          name: cnx-cache-volume
        - mountPath: /etc/cnx-manager-web-tls
          name: cnx-manager-tls
      containers:
      - name: cnx-manager
        image: {{site.imageNames["cnxManager"]}}:{{site.data.versions[page.version].first.components["cnx-manager"].version}}
        env:
          - name: CNX_WEB_AUTHENTICATION_TYPE
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.authentication-type
          - name: CNX_WEB_OIDC_AUTHORITY
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.oidc-authority
{%- if include.datastore == "etcd" %}
          - name: CNX_WEB_OIDC_CLIENT_ID
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.oidc-client-id
          - name: CNX_WEB_OAUTH_AUTHORITY
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.oauth-authority
          - name: CNX_WEB_OAUTH_CLIENT_ID
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.oauth-client-id
          - name: CNX_PROMETHEUS_API_URL
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.prometheus-api-url
          - name: CNX_QUERY_API_URL
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.query-api-url
        volumeMounts:
        - mountPath: /etc/cnx-manager-web-tls
          name: cnx-manager-tls
{%- elsif include.datastore == "kdd" %}
          - name: CNX_WEB_OIDC_CLIENT_ID
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.oidc-client-id
          - name: CNX_PROMETHEUS_API_URL
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.prometheus-api-url
          - name: CNX_QUERY_API_URL
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.query-api-url
{%- endif %}
        livenessProbe:
          httpGet:
            path: /
            port: 8443
            scheme: HTTPS
          initialDelaySeconds: 5
          periodSeconds: 3
      - name: cnx-manager-proxy
        image: {{site.imageNames["cnxManagerProxy"]}}:{{site.data.versions[page.version].first.components["cnx-manager-proxy"].version}}
        env:
          - name: CNX_WEB_AUTHENTICATION_TYPE
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.authentication-type
          - name: CNX_WEB_OIDC_AUTHORITY
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.oidc-authority
          - name: CNX_WEB_OIDC_CLIENT_ID
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.oidc-client-id
{%- if include.datastore == "etcd" %}
          - name: CNX_WEB_OAUTH_AUTHORITY
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.oauth-authority
          - name: CNX_WEB_OAUTH_CLIENT_ID
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.oauth-client-id
        volumeMounts:
        - mountPath: /etc/cnx-manager-web-tls
          name: cnx-manager-tls
        - mountPath: /cache
          name: cnx-cache-volume
{%- endif %}
        livenessProbe:
          httpGet:
            path: /
            port: 9443
            scheme: HTTPS
          initialDelaySeconds: 5
          periodSeconds: 3
      volumes:
      - name: cnx-cache-volume
        emptyDir: {}
      - name: cnx-manager-tls
        secret:
          secretName: cnx-manager-tls

{%- if include.install_type == "openshift" %}
---
# This manifest creates a namespace for deploying prometheus-operator,
# Prometheus and alertmanager.
apiVersion: v1
kind: Namespace
metadata:
  name: calico-monitoring
---
kind: NetworkPolicy
apiVersion: extensions/v1beta1
metadata:
  name: default-deny
  namespace: calico-monitoring
spec:
  podSelector:
    matchLabels: {}
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: calico-prometheus-operator
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: calico-prometheus-operator
subjects:
- kind: ServiceAccount
  name: calico-prometheus-operator
  namespace: calico-monitoring
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: calico-prometheus-operator
  namespace: calico-monitoring
rules:
- apiGroups:
  - apiextensions.k8s.io
  resources:
  - customresourcedefinitions
  verbs:
  - "*"
- apiGroups:
  - monitoring.coreos.com
  resources:
  - alertmanagers
  - prometheuses
  - servicemonitors
  verbs:
  - "*"
- apiGroups:
  - apps
  resources:
  - statefulsets
  verbs: ["*"]
- apiGroups: [""]
  resources:
  - configmaps
  - secrets
  verbs: ["*"]
- apiGroups: [""]
  resources:
  - pods
  verbs: ["list", "delete"]
- apiGroups: [""]
  resources:
  - services
  - endpoints
  verbs: ["get", "create", "update"]
- apiGroups: [""]
  resources:
  - nodes
  verbs: ["list", "watch"]
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: calico-prometheus-operator
  namespace: calico-monitoring
---

# This manifest deploys the Calico Prometheus Operator.
apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: calico-prometheus-operator
  namespace: calico-monitoring
  labels:
    operator: prometheus
spec:
  replicas: 1
  template:
    metadata:
      labels:
        operator: prometheus
    spec:
      # If necessary, uncomment the 2 lines below and edit the nodeSelector
      # key:value pair with appropriate values for your environment.
      #nodeSelector:
        #node.role: infrastructure
      serviceAccountName: calico-prometheus-operator
      containers:
      - name: calico-prometheus-operator
        image: quay.io/coreos/prometheus-operator:{{site.data.versions[page.version].first.components["prometheus-operator"].version}}
        args:
          - --prometheus-config-reloader=quay.io/coreos/prometheus-config-reloader:{{site.data.versions[page.version].first.components["prometheus-config-reloader"].version}}
          - --config-reloader-image=quay.io/coreos/configmap-reload:{{site.data.versions[page.version].first.components["configmap-reload"].version}}
        resources:
          requests:
            cpu: 100m
            memory: 50Mi
          limits:
            cpu: 200m
            memory: 100Mi

---

# This manifest installs the Service which gets traffic to the calico-node metrics
# reporting endpoint.
apiVersion: v1
kind: Service
metadata:
  namespace: kube-system
  name: calico-node-metrics
  labels:
    k8s-app: calico-node
spec:
  selector:
    k8s-app: calico-node
  type: ClusterIP
  clusterIP: None
  ports:
  - name: calico-metrics-port
    port: 9081
    targetPort: 9081
    protocol: TCP
---

# This manifest creates a network policy to allow traffic to Alertmanager
# (TCP port 9093).
apiVersion: extensions/v1beta1
kind: NetworkPolicy
metadata:
  name: calico-node-alertmanager
  namespace: calico-monitoring
spec:
  ingress:
  - ports:
    - port: 9093
      protocol: TCP
  podSelector:
    matchLabels:
      alertmanager: calico-node-alertmanager
      app: alertmanager
---

# This manifest creates a network policy to allow traffic between
# Alertmanagers for HA configuration (TCP port 6783).
apiVersion: extensions/v1beta1
kind: NetworkPolicy
metadata:
  name: calico-node-alertmanager-mesh
  namespace: calico-monitoring
spec:
  ingress:
  - from:
    - podSelector:
        matchExpressions:
        - key: app
          operator: In
          values:
          - alertmanager
        - key: alertmanager
          operator: In
          values:
          - calico-node-alertmanager
    ports:
    - port: 6783
      protocol: TCP
  podSelector:
    matchLabels:
      alertmanager: calico-node-alertmanager
      app: alertmanager
---

# This manifest creates a network policy to allow traffic to access the
# Prometheus (TCP port 9090).
apiVersion: extensions/v1beta1
kind: NetworkPolicy
metadata:
  name: prometheus
  namespace: calico-monitoring
spec:
  ingress:
  - ports:
    - port: 9090
      protocol: TCP
  podSelector:
    matchLabels:
      app: prometheus
      prometheus: calico-node-prometheus
---

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRole
metadata:
  name: prometheus
rules:
- apiGroups: [""]
  resources:
  - nodes
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["get"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: calico-monitoring
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: calico-monitoring
---

# This manifest creates a ServiceMonitor to select calico-node metrics endpoints.
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: calico-node-monitor
  namespace: calico-monitoring
  labels:
    team: network-operators
spec:
  selector:
    matchLabels:
      k8s-app: calico-node
  namespaceSelector:
    matchNames:
    - kube-system
  endpoints:
  - port: calico-metrics-port
    interval: 5s
    scrapeTimeout: 5s
    honorLabels: true
---

{% raw %}
# This ConfigMap is used to create the denied packets alerting rules in Prometheus.
apiVersion: v1
kind: ConfigMap
metadata:
  name: calico-prometheus-dp-rate
  namespace: calico-monitoring
  labels:
    role: calico-prometheus-rules
    prometheus: calico-node-prometheus
data:
  calico.rules.yaml: |+
    groups:
    - name: ./calico.rules
      rules:
      - alert: DeniedPacketsRate
        expr: rate(calico_denied_packets[10s]) > 50
        labels:
          severity: critical
        annotations:
          summary: "Instance {{$labels.instance}} - Large rate of packets denied"
          description: "{{$labels.instance}} with calico-node pod {{$labels.pod}} has been denying packets at a fast rate {{$labels.sourceIp}} by policy {{$labels.policy}}."
{% endraw %}

---

# This manifest creates a secret that will be mounted as the Alertmanager
# configuration file.
# Write your alertmanager configuration file based on
# https://prometheus.io/docs/alerting/configuration/
# and save it to a file, say alertmanager.yaml and then run:
#
#       $ cat alertmanager.yaml | base64 -w 0
#
# and paste the output below.
#
# The encoded secret below decodes to this configuration.
#
# global:
#   resolve_timeout: 5m
# route:
#   group_by: ['job']
#   group_wait: 30s
#   group_interval: 1m
#   repeat_interval: 5m
#   receiver: 'webhook'
#     group_by: ['alertname']
# receivers:
# - name: 'webhook'
#   webhook_configs:
#   - url: 'http://calico-alertmanager-webhook:30501/'

apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-calico-node-alertmanager
  namespace: calico-monitoring
data:
  alertmanager.yaml: Z2xvYmFsOgogIHJlc29sdmVfdGltZW91dDogNW0Kcm91dGU6CiAgZ3JvdXBfYnk6IFsnam9iJ10KICBncm91cF93YWl0OiAzMHMKICBncm91cF9pbnRlcnZhbDogMW0KICByZXBlYXRfaW50ZXJ2YWw6IDVtCiAgcmVjZWl2ZXI6ICd3ZWJob29rJwpyZWNlaXZlcnM6Ci0gbmFtZTogJ3dlYmhvb2snCiAgd2ViaG9va19jb25maWdzOgogIC0gdXJsOiAnaHR0cDovL2NhbGljby1hbGVydG1hbmFnZXItd2ViaG9vazozMDUwMS8nCg==
---

# This manifest creates the {{site.prodname}} Alertmanager.
apiVersion: monitoring.coreos.com/v1
kind: Alertmanager
metadata:
  name: calico-node-alertmanager
  namespace: calico-monitoring
spec:
  version: {{site.data.versions[page.version].first.components["alertmanager"].version}}
  baseImage: quay.io/prometheus/alertmanager
  # If necessary, uncomment the 2 lines below and edit the nodeSelector
  # key:value pair with appropriate values for your environment.
  #nodeSelector:
    #node.role: infrastructure
  replicas: 3
---

# This manifest installs the Service which gets traffic to the Calico
# AlertManager.
apiVersion: v1
kind: Service
metadata:
  name: calico-node-alertmanager
  namespace: calico-monitoring
spec:
  type: NodePort
  ports:
  - name: web
    nodePort: 30903
    port: 9093
    protocol: TCP
    targetPort: web
  selector:
    alertmanager: calico-node-alertmanager
---

# This manifest runs a Prometheus instance that will monitor calico-node
# denied packet metrics.
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: calico-node-prometheus
  namespace: calico-monitoring
spec:
  baseImage: quay.io/prometheus/prometheus
  # If necessary, uncomment the 2 lines below and edit the nodeSelector
  # key:value pair with appropriate values for your environment.
  #nodeSelector:
    #node.role: infrastructure
  serviceAccountName: prometheus
  serviceMonitorSelector:
    matchLabels:
      team: network-operators
  version: {{site.data.versions[page.version].first.components["prometheus"].version}}
  retention: 24h
  resources:
    requests:
      memory: 400Mi
  ruleSelector:
    matchLabels:
      role: calico-prometheus-rules
      prometheus: calico-node-prometheus
  alerting:
    alertmanagers:
      - namespace: calico-monitoring
        name: calico-node-alertmanager
        port: web
        scheme: http
---

# This manifest installs the Service which gets traffic to the Calico
# Prometheus.
apiVersion: v1
kind: Service
metadata:
  name: calico-node-prometheus
  namespace: calico-monitoring
spec:
  type: NodePort
  ports:
  - name: web
    nodePort: 30909
    port: 9090
    protocol: TCP
    targetPort: web
  selector:
    prometheus: calico-node-prometheus
{%- endif %}
