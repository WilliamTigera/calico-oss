# This manifest adds the additional Tigera Secure EE Manager components to a cluster
# that has already had the Calico part of Tigera Secure EE deployed.
# - Update the tigera-cnx-manager-config ConfigMap below before use.
# - This manifest makes the Tigera Secure EE Manager web server available via a NodePort
#   serving on port 30003.  You may wish to update how this is exposed; do
#   so by editing the tigera-cnx-manager-access Service below.

# Update this ConfigMap with the Google login client id.
kind: ConfigMap
apiVersion: v1
metadata:
  name: tigera-cnx-manager-config
  namespace: calico-monitoring
data:
  # Authentication type.  Must be set to "OIDC", "Basic", "Token", or "OAuth".
  {{- $authType := .Values.manager.auth.type | lower }}

  {{- if eq $authType "token" }}
  tigera.cnx-manager.authentication-type: "Token"

  {{- else if eq $authType "basic" }}
  tigera.cnx-manager.authentication-type: "Basic"

  {{- else if eq $authType "oidc" }}
  tigera.cnx-manager.authentication-type: "OIDC"

  {{- else if eq $authType "oauth" }}
  tigera.cnx-manager.authentication-type: "OAuth"

  {{- else }}
    {{ $errMsg := printf "invalid selection '%s' for manager.auth.type. Must be 'OIDC', 'Basic', 'Token', or 'OAuth'." .Values.manager.auth.type }}
    {{ fail $errMsg }}
  {{- end }}

  # The OIDC authority.  Required if authentication-type is OIDC, ignored otherwise.
{{- if .Values.manager.auth }}
  tigera.cnx-manager.oidc-authority: {{ .Values.manager.auth.authority | default "https://accounts.google.com" | quote }}
{{- else }}
  tigera.cnx-manager.oidc-authority: "https://accounts.google.com"
{{- end }}
  # The OIDC client id to use for OIDC login.  Kubelet must be configured accordingly.
  # Value is ignored if not using OIDC login.
{{- if .Values.manager.auth }}
  tigera.cnx-manager.oidc-client-id: {{ .Values.manager.auth.clientID | default "<oidc-client-id>" | quote }}
{{- else }}
  tigera.cnx-manager.oidc-client-id: "<oidc-client-id>"
{{- end }}
  # The OAuth endpoint. Required if authentication-type is OAuth, ignored otherwise.
{{- if .Values.manager.auth }}
  tigera.cnx-manager.oauth-authority: {{ .Values.manager.auth.authority | default "https://<oauth-authority>/oauth/authorize" | quote }}
{{- else }}
  tigera.cnx-manager.oauth-authority: "https://<oauth-authority>/oauth/authorize"
{{- end }}
  # The OAuth client id to use for OAuth login. Value is ignored if not using OAuth login.
{{- if .Values.manager.auth }}
  tigera.cnx-manager.oauth-client-id: {{ .Values.manager.auth.clientID | default "cnx-manager" | quote }}
{{- else }}
  tigera.cnx-manager.oauth-client-id: "cnx-manager"
{{- end }}
  # Prometheus server resource path
  tigera.cnx-manager.prometheus-api-url: "/api/v1/namespaces/calico-monitoring/services/calico-node-prometheus:9090/proxy/api/v1"
  # Compliance resource path
  tigera.cnx-manager.compliance-reports-api-url: "/compliance/reports"
  # Query api url
  tigera.cnx-manager.query-api-url: "/api/v1/namespaces/kube-system/services/https:cnx-api:8080/proxy"
  # Elasticsearch service resource path
  tigera.cnx-manager.elasticsearch-api-url: "/tigera-elasticsearch"
  # Path to Kibana.  The default is for a port forwarded to the NodePort included with the operator based install.
  # Replace this with the URL of your Kibana if you installed it yourself or are accessing it differently.
  tigera.cnx-manager.kibana-url: {{ include "tigera-secure-ee.kibanaURL" . }}
  # Whether sentry.io automatic error reporting of client side UI bugs is enabled.
  tigera.cnx-manager.error-tracking: "false"
  # Enable ALP support in the UI
  tigera.cnx-manager.alp-support: "false"
  # The name of the cluster.  This field is used as part of the index name of Elasticsearch logs, and is intended
  # to allow multiple clusters to share one Elasticsearch cluster.  The value of this field must match that of 
  # ELASTIC_INDEX_SUFFIX in tigera-fluentd-node.
  tigera.cnx-manager.cluster-name: "cluster"

---

{{- if eq .Values.manager.service.type "NodePort" }}
# Optionally update this Service to change how Tigera Secure EE Manager is accessed.
# If using Google login, the URL for the web server must be configured
# as a redirect URI in the Google project.  If the web server will be
# accessed at https://<host>:<port>, add https://<host>:<port>/login/oidc/callback
# to the redirect URI list for the project.
{{- end }}
apiVersion: v1
kind: Service
metadata:
  labels:
    k8s-app: cnx-manager
  name: cnx-manager
  namespace: calico-monitoring
spec:
  selector:
    k8s-app: cnx-manager
  ports:
    - port: 9443
      targetPort: 9443
{{- if .Values.manager.service.nodePort }}
      nodePort: {{ .Values.manager.service.nodePort }}
{{- end }}
  type: {{ .Values.manager.service.type }}
{{- if .Values.manager.service.clusterIP }}
  clusterIP: {{ .Values.manager.service.clusterIP }}
{{- end }}
{{- if .Values.manager.service.loadBalancerIP }}
  loadBalancerIP: {{ .Values.manager.service.loadBalancerIP }}
{{- end }}

---

kind: ServiceAccount
apiVersion: v1
metadata:
  name: cnx-manager
  namespace: calico-monitoring

---

# Give cnx-manager ServiceAccount permissions needed for
# running authorization checks.
# This is only required for the tigera-es-proxy container.
kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: cnx-manager-role
rules:
  - apiGroups: ["authentication.k8s.io"]
    resources:
      - tokenreviews
    verbs:
      - create
  - apiGroups: ["authorization.k8s.io"]
    resources:
      - subjectaccessreviews
    verbs:
      - create

---

apiVersion: rbac.authorization.k8s.io/v1beta1
kind: ClusterRoleBinding
metadata:
  name: cnx-manager-binding
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cnx-manager-role
subjects:
- kind: ServiceAccount
  name: cnx-manager
  namespace: calico-monitoring

---

apiVersion: extensions/v1beta1
kind: Deployment
metadata:
  name: cnx-manager
  namespace: calico-monitoring
  labels:
    k8s-app: cnx-manager
spec:
  replicas: 1
  strategy:
    type: Recreate
  template:
    metadata:
      name: cnx-manager
      namespace: calico-monitoring
      labels:
        k8s-app: cnx-manager
      annotations:
        # Mark this pod as a critical add-on; when enabled, the critical add-on scheduler
        # reserves resources for critical add-on pods so that they can be rescheduled after
        # a failure.  This annotation works in tandem with the toleration below.
        scheduler.alpha.kubernetes.io/critical-pod: ''
    spec:
      nodeSelector:
        beta.kubernetes.io/os: linux
      serviceAccountName: cnx-manager
      tolerations:
      - key: node-role.kubernetes.io/master
        effect: NoSchedule
      # Allow this pod to be rescheduled while the node is in "critical add-ons only" mode.
      # This, along with the annotation above marks this pod as a critical add-on.
      - key: CriticalAddonsOnly
        operator: Exists
{{- if .Values.imagePullSecrets }}
      imagePullSecrets:
{{- range $key, $value := .Values.imagePullSecrets }}
        - name: {{ $key }}
{{- end }}
{{- end }}
      containers:
      - name: cnx-manager
        image: {{.Values.manager.image}}:{{.Values.manager.tag}}
{{ tuple .Values.manager.resources | include "tigera-secure-ee.resourceLimits" | indent 8 -}}
        env:
          - name: CNX_WEB_AUTHENTICATION_TYPE
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.authentication-type
          - name: CNX_WEB_OIDC_AUTHORITY
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.oidc-authority
          - name: CNX_WEB_OIDC_CLIENT_ID
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.oidc-client-id
          - name: CNX_PROMETHEUS_API_URL
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.prometheus-api-url
          - name: CNX_COMPLIANCE_REPORTS_API_URL
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.compliance-reports-api-url
          - name: CNX_QUERY_API_URL
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.query-api-url
          - name: CNX_ELASTICSEARCH_API_URL
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.elasticsearch-api-url
          - name: CNX_ELASTICSEARCH_KIBANA_URL
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.kibana-url
          - name: CNX_ENABLE_ERROR_TRACKING
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.error-tracking
          - name: CNX_ALP_SUPPORT
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.alp-support
          - name: CNX_CLUSTER_NAME
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.cluster-name
          - name: CNX_WEB_OAUTH_AUTHORITY
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.oauth-authority
          - name: CNX_WEB_OAUTH_CLIENT_ID
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.oauth-client-id
{{- if .Values.manager.env }}
{{ toYaml .Values.manager.env | indent 10 }}
{{- end }}
        # Use the same liveness check as the proxy since
        # the static content is only served over the localhost
        # interface. Liveness check hits the static content through the proxy.
        livenessProbe:
          httpGet:
            path: /
            port: 9443
            scheme: HTTPS
          initialDelaySeconds: 90
          periodSeconds: 10
        securityContext:
          runAsNonRoot: true
          allowPrivilegeEscalation: false
      - name: cnx-manager-proxy
        image: {{.Values.managerProxy.image}}:{{.Values.managerProxy.tag}}
{{ tuple .Values.managerProxy.resources | include "tigera-secure-ee.resourceLimits" | indent 8 -}}
        env:
          - name: CNX_WEB_AUTHENTICATION_TYPE
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.authentication-type
          - name: CNX_WEB_OIDC_AUTHORITY
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.oidc-authority
          - name: CNX_WEB_OIDC_CLIENT_ID
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.oidc-client-id
          - name: CNX_WEB_OAUTH_AUTHORITY
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.oauth-authority
          - name: CNX_WEB_OAUTH_CLIENT_ID
            valueFrom:
              configMapKeyRef:
                name: tigera-cnx-manager-config
                key: tigera.cnx-manager.oauth-client-id
{{- if .Values.managerProxy.env }}
{{ toYaml .Values.managerProxy.env | indent 10 }}
{{- end }}
        volumeMounts:
        - mountPath: /etc/cnx-manager-web-tls
          name: cnx-manager-tls
        livenessProbe:
          httpGet:
            path: /
            port: 9443
            scheme: HTTPS
          initialDelaySeconds: 90
          periodSeconds: 10
        securityContext:
          runAsNonRoot: true
          allowPrivilegeEscalation: false
      - name: tigera-es-proxy
        image: {{.Values.esProxy.image}}:{{.Values.esProxy.tag}}
{{ tuple .Values.esProxy.resources | include "tigera-secure-ee.resourceLimits" | indent 8 -}}
        env:
          - name: LOG_LEVEL
            value: "info"
          - name: ELASTIC_ACCESS_MODE
            valueFrom:
              configMapKeyRef:
                name: tigera-es-config
                key: tigera.elasticsearch.access-mode
          - name: ELASTIC_SCHEME
            valueFrom:
              configMapKeyRef:
                name: tigera-es-config
                key: tigera.elasticsearch.scheme
          - name: ELASTIC_HOST
            valueFrom:
              configMapKeyRef:
                name: tigera-es-config
                key: tigera.elasticsearch.host
          - name: ELASTIC_PORT
            valueFrom:
              configMapKeyRef:
                name: tigera-es-config
                key: tigera.elasticsearch.port
          - name: ELASTIC_INSECURE_SKIP_VERIFY
            value: "false"
          - name: ELASTIC_USERNAME
            valueFrom:
              secretKeyRef:
                name: tigera-es-config
                key: tigera.elasticsearch.username
                optional: true
          - name: ELASTIC_PASSWORD
            valueFrom:
              secretKeyRef:
                name: tigera-es-config
                key: tigera.elasticsearch.password
                optional: true
          - name: ELASTIC_CA
            valueFrom:
              configMapKeyRef:
                name: tigera-es-config
                key: tigera.elasticsearch.ca.path
                optional: true
{{- if .Values.esProxy.env }}
{{ toYaml .Values.esProxy.env | indent 10 }}
{{- end }}
        # Use the same liveness check as the cnx-manager-proxy since
        # the es-proxy only listens for connections over localhost
        # interface. Liveness check reaches tigera-es-proxy via the
        # cnx-manager-proxy.
        livenessProbe:
          httpGet:
            path: /tigera-elasticsearch/version
            port: 9443
            scheme: HTTPS
          initialDelaySeconds: 90
          periodSeconds: 10
        volumeMounts:
        - mountPath: /etc/ssl/elastic/
          name: tigera-es-proxy-tls
      volumes:
      - name: cnx-manager-tls
        secret:
          secretName: cnx-manager-tls
      - name: tigera-es-proxy-tls
        secret:
          optional: true
          items:
          - key: tigera.elasticsearch.ca
            path: ca.pem
          secretName: tigera-es-config
---

kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: tigera-ui-user
rules:
# "list" requests that the Tigera EE Manager needs
- apiGroups: ["projectcalico.org","networking.k8s.io","extensions",""]
  # Use both networkpolicies and tier.networkpolicies, and globalnetworkpolicies and tier.globalnetworkpolicies resource
  # types to ensure identical behavior irrespective of the Calico RBAC scheme (see the ClusterRole
  # "ee-calico-tiered-policy-passthru" for more details).
  resources: ["tiers","networkpolicies","tier.networkpolicies","globalnetworkpolicies","tier.globalnetworkpolicies","namespaces","globalnetworksets"]
  verbs: ["watch","list"]
# Access to statistics
- apiGroups: [""]
  resources: ["services/proxy"]
  resourceNames: ["https:cnx-api:8080", "calico-node-prometheus:9090"]
  verbs: ["get","create"]
- apiGroups: ["lma.tigera.io"]
  resources: ["index"]
  resourceNames: ["flows", "audit*", "events"]
  verbs: ["get"]
# Access to policies in the default tier
- apiGroups: ["projectcalico.org"]
  resources: ["tiers"]
  resourceNames: ["default"]
  verbs: ["get"]
# List and download the reports in the UI.
- apiGroups: ["projectcalico.org"]
  resources: ["globalreports"]
  verbs: ["get", "list"]
- apiGroups: ["projectcalico.org"]
  resources: ["globalreporttypes"]
  verbs: ["get"]

---

kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1beta1
metadata:
  name: network-admin
rules:
# Full access to all network policies
- apiGroups: ["projectcalico.org","networking.k8s.io","extensions"]
  # Use both networkpolicies and tier.networkpolicies, and globalnetworkpolicies and tier.globalnetworkpolicies resource
  # types to ensure identical behavior irrespective of the Calico RBAC scheme (see the ClusterRole
  # "ee-calico-tiered-policy-passthru" for more details).
  resources: ["tiers","networkpolicies","tier.networkpolicies","globalnetworkpolicies","tier.globalnetworkpolicies","globalnetworksets"]
  verbs: ["create","update","delete","patch","get","watch","list"]
# Additional "list" requests that the Tigera EE Manager needs
- apiGroups: [""]
  resources: ["namespaces"]
  verbs: ["watch","list"]
# Access to statistics
- apiGroups: [""]
  resources: ["services/proxy"]
  resourceNames: ["https:cnx-api:8080", "calico-node-prometheus:9090"]
  verbs: ["get","create"]
# Access to flow logs, audit logs, and statistics
- apiGroups: ["lma.tigera.io"]
  resources: ["index"]
  resourceNames: ["flows", "audit*", "events"]
  verbs: ["get"]
# Manage globalreport configuration, view report generation status, and list reports in the UI.
- apiGroups: ["projectcalico.org"]
  resources: ["globalreports"]
  verbs: ["*"]
- apiGroups: ["projectcalico.org"]
  resources: ["globalreports/status"]
  verbs: ["get", "list", "watch"]
# Download the reports in the UI.
- apiGroups: ["projectcalico.org"]
  resources: ["globalreporttypes"]
  verbs: ["get"]

---

# Allow users to access Tigera Secure EE Manager.
apiVersion: projectcalico.org/v3
kind: NetworkPolicy
metadata:
  name: allow-cnx.cnx-manager-access
  namespace: calico-monitoring
spec:
  order: 1
  tier: allow-cnx
  selector: k8s-app == 'cnx-manager'
  types:
  - Ingress
  ingress:
  - action: Allow
    protocol: TCP
    source:
      # This policy allows access to Tigera Secure EE Manager from anywhere: narrow it down if
      # only certain subnets should be allowed.
      nets: ["0.0.0.0/0"]
    destination:
      # By default, Tigera Secure EE Manager is accessed over https. Update this if needed.
      ports: [9443]
---

# Allow internal communication to compliance-server from Manager.
apiVersion: projectcalico.org/v3
kind: NetworkPolicy
metadata:
  name: allow-cnx.compliance-server
  namespace: calico-monitoring
spec:
  order: 1
  tier: allow-cnx
  selector: k8s-app == 'compliance-server'
  types:
  - Ingress
  ingress:
  - action: Allow
    protocol: TCP
    source:
      selector: k8s-app == 'cnx-manager'
    destination:
      ports: [5443]
