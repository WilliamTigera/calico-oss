{{- $elasticMode := include "tigera-secure-ee.elasticsearch.mode" . }}
---

# This ConfigMap contains customizations for Elasticsearch and Curator.
kind: ConfigMap
apiVersion: v1
metadata:
  name: tigera-es-config
  namespace: calico-monitoring
data:
  # The name of the cluster.  This field is used as part of the index name of Elasticsearch logs, and is intended
  # to allow multiple clusters to share one Elasticsearch cluster.  The value of this field must match that of
  # tigera.cnx-manager.cluster-name in tigera-cnx-manager-config.
  tigera.elasticsearch.cluster-name: "cluster"
  # The number of shards for index tigera_secure_ee_flows.
  tigera.elasticsearch.flows-index-shards: "5"
  # The number of shards for index tigera_secure_ee_dns.
  tigera.elasticsearch.dns-index-shards: "5"
  # The retention period for flow logs, in days.  Logs written on a day that started at least this long ago are
  # removed.  To keep logs for at least x days, use a retention period of x+1.
  tigera.elasticsearch.flow-retention: "8"
  # The retention period for audit logs, in days.  Logs written on a day that started at least this long ago are
  # removed.  To keep logs for at least x days, use a retention period of x+1.
  tigera.elasticsearch.audit-retention: "91"
  # The retention period for snapshots, in days. Snapshots are periodic captures
  # of resources which along with audit events are used to generate reports.
  # Consult the Compliance Reporting documentation for more details on snapshots.
  # Logs written on a day that started at least this long ago are
  # removed.  To keep logs for at least x days, use a retention period of x+1.
  tigera.elasticsearch.snapshot-retention: "91"
  # The retention period for compliance reports, in days. Reports are output
  # from the analysis of the system state and audit events for compliance reporting.
  # Consult the Compliance Reporting documentation for more details on reports.
  # Logs written on a day that started at least this long ago are
  # removed.  To keep logs for at least x days, use a retention period of x+1.
  tigera.elasticsearch.compliance-report-retention: "91"
  # The retention period for DNS logs, in days.  Logs written on a day that started at least this long ago are
  # removed.  To keep logs for at least x days, use a retention period of x+1.
  tigera.elasticsearch.dns-retention: "8"
  # Indices are removed (starting with the oldest) when the disk utilization exceeds
  # max-total-storage-pct. Lowering the value leads to lower disk utilization, while
  # increasing it by too much can have an impact on the stability of your Elasticsearch
  # cluster.
  tigera.elasticsearch.max-total-storage-pct: "80"
  # TSEE will remove dns and flow log indices once the combined data exceeds this
  # threshold. The default value (70% of the cluster size) is used because flow
  # logs and dns logs often use the most disk space; this allows compliance and
  # security indices to be retained longer. The oldest indices are removed first.
  # Set this value to be lower than or equal to, the value for
  # max-total-storage-pct.
  tigera.elasticsearch.max-logs-storage-pct: "70"
  # Whether to include the custom flow filters defined below in the fluentd config
  # file.  Value must be "true" or "false".
  tigera.elasticsearch.flow-filtering: "false"
  # Flow filtering additional fluentd configuration.  This field is only used when
  # tigera.elasticsearch-flow-filtering is set to "true".  The example here filters
  # out all flows to/from the dev namespace.
  tigera.elasticsearch.flow-filters.conf: |-
    <filter flows>
      @type grep
      <exclude>
        key source_namespace
        pattern dev
      </exclude>
      <exclude>
        key dest_namespace
        pattern dev
      </exclude>
    </filter>
  # Whether to include the custom DNS filters defined below in the fluentd config
  # file.  Value must be "true" or "false".
  tigera.elasticsearch.dns-filtering: "false"
  # DNS filtering additional fluentd configuration.  This field is only used when
  # tigera.elasticsearch-dns-filtering is set to "true".  The example here filters
  # out DNS lookups for cluster-internal domain names.
  tigera.elasticsearch.dns-filters.conf: |-
    <filter dns>
      @type grep
      <exclude>
        key qname
        pattern /\.cluster\.local$/
      </exclude>
    </filter>
  tigera.elasticsearch.access-mode: "serviceuser"
  tigera.elasticsearch.scheme: "https"
{{- if eq $elasticMode "external" }}
  tigera.elasticsearch.host: {{ .Values.elasticsearch.host | default "__ELASTICSEARCH_HOST__" | quote }}
  tigera.elasticsearch.port: {{ .Values.elasticsearch.port | default "__ELASTICSEARCH_PORT__" | quote }}
  tigera.elasticsearch.ca.path: /etc/ssl/elastic/ca.pem
  tigera.kibana.host: {{ .Values.kibana.host | default "__KIBANA_HOST__" | quote }}
  tigera.kibana.port: {{ .Values.kibana.port | default "__KIBANA_PORT__" | quote }}
{{- else }}
  tigera.elasticsearch.host: {{ .Values.elasticsearch.host | default "tigera-elasticsearch-es-http.calico-monitoring.svc.cluster.local" }}
  tigera.elasticsearch.port: {{ .Values.elasticsearch.port | default "9200" | quote }}
  tigera.elasticsearch.ca.path: /etc/ssl/elastic/ca.pem
  tigera.kibana.host: {{ .Values.kibana.host | default "tigera-kibana-kb-http" | quote }}
  tigera.kibana.port: {{ .Values.kibana.port | default "5601" | quote }}
{{- end }}
---

# This manifest installs the Service which gets traffic to the calico-node metrics
# reporting endpoint.
apiVersion: v1
kind: Service
metadata:
  namespace: kube-system
  name: calico-node-metrics
  labels:
    k8s-app: {{ include "nodeName" . }}
spec:
  selector:
    k8s-app: {{ include "nodeName" . }}
  type: ClusterIP
  clusterIP: None
  ports:
  - name: calico-metrics-port
    port: 9081
    targetPort: 9081
    protocol: TCP
---

# This manifest creates a network policy to allow traffic to Alertmanager
# (TCP port 9093).
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: calico-node-alertmanager
  namespace: calico-monitoring
spec:
  ingress:
  - ports:
    - port: 9093
      protocol: TCP
  podSelector:
    matchLabels:
      alertmanager: calico-node-alertmanager
      app: alertmanager
---

# This manifest creates a network policy to allow traffic between
# Alertmanagers for HA configuration (TCP port 6783).
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: calico-node-alertmanager-mesh
  namespace: calico-monitoring
spec:
  ingress:
  - from:
    - podSelector:
        matchExpressions:
        - key: app
          operator: In
          values:
          - alertmanager
        - key: alertmanager
          operator: In
          values:
          - calico-node-alertmanager
    ports:
    - port: 6783
      protocol: TCP
    - port: 6783
      protocol: UDP
  podSelector:
    matchLabels:
      alertmanager: calico-node-alertmanager
      app: alertmanager
---

# This manifest creates a network policy to allow traffic to access the
# Prometheus (TCP port 9090).
apiVersion: networking.k8s.io/v1
kind: NetworkPolicy
metadata:
  name: prometheus
  namespace: calico-monitoring
spec:
  ingress:
  - ports:
    - port: 9090
      protocol: TCP
  podSelector:
    matchLabels:
      app: prometheus
      prometheus: calico-node-prometheus
---

apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: prometheus
rules:
- apiGroups: [""]
  resources:
  - nodes
  - services
  - endpoints
  - pods
  verbs: ["get", "list", "watch"]
- apiGroups: [""]
  resources:
  - configmaps
  verbs: ["get"]
- nonResourceURLs: ["/metrics"]
  verbs: ["get"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: prometheus
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: prometheus
subjects:
- kind: ServiceAccount
  name: prometheus
  namespace: calico-monitoring
---
apiVersion: v1
kind: ServiceAccount
metadata:
  name: prometheus
  namespace: calico-monitoring
---

# This manifest creates a ServiceMonitor to select calico-node metrics endpoints.
apiVersion: monitoring.coreos.com/v1
kind: ServiceMonitor
metadata:
  name: calico-node-monitor
  namespace: calico-monitoring
  labels:
    team: network-operators
spec:
  selector:
    matchLabels:
      k8s-app: {{ include "nodeName" . }}
  namespaceSelector:
    matchNames:
    - kube-system
  endpoints:
  - port: calico-metrics-port
    interval: 5s
    scrapeTimeout: 5s
    honorLabels: true
---

{{ .Files.Get "prometheus-dp-alerting.yaml" }}

---

# This manifest creates a secret that will be mounted as the Alertmanager
# configuration file.
# Write your alertmanager configuration file based on
# https://prometheus.io/docs/alerting/configuration/
# and save it to a file, say alertmanager.yaml and then run:
#
#       $ cat alertmanager.yaml | base64 -w 0
#
# and paste the output below.
#
# The encoded secret below decodes to this configuration.
#
# global:
#   resolve_timeout: 5m
# route:
#   group_by: ['job']
#   group_wait: 30s
#   group_interval: 1m
#   repeat_interval: 5m
#   receiver: 'webhook'
#     group_by: ['alertname']
# receivers:
# - name: 'webhook'
#   webhook_configs:
#   - url: 'http://calico-alertmanager-webhook:30501/'

apiVersion: v1
kind: Secret
metadata:
  name: alertmanager-calico-node-alertmanager
  namespace: calico-monitoring
data:
  alertmanager.yaml: Z2xvYmFsOgogIHJlc29sdmVfdGltZW91dDogNW0Kcm91dGU6CiAgZ3JvdXBfYnk6IFsnam9iJ10KICBncm91cF93YWl0OiAzMHMKICBncm91cF9pbnRlcnZhbDogMW0KICByZXBlYXRfaW50ZXJ2YWw6IDVtCiAgcmVjZWl2ZXI6ICd3ZWJob29rJwpyZWNlaXZlcnM6Ci0gbmFtZTogJ3dlYmhvb2snCiAgd2ViaG9va19jb25maWdzOgogIC0gdXJsOiAnaHR0cDovL2NhbGljby1hbGVydG1hbmFnZXItd2ViaG9vazozMDUwMS8nCg==
---

apiVersion: monitoring.coreos.com/v1
kind: Alertmanager
metadata:
  name: calico-node-alertmanager
  namespace: calico-monitoring
spec:
  version: {{.Values.alertmanager.tag}}
  baseImage: {{.Values.alertmanager.image}}
  nodeSelector:
    beta.kubernetes.io/os: linux
    # If necessary, uncomment the line below and edit key:value pair with appropriate value for your environment.
    #node.role: infrastructure
  replicas: 3
---

# This manifest installs the Service which gets traffic to the Calico
# AlertManager.
apiVersion: v1
kind: Service
metadata:
  name: calico-node-alertmanager
  namespace: calico-monitoring
spec:
  type: {{ .Values.alertmanager.service.type }}
  ports:
  - name: web
{{- if .Values.alertmanager.service.nodePort }}
    nodePort: {{ .Values.alertmanager.service.nodePort }}
{{- end }}
    port: 9093
    protocol: TCP
    targetPort: web
  selector:
    alertmanager: calico-node-alertmanager
---

# This manifest runs a Prometheus instance that will monitor calico-node
# denied packet metrics.
apiVersion: monitoring.coreos.com/v1
kind: Prometheus
metadata:
  name: calico-node-prometheus
  namespace: calico-monitoring
spec:
  baseImage: {{.Values.prometheus.image}}
  nodeSelector:
    beta.kubernetes.io/os: linux
    # If necessary, uncomment the line below and edit key:value pair with appropriate value for your environment.
    #node.role: infrastructure
  serviceAccountName: prometheus
  serviceMonitorSelector:
    matchLabels:
      team: network-operators
  version: {{.Values.prometheus.tag}}
  retention: 24h
  resources:
    requests:
      memory: 400Mi
  ruleSelector:
    matchLabels:
      role: calico-prometheus-rules
      prometheus: calico-node-prometheus
  alerting:
    alertmanagers:
      - namespace: calico-monitoring
        name: calico-node-alertmanager
        port: web
        scheme: http
---

# This manifest installs the Service which gets traffic to the Calico
# Prometheus.
apiVersion: v1
kind: Service
metadata:
  name: calico-node-prometheus
  namespace: calico-monitoring
spec:
  type: {{ .Values.prometheus.scrapeTargets.node.service.type }}
  ports:
  - name: web
{{- if .Values.prometheus.scrapeTargets.node.service.nodePort }}
    nodePort: {{ .Values.prometheus.scrapeTargets.node.service.nodePort }}
{{- end }}
    port: 9090
    protocol: TCP
    targetPort: web
  selector:
    prometheus: calico-node-prometheus

{{- if eq $elasticMode "operator" }}

---

{{- if eq .Values.kibana.service.type "NodePort" }}
# Add a NodePort for access to Kibana on port 30601.
# This can be customized if you would like to expose Kibana in a different way.
{{- end }}
apiVersion: v1
kind: Service
metadata:
  labels:
    name: kibana-tigera
  name: tigera-kibana
  namespace: calico-monitoring
spec:
  selector:
    k8s-app: tigera-kibana
  ports:
    - port: 5601
      targetPort: 5601
{{- if .Values.kibana.service.nodePort }}
      nodePort: {{ .Values.kibana.service.nodePort }}
{{- end }}
  type: {{ .Values.kibana.service.type }}
{{- if .Values.kibana.service.clusterIP }}
  clusterIP: {{ .Values.kibana.service.clusterIP }}
{{- end }}
{{- if .Values.kibana.service.loadBalancerIP }}
  loadBalancerIP: {{ .Values.kibana.service.loadBalancerIP }}
{{- end }}
{{- end }}
---

apiVersion: v1
kind: ServiceAccount
metadata:
  name: tigera-fluentd
  namespace: calico-monitoring
---

kind: DaemonSet
apiVersion: apps/v1
metadata:
  name: tigera-fluentd-node
  namespace: calico-monitoring
  labels:
    k8s-app: tigera-fluentd-node
spec:
  selector:
    matchLabels:
      k8s-app: tigera-fluentd-node
  updateStrategy:
    type: RollingUpdate
    rollingUpdate:
      maxUnavailable: 1
  template:
    metadata:
      labels:
        k8s-app: tigera-fluentd-node
    spec:
      nodeSelector:
        beta.kubernetes.io/os: linux
      tolerations:
        # Make sure fluentd-node gets scheduled on all nodes.
        - effect: NoSchedule
          operator: Exists
        - effect: NoExecute
          operator: Exists
      serviceAccountName: tigera-fluentd
{{- if .Values.imagePullSecrets }}
      imagePullSecrets:
{{- range $key, $value := .Values.imagePullSecrets }}
        - name: {{ $key }}
{{- end }}
{{- end }}
      containers:
        - name: fluentd
          image: {{.Values.fluentd.image}}:{{.Values.fluentd.tag}}
{{ tuple .Values.fluentd.resources | include "tigera-secure-ee.resourceLimits" | indent 10 -}}
          env:
            - name: FLUENT_UID
              value: "0"
            - name: ELASTIC_INDEX_SUFFIX
              valueFrom:
                configMapKeyRef:
                  name: tigera-es-config
                  key: tigera.elasticsearch.cluster-name
            - name: ELASTIC_FLOWS_INDEX_SHARDS
              valueFrom:
                configMapKeyRef:
                  name: tigera-es-config
                  key: tigera.elasticsearch.flows-index-shards
            - name: ELASTIC_DNS_INDEX_SHARDS
              valueFrom:
                configMapKeyRef:
                  name: tigera-es-config
                  key: tigera.elasticsearch.dns-index-shards
            - name: FLUENTD_FLOW_FILTERS
              valueFrom:
                configMapKeyRef:
                  name: tigera-es-config
                  key: tigera.elasticsearch.flow-filtering
            - name: FLUENTD_DNS_FILTERS
              valueFrom:
                configMapKeyRef:
                  name: tigera-es-config
                  key: tigera.elasticsearch.dns-filtering
            - name: FLOW_LOG_FILE
              value: /var/log/calico/flowlogs/flows.log
            - name: DNS_LOG_FILE
              value: /var/log/calico/dnslogs/dns.log
            - name: ELASTIC_HOST
              valueFrom:
                configMapKeyRef:
                  name: tigera-es-config
                  key: tigera.elasticsearch.host
            - name: ELASTIC_PORT
              valueFrom:
                configMapKeyRef:
                  name: tigera-es-config
                  key: tigera.elasticsearch.port
            - name: FLUENTD_ES_SECURE
              value: "true"
            - name: ELASTIC_SSL_VERIFY
              value: "true"
{{- if eq $elasticMode "external" }}
            - name: ELASTIC_USER
              valueFrom:
                secretKeyRef:
                  name: elastic-fluentd-user
                  key: username
            - name: ELASTIC_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: elastic-fluentd-user
                  key: password
{{- else }}
{{- if .Values.elasticsearch.fluentd.password }}
            - name: ELASTIC_USER
              value: {{ .Values.elasticsearch.fluentd.username | quote }}
            - name: ELASTIC_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: elastic-fluentd-user
                  key: password
                  optional: true
{{- else }}
            - name: ELASTIC_USER
              value: "elastic"
            - name: ELASTIC_PASSWORD
              valueFrom:
                secretKeyRef:
                  name: tigera-elasticsearch-es-elastic-user
                  key: elastic
{{- end }}
{{- end }}
            - name: AWS_KEY_ID
              valueFrom:
                secretKeyRef:
                  name: tigera-s3-archiving
                  key: aws.key.id
                  optional: true
            - name: AWS_SECRET_KEY
              valueFrom:
                secretKeyRef:
                  name: tigera-s3-archiving
                  key: aws.secret.key
                  optional: true
            - name: S3_STORAGE
              valueFrom:
                configMapKeyRef:
                  name: tigera-s3-archiving
                  key: s3.storage
                  optional: true
            - name: S3_BUCKET_NAME
              valueFrom:
                configMapKeyRef:
                  name: tigera-s3-archiving
                  key: s3.bucket.name
                  optional: true
            - name: AWS_REGION
              valueFrom:
                configMapKeyRef:
                  name: tigera-s3-archiving
                  key: aws.region
                  optional: true
            - name: S3_BUCKET_PATH
              valueFrom:
                configMapKeyRef:
                  name: tigera-s3-archiving
                  key: s3.bucket.path
                  optional: true
            - name: S3_FLUSH_INTERVAL
              valueFrom:
                configMapKeyRef:
                  name: tigera-s3-archiving
                  key: s3.flush-interval
                  optional: true
            - name: SYSLOG_FLOW_LOG
              valueFrom:
                configMapKeyRef:
                  name: tigera-syslog-archiving
                  key: flow-logs
                  optional: true
            - name: SYSLOG_AUDIT_LOG
              valueFrom:
                configMapKeyRef:
                  name: tigera-syslog-archiving
                  key: audit-logs
                  optional: true
            - name: SYSLOG_HOST
              valueFrom:
                configMapKeyRef:
                  name: tigera-syslog-archiving
                  key: host
                  optional: true
            - name: SYSLOG_PORT
              valueFrom:
                configMapKeyRef:
                  name: tigera-syslog-archiving
                  key: port
                  optional: true
            - name: SYSLOG_PROTOCOL
              valueFrom:
                configMapKeyRef:
                  name: tigera-syslog-archiving
                  key: protocol
                  optional: true
            - name: SYSLOG_FLUSH_INTERVAL
              valueFrom:
                configMapKeyRef:
                  name: tigera-syslog-archiving
                  key: flush-interval
                  optional: true
            - name: SYSLOG_TLS
              valueFrom:
                configMapKeyRef:
                  name: tigera-syslog-archiving
                  key: tls
                  optional: true
            - name: SYSLOG_VERIFY_MODE
              valueFrom:
                configMapKeyRef:
                  name: tigera-syslog-archiving
                  key: verify_mode
                  optional: true
            - name: SYSLOG_PACKET_SIZE
              valueFrom:
                configMapKeyRef:
                  name: tigera-syslog-archiving
                  key: packet-size
                  optional: true
            - name: SYSLOG_HOSTNAME
              valueFrom:
                fieldRef:
                  fieldPath: spec.nodeName
{{- if ne .Values.fluentd.kubeAuditMountPath "/var/log/calico" }}
            - name: KUBE_AUDIT_LOG
              value: {{ .Values.fluentd.kubeAuditMountPath }}/kube-audit.log
{{- end }}
{{- if .Values.fluentd.env }}
{{ toYaml .Values.fluentd.env | indent 12 }}
{{- end }}
{{- if .Values.fluentd.runAsPrivileged }}
          securityContext:
            privileged: true
{{- end }}
          volumeMounts:
            - name: var-log-calico
              mountPath: /var/log/calico
{{- if ne .Values.fluentd.kubeAuditMountPath "/var/log/calico" }}
            - name: kube-audit-logs
              mountPath: {{ .Values.fluentd.kubeAuditMountPath }}
{{- end }}
            - name: es-config
              mountPath: /etc/fluentd/flow-filters.conf
              subPath: tigera.elasticsearch.flow-filters.conf
            - name: es-config
              mountPath: /etc/fluentd/dns-filters.conf
              subPath: tigera.elasticsearch.dns-filters.conf
            - name: elastic-ca-cert-volume
              mountPath: /etc/fluentd/elastic
            - name: syslog-config
              mountPath: /etc/fluentd/syslog/ca.pem
              subPath: ca
          readinessProbe:
            exec:
              command:
                - sh
                - -c
                - /bin/readiness.sh
            initialDelaySeconds: 60
            periodSeconds: 60
          livenessProbe:
            exec:
              command:
                - sh
                - -c
                - /bin/liveness.sh
            initialDelaySeconds: 60
            periodSeconds: 60
      volumes:
        - name: var-log-calico
          hostPath:
            type: DirectoryOrCreate
            path: /var/log/calico
{{- if ne .Values.fluentd.kubeAuditMountPath "/var/log/calico" }}
        - name: kube-audit-logs
          hostPath:
            type: DirectoryOrCreate
            path: {{ .Values.fluentd.kubeAuditMountPath }}
{{- end }}
        - name: es-config
          configMap:
            name: tigera-es-config
        # Should be created during installation process.
{{- if eq $elasticMode "external" }}
        - name: elastic-ca-cert-volume
          secret:
            optional: true
            items:
            - key: tigera.elasticsearch.ca
              path: ca.pem
            secretName: tigera-es-config
{{- else }}
        - name: elastic-ca-cert-volume
          secret:
            items:
            - key: tls.crt
              path: ca.pem
            secretName: tigera-elasticsearch-es-http-certs-public
{{- end}}
        - name: syslog-config
          configMap:
            name: tigera-syslog-archiving
            defaultMode: 420
            optional: true
---

apiVersion: batch/v1beta1
kind: CronJob
metadata:
  name: tigera-es-curator
  namespace: calico-monitoring
  labels:
    k8s-app: es-curator
spec:
  schedule: "@hourly"
  jobTemplate:
    spec:
      template:
        metadata:
          name: tigera-es-curator
          namespace: calico-monitoring
          labels:
            k8s-app: es-curator
        spec:
          restartPolicy: OnFailure
          nodeSelector:
            beta.kubernetes.io/os: linux
          tolerations:
            - key: node-role.kubernetes.io/master
              effect: NoSchedule
{{- if .Values.imagePullSecrets }}
          imagePullSecrets:
{{- range $key, $value := .Values.imagePullSecrets }}
            - name: {{ $key }}
{{- end }}
{{- end }}
          containers:
            - name: tigera-es-curator
              image: {{.Values.esCurator.image}}:{{.Values.esCurator.tag}}
{{ tuple .Values.esCurator.resources | include "tigera-secure-ee.resourceLimits" | indent 14 -}}
              env:
                - name: ELASTIC_INDEX_SUFFIX
                  valueFrom:
                    configMapKeyRef:
                      name: tigera-es-config
                      key: tigera.elasticsearch.cluster-name
                - name: EE_FLOWS_INDEX_RETENTION_PERIOD
                  valueFrom:
                    configMapKeyRef:
                      name: tigera-es-config
                      key: tigera.elasticsearch.flow-retention
                - name: EE_AUDIT_INDEX_RETENTION_PERIOD
                  valueFrom:
                    configMapKeyRef:
                      name: tigera-es-config
                      key: tigera.elasticsearch.audit-retention
                - name: EE_SNAPSHOT_INDEX_RETENTION_PERIOD
                  valueFrom:
                    configMapKeyRef:
                      name: tigera-es-config
                      key: tigera.elasticsearch.snapshot-retention
                - name: EE_COMPLIANCE_REPORT_INDEX_RETENTION_PERIOD
                  valueFrom:
                    configMapKeyRef:
                      name: tigera-es-config
                      key: tigera.elasticsearch.compliance-report-retention
                - name: EE_MAX_LOGS_STORAGE_PCT
                  valueFrom:
                    configMapKeyRef:
                      name: tigera-es-config
                      key: tigera.elasticsearch.max-logs-storage-pct
                - name: EE_MAX_TOTAL_STORAGE_PCT
                  valueFrom:
                    configMapKeyRef:
                      name: tigera-es-config
                      key: tigera.elasticsearch.max-total-storage-pct
                - name: EE_DNS_INDEX_RETENTION_PERIOD
                  valueFrom:
                    configMapKeyRef:
                      name: tigera-es-config
                      key: tigera.elasticsearch.dns-retention
                - name: ELASTIC_HOST
                  valueFrom:
                    configMapKeyRef:
                      name: tigera-es-config
                      key: tigera.elasticsearch.host
                - name: ELASTIC_PORT
                  valueFrom:
                    configMapKeyRef:
                      name: tigera-es-config
                      key: tigera.elasticsearch.port
                - name: ELASTIC_SSL_VERIFY
                  value: "true"
{{- if eq $elasticMode "external" }}
                - name: ELASTIC_USER
                  valueFrom:
                    secretKeyRef:
                      name: elastic-curator-user
                      key: username
                - name: ELASTIC_PASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: elastic-curator-user
                      key: password
{{- else }}
{{- if .Values.elasticsearch.curator.password }}
                - name: ELASTIC_USER
                  value: {{ .Values.elasticsearch.curator.username | quote }}
                - name: ELASTIC_PASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: elastic-curator-user
                      key: password
                      optional: true
{{- else }}
                - name: ELASTIC_USER
                  value: "elastic"
                - name: ELASTIC_PASSWORD
                  valueFrom:
                    secretKeyRef:
                      name: tigera-elasticsearch-es-elastic-user
                      key: elastic
{{- end }}
{{- end }}
                - name: ES_CURATOR_BACKEND_CERT
                  value: /etc/curator/elastic/ca.pem
{{- if .Values.esCurator.env }}
{{ toYaml .Values.esCurator.env | indent 16 }}
{{- end }}
              volumeMounts:
                - name: elastic-ca-cert-volume
                  mountPath: /etc/curator/elastic
              livenessProbe:
                exec:
                  command:
                    - sh
                    - -c
                    - /usr/bin/curator --config /curator/curator_config.yaml --dry-run /curator/curator_action.yaml
                initialDelaySeconds: 60
                periodSeconds: 60
              securityContext:
                runAsNonRoot: true
                allowPrivilegeEscalation: false
          volumes:
            # Should be created during installation process.
{{- if eq $elasticMode "external" }}
            - name: elastic-ca-cert-volume
              secret:
                optional: true
                items:
                - key: tigera.elasticsearch.ca
                  path: ca.pem
                secretName: tigera-es-config
{{- else }}
            - name: elastic-ca-cert-volume
              secret:
                items:
                - key: tls.crt
                  path: ca.pem
                secretName: tigera-elasticsearch-es-http-certs-public
{{- end}}
---
{{- if .Values.elasticTseeInstaller.enable }}

apiVersion: batch/v1
kind: Job
metadata:
  name: elastic-tsee-installer
  namespace: calico-monitoring
spec:
  template:
    spec:
      restartPolicy: OnFailure
{{- if .Values.imagePullSecrets }}
      imagePullSecrets:
{{- range $key, $value := .Values.imagePullSecrets }}
        - name: {{ $key }}
{{- end }}
{{- end }}
      containers:
      - name: install
        image: {{.Values.elasticTseeInstaller.image}}:{{.Values.elasticTseeInstaller.tag}}
{{ tuple .Values.elasticTseeInstaller.resources | include "tigera-secure-ee.resourceLimits" | indent 8 -}}
        env:
{{- if eq $elasticMode "external" }}
          - name: USER
            valueFrom:
              secretKeyRef:
                name: elastic-ee-installer
                key: username
          - name: PASSWORD
            valueFrom:
              secretKeyRef:
                name: elastic-ee-installer
                key: password
{{- else }}
          - name: USER
            value: "elastic"
          - name: PASSWORD
            valueFrom:
              secretKeyRef:
                name: tigera-elasticsearch-es-elastic-user
                key: elastic
          - name: START_XPACK_TRIAL
            value: "false"
{{- end }}
          - name: ELASTIC_HOST
            valueFrom:
              configMapKeyRef:
                name: tigera-es-config
                key: tigera.elasticsearch.host
          - name: ELASTIC_PORT
            valueFrom:
              configMapKeyRef:
                name: tigera-es-config
                key: tigera.elasticsearch.port
          - name: ES_CA_CERT
            value: /certs/es-ca.pem
          - name: KB_CA_CERT
            value: /certs/kb-ca.pem
          - name: KIBANA_HOST
            valueFrom:
              configMapKeyRef:
                name: tigera-es-config
                key: tigera.kibana.host
          - name: KIBANA_PORT
            valueFrom:
              configMapKeyRef:
                name: tigera-es-config
                key: tigera.kibana.port
          - name: CLUSTER_NAME
            valueFrom:
              configMapKeyRef:
                name: tigera-es-config
                key: tigera.elasticsearch.cluster-name
        volumeMounts:
          - mountPath: /certs/es-ca.pem
            subPath: es-ca.pem
            name: es-certs
          - mountPath: /certs/kb-ca.pem
            subPath: kb-ca.pem
            name: kb-certs
      volumes:
          {{- if eq $elasticMode "external" }}
        - name: es-certs
          secret:
            optional: true
            items:
              - key: tigera.elasticsearch.ca
                path: es-ca.pem
            secretName: tigera-es-config
        - name: kb-certs
          secret:
            optional: true
            items:
              - key: tigera.elasticsearch.ca
                path: kb-ca.pem
            secretName: tigera-es-config
{{- else }}
        - name: es-certs
          secret:
            defaultMode: 420
            items:
              - key: tls.crt
                path: es-ca.pem
            secretName: tigera-elasticsearch-es-http-certs-public
        - name: kb-certs
          secret:
            defaultMode: 420
            items:
              - key: tls.crt
                path: kb-ca.pem
            secretName: tigera-kibana-kb-http-certs-public
{{- end}}
{{- if .Values.elasticTseeInstaller.env }}
{{ toYaml .Values.elasticTseeInstaller.env | indent 10 }}
{{- end }}
---
{{- end }}

apiVersion: v1
kind: ServiceAccount
metadata:
  name: intrusion-detection-controller
  namespace: calico-monitoring
---

kind: ClusterRole
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: intrusion-detection-controller
rules:
  - apiGroups:
      - "projectcalico.org"
    resources:
      - globalalerts
      - globalalerts/status
      - globalthreatfeeds
      - globalthreatfeeds/status
      - globalnetworksets
    verbs: ["*"]
---

kind: ClusterRoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: intrusion-detection-controller
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: intrusion-detection-controller
subjects:
- kind: ServiceAccount
  name: intrusion-detection-controller
  namespace: calico-monitoring
---

kind: Role
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: intrusion-detection-controller
  namespace: calico-monitoring
rules:
  - apiGroups:
      - ""
    resources:
      - secrets
      - configmaps
    verbs: ["get"]
---

kind: RoleBinding
apiVersion: rbac.authorization.k8s.io/v1
metadata:
  name: intrusion-detection-controller
  namespace: calico-monitoring
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: intrusion-detection-controller
subjects:
- kind: ServiceAccount
  name: intrusion-detection-controller
  namespace: calico-monitoring
---

apiVersion: apps/v1
kind: Deployment
metadata:
  name: intrusion-detection-controller
  namespace: calico-monitoring
  labels:
    k8s-app: intrusion-detection-controller
spec:
  replicas: 1
  selector:
    matchLabels:
      k8s-app: intrusion-detection-controller
  template:
    metadata:
      labels:
        k8s-app: intrusion-detection-controller
    spec:
      serviceAccountName: intrusion-detection-controller
{{- if .Values.imagePullSecrets }}
      imagePullSecrets:
{{- range $key, $value := .Values.imagePullSecrets }}
        - name: {{ $key }}
{{- end }}
{{- end }}
      containers:
      - name: controller
        image: {{ .Values.intrusionDetectionController.image }}:{{ .Values.intrusionDetectionController.tag }}
        livenessProbe:
          exec:
            command: ["/healthz", "liveness"]
          initialDelaySeconds: 5
        name: controller
        env:
          - name: CLUSTER_NAME
            valueFrom:
              configMapKeyRef:
                name: tigera-es-config
                key: tigera.elasticsearch.cluster-name
{{- if eq $elasticMode "external" }}
          - name: ELASTIC_HOST
            valueFrom:
              configMapKeyRef:
                name: tigera-es-config
                key: tigera.elasticsearch.host
          - name: ELASTIC_PORT
            valueFrom:
              configMapKeyRef:
                name: tigera-es-config
                key: tigera.elasticsearch.port
          - name: ELASTIC_USER
            valueFrom:
              secretKeyRef:
                name: elastic-ee-intrusion-detection
                key: username
          - name: ELASTIC_PASSWORD
            valueFrom:
              secretKeyRef:
                name: elastic-ee-intrusion-detection
                key: password
{{- else }}
{{- if .Values.elasticsearch.intrusionDetection.password }}
          - name: ELASTIC_USER
            value: {{ .Values.elasticsearch.intrusionDetection.username | quote }}
          - name: ELASTIC_PASSWORD
            valueFrom:
              secretKeyRef:
                name: elastic-ee-intrusion-detection
                key: password
                optional: true
          - name: ELASTIC_HOST
            value: tigera-elasticsearch-es-http.calico-monitoring.svc.cluster.local
          - name: ELASTIC_PORT
            value: "9200"
{{- else }}
          - name: ELASTIC_USER
            value: "elastic"
          - name: ELASTIC_PASSWORD
            valueFrom:
              secretKeyRef:
                name: tigera-elasticsearch-es-elastic-user
                key: elastic
                optional: true
          - name: ELASTIC_HOST
            value: tigera-elasticsearch-es-http.calico-monitoring.svc.cluster.local
          - name: ELASTIC_PORT
            value: "9200"
{{- end}}
{{- end}}
          - name: ELASTIC_CA
            value: /certs/ca.pem
          - name: CONFIG_MAP_NAMESPACE
            value: calico-monitoring
          - name: SECRETS_NAMESPACE
            value: calico-monitoring
        volumeMounts:
          - name: certs
            mountPath: /certs/
      volumes:
        - name: certs
          secret:
            optional: true
            defaultMode: 420
            items:
{{- if eq $elasticMode "external" }}
            - key: tigera.elasticsearch.ca
              path: ca.pem
            secretName: tigera-es-config
{{- else }}
            - key: tls.crt
              path: ca.pem
            secretName: tigera-elasticsearch-es-http-certs-public
{{- end }}
---
apiVersion: projectcalico.org/v3
kind: NetworkPolicy
metadata:
  name: allow-cnx.intrusion-detection-controller
  namespace: calico-monitoring
spec:
  order: 1
  tier: allow-cnx
  selector: k8s-app == 'intrusion-detection-controller'
  types:
    - Ingress
    - Egress
  ingress:
  # Intrusion detection controller doesn't listen on any external ports
  - action: Deny
  egress:
  # Block any link local IPs, e.g. cloud metadata, which are often targets
  # of server-side request forgery (SSRF) attacks
  - action: Deny
    protocol: TCP
    destination:
      nets:
      - "169.254.0.0/16"
  - action: Deny
    protocol: TCP
    destination:
      nets:
      - "fe80::/10"
  # Pass to subsequent tiers or profiles for further refinement
  - action: Pass

{{- if eq $elasticMode "operator" }}
---
# Allow access to Elasticsearch client nodes from Kibana, fluentd, and the
# intrusion detection installer
apiVersion: projectcalico.org/v3
kind: NetworkPolicy
metadata:
  name: allow-cnx.elasticsearch-access
  namespace: calico-monitoring
spec:
  order: 1
  tier: allow-cnx
  selector: elasticsearch.k8s.elastic.co/cluster-name == 'tigera-elasticsearch'
  types:
    - Ingress
  ingress:
    - action: Allow
      protocol: TCP
      source:
        selector: k8s-app == 'tigera-fluentd-node'
      destination:
        ports: [9200]
    - action: Allow
      protocol: TCP
      source:
        selector: job-name == 'elastic-tsee-installer'
      destination:
        ports: [9200]
    - action: Allow
      protocol: TCP
      source:
        selector: k8s-app == 'es-curator'
      destination:
        ports: [9200]
    - action: Allow
      protocol: TCP
      source:
        selector: k8s-app == 'cnx-manager'
      destination:
        ports: [9200]
    - action: Allow
      destination:
        ports:
          - 9200
      protocol: TCP
      source:
        selector: k8s-app == 'compliance-controller'
    - action: Allow
      destination:
        ports:
          - 9200
      protocol: TCP
      source:
        selector: k8s-app == 'compliance-server'
    - action: Allow
      destination:
        ports:
          - 9200
      protocol: TCP
      source:
        selector: k8s-app == 'compliance-snapshotter'
    - action: Allow
      destination:
        ports:
          - 9200
      protocol: TCP
      source:
        selector: k8s-app == 'compliance-reporter'
    - action: Allow
      destination:
        ports:
          - 9200
      protocol: TCP
      source:
        selector: k8s-app == 'compliance-benchmarker'
    - action: Allow
      destination:
        ports:
          - 9200
      protocol: TCP
      source:
        selector: k8s-app == 'intrusion-detection-controller'
    - action: Allow
      destination:
        ports:
          - 9200
      protocol: TCP
      source:
        selector: k8s-app == 'elastic-operator'
    - action: Allow
      destination:
        ports:
          - 9200
      protocol: TCP
      source:
        selector: k8s-app == 'tigera-kibana'

---

# Allow internal communication within the ElasticSearch cluster
apiVersion: projectcalico.org/v3
kind: NetworkPolicy
metadata:
  name: allow-cnx.elasticsearch-internal
  namespace: calico-monitoring
spec:
  order: 1
  tier: allow-cnx
  selector: elasticsearch.k8s.elastic.co/cluster-name == 'tigera-elasticsearch'
  types:
  - Ingress
  ingress:
  - action: Allow
    protocol: TCP
    source:
      selector: elasticsearch.k8s.elastic.co/cluster-name == 'tigera-elasticsearch'
    destination:
      ports: [9300]

---

kind: NetworkPolicy
apiVersion: projectcalico.org/v3
metadata:
  name: allow-cnx.elastic-operator-access
  namespace: calico-monitoring
spec:
  order: 1
  tier: allow-cnx
  selector: k8s-app == 'elastic-operator'
  types:
    - Ingress
  ingress:
    - action: Allow
      protocol: TCP
      destination:
        ports:
          - 443
          - 9443
---

# Allow access to Kibana
apiVersion: projectcalico.org/v3
kind: NetworkPolicy
metadata:
  name: allow-cnx.kibana-access
  namespace: calico-monitoring
spec:
  order: 1
  tier: allow-cnx
  selector: k8s-app == 'tigera-kibana'
  types:
  - Ingress
  ingress:
  - action: Allow
    protocol: TCP
    source:
      # This policy allows access to Kibana from anywhere.
      # Narrow this down to your management network or remove this
      # policy to block access to Kibana.
      nets: ["0.0.0.0/0"]
    destination:
      ports: [5601]
  - action: Allow
    protocol: TCP
    source:
      # Allow the installer to access Kibana
      selector: job-name == 'elastic-tsee-installer'
    destination:
      ports: [5601]
{{- end }}

{{- if .Values.fluentd.runAsPrivileged }}
---
# This manifest adds an additional security context for
# OpenShift so fluentd can ingest logs from the host.
kind: SecurityContextConstraints
apiVersion: v1
metadata:
  name: calico-fluentd-hostpath
allowPrivilegedContainer: true
allowHostDirVolumePlugin: true
runAsUser:
  type: RunAsAny
seLinuxContext:
  type: MustRunAs
fsGroup:
  type: RunAsAny
supplementalGroups:
  type: RunAsAny
users:
- system:serviceaccount:calico-monitoring:tigera-fluentd
groups:
- system:authenticated
{{- end }}

---

{{ .Files.Get "global-alert-templates.yaml" }}
