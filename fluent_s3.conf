<source>
  @type monitor_agent
  bind 127.0.0.1
  port 24220
</source>

<source>
  @type tail
  @id in_tail_flow_logs
  path "#{ENV['FLOW_LOG_FILE']}"
  pos_file "#{ENV['POS_DIR']}/flows.log.pos"
  tag flows
  read_from_head true
  <parse>
    @type json
  </parse>
</source>

<filter flows>
  @type record_transformer
  <record>
    host "#{Socket.gethostname}"
  </record>
</filter>

<match flows>
  @type copy
  <store>
    @type elasticsearch
    host "#{ENV['ELASTIC_HOST']}"
    port "#{ENV['ELASTIC_PORT']}"
    scheme "https"
    user "#{ENV['ELASTIC_USER']}"
    password "#{ENV['ELASTIC_PASSWORD']}"
    ca_file /etc/fluentd/elastic/ca.pem
    ssl_verify "#{ENV['ELASTIC_SSL_VERIFY']}"
    index_name "tigera_secure_ee_flows.#{ENV['ELASTIC_INDEX_SUFFIX']}.%Y%m%d"
    template_file /fluentd/etc/elastic_mapping_flows.template
    template_name tigera_secure_ee_flows
    <buffer time>
      timekey      1d # index renew frequency

      flush_mode interval
      flush_interval "#{ENV['ELASTIC_FLUSH_INTERVAL']}"
    </buffer>
  </store>
  <store>
    @type s3
    aws_key_id "#{ENV['AWS_KEY_ID']}"
    aws_sec_key "#{ENV['AWS_SEC_KEY']}"
    s3_bucket "#{ENV['S3_BUCKET_NAME']}"
    s3_region "#{ENV['AWS_REGION']}"
    path "#{ENV['S3_BUCKET_PATH']}"
    <buffer>
      flush_mode interval
      flush_interval "#{ENV['S3_FLUSH_INTERVAL']}"
    </buffer>
  </store>
</match>

<source>
  @type tail
  path /var/log/calico/audit/tsee-audit.log
  pos_file "#{ENV['POS_DIR']}/tsee-audit.log.pos"
  tag audit.tsee
  format json
  time_key time
  time_format %Y-%m-%dT%H:%M:%S.%N%z
</source>

<source>
  @type tail
  path "#{ENV['KUBE_AUDIT_LOG']}"
  pos_file "#{ENV['KUBE_AUDIT_POS']}"
  tag audit.kube
  format json
  time_key time
  time_format %Y-%m-%dT%H:%M:%S.%N%z
</source>

<filter audit.*>
  @type record_transformer
  enable_ruby
  <record>
    # Create a name field from the objectRef.name or responseObject.metadata.name
    name ${record["responseObject"].nil? ? (record["objectRef"].nil? ? "-" : record["objectRef"]["name"]) : record["responseObject"]["metadata"]["name"]}
  </record>
</filter>

<match audit.tsee>
  @type copy
  <store>
    @type elasticsearch
    host "#{ENV['ELASTIC_HOST']}"
    port "#{ENV['ELASTIC_PORT']}"
    scheme "https"
    user "#{ENV['ELASTIC_USER']}"
    password "#{ENV['ELASTIC_PASSWORD']}"
    ca_file /etc/fluentd/elastic/ca.pem
    ssl_verify "#{ENV['ELASTIC_SSL_VERIFY']}"
    index_name "tigera_secure_ee_audit_ee.#{ENV['ELASTIC_INDEX_SUFFIX']}.%Y%m%d"
    <buffer time>
      timekey      1d # index renew frequency

      flush_mode interval
      flush_interval "#{ENV['ELASTIC_FLUSH_INTERVAL']}"
    </buffer>
  </store>
  <store>
    @type s3
    aws_key_id "#{ENV['AWS_KEY_ID']}"
    aws_sec_key "#{ENV['AWS_SEC_KEY']}"
    s3_bucket "#{ENV['S3_BUCKET_NAME']}"
    s3_region "#{ENV['AWS_REGION']}"
    path "#{ENV['S3_BUCKET_PATH']}"
    <buffer>
      flush_mode interval
      flush_interval "#{ENV['S3_FLUSH_INTERVAL']}"
    </buffer>
  </store>
</match>

<match audit.kube>
  @type copy
  <store>
    @type elasticsearch
    host "#{ENV['ELASTIC_HOST']}"
    port "#{ENV['ELASTIC_PORT']}"
    scheme "https"
    user "#{ENV['ELASTIC_USER']}"
    password "#{ENV['ELASTIC_PASSWORD']}"
    ca_file /etc/fluentd/elastic/ca.pem
    ssl_verify "#{ENV['ELASTIC_SSL_VERIFY']}"
    index_name "tigera_secure_ee_audit_kube.#{ENV['ELASTIC_INDEX_SUFFIX']}.%Y%m%d"
    <buffer time>
      timekey      1d # index renew frequency

      flush_mode interval
      flush_interval "#{ENV['ELASTIC_FLUSH_INTERVAL']}"
    </buffer>
  </store>
  <store>
    @type s3
    aws_key_id "#{ENV['AWS_KEY_ID']}"
    aws_sec_key "#{ENV['AWS_SEC_KEY']}"
    s3_bucket "#{ENV['S3_BUCKET_NAME']}"
    s3_region "#{ENV['AWS_REGION']}"
    path "#{ENV['S3_BUCKET_PATH']}"
    <buffer>
      flush_mode interval
      flush_interval "#{ENV['S3_FLUSH_INTERVAL']}"
    </buffer>
  </store>
</match>
